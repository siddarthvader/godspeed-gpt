{"current_date":"2023-05-09T19:48:30.922Z","author":"sid","url":"https://docs.godspeed.systems/sitemap.xml","tokens":59287,"length":229446,"docs":[{"title":"Markdown page example | Godspeed Docs","url":"https://docs.godspeed.systems/markdown-page","date":"2023-05-09T19:48:20.559Z","content":"Markdown page example You don't need React to write simple standalone pages.","tokens":15,"length":76,"chunks":[{"doc_title":"Markdown page example | Godspeed Docs","doc_url":"https://docs.godspeed.systems/markdown-page","doc_date":"2023-05-09T19:48:20.559Z","content":"Markdown page example You don't need React to write simple standalone pages.","content_length":76,"content_tokens":15,"embedding":[]}]},{"title":"Learning Modules | Godspeed Docs","url":"https://docs.godspeed.systems/tutorial","date":"2023-05-09T19:48:20.731Z","content":"Learning Module - Level 1 [Beginner Track] March 26, 2023 · 3 min read Install Godspeed if you have not. The updated and maintained version is available at Godspeed website . Read More Learning Module - Level 2 [Intermediate Track] March 27, 2023 · 3 min read Before you start with this track, make sure you have completed the previous beginner track. It’s probably a good idea to revise all you have learnt so far. Read More Learning Module - Level 3 [Advanced Track] March 28, 2023 · One min read Before you start with the advanced track, make sure you have completed the previous beginner and intermediate track. It’s probably a good idea to revise all you have learnt so far. Read More","tokens":153,"length":689,"chunks":[{"doc_title":"Learning Modules | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial","doc_date":"2023-05-09T19:48:20.731Z","content":"Learning Module - Level 1 [Beginner Track] March 26, 2023 · 3 min read Install Godspeed if you have not. The updated and maintained version is available at Godspeed website . Read More Learning Module - Level 2 [Intermediate Track] March 27, 2023 · 3 min read Before you start with this track, make sure you have completed the previous beginner track. It’s probably a good idea to revise all you have learnt so far. Read More Learning Module - Level 3 [Advanced Track] March 28, 2023 · One min read Before you start with the advanced track, make sure you have completed the previous beginner and intermediate track. It’s probably a good idea to revise all you have learnt so far. Read More","content_length":689,"content_tokens":153,"embedding":[]}]},{"title":"Archive | Godspeed Docs","url":"https://docs.godspeed.systems/tutorial/archive","date":"2023-05-09T19:48:20.868Z","content":"","tokens":0,"length":0,"chunks":[{"doc_title":"Archive | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/archive","doc_date":"2023-05-09T19:48:20.868Z","content":"","content_length":0,"content_tokens":0,"embedding":[]}]},{"title":"Learning Module - Level 3 [Advanced Track] | Godspeed Docs","url":"https://docs.godspeed.systems/tutorial/learning-module/advanced-track","date":"2023-05-09T19:48:20.991Z","content":"Learning Module - Level 3 [Advanced Track] March 28, 2023 · One min read Before you start with the advanced track, make sure you have completed the previous beginner and intermediate track. It’s probably a good idea to revise all you have learnt so far. Study Log fields masking feature which is used if you want to mask any sensitive information in the logs. link Study Log format on different environments. link Study how to add a custom identifier in logs.(This will be useful if you want any custom unique identifier to be added in the logs.) link Study about OTEL and logs/metrics/traces. Read online. Custom logs, traces and metrics in YAML DSL. link Study Encryption of prisma field. link Elasticgraph setup for Elasticsearch. link Authorization within workflows. link Validation errors in event. link Thanks! I hope you enjoyed learning about Godspeed.","tokens":184,"length":860,"chunks":[{"doc_title":"Learning Module - Level 3 [Advanced Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/advanced-track","doc_date":"2023-05-09T19:48:20.991Z","content":"Learning Module - Level 3 [Advanced Track] March 28, 2023 · One min read Before you start with the advanced track, make sure you have completed the previous beginner and intermediate track. It’s probably a good idea to revise all you have learnt so far. Study Log fields masking feature which is used if you want to mask any sensitive information in the logs. link Study Log format on different environments. link Study how to add a custom identifier in logs.(This will be useful if you want any custom unique identifier to be added in the logs.) link Study about OTEL and logs/metrics/traces. Read online. Custom logs, traces and metrics in YAML DSL. link Study Encryption of prisma field. link Elasticgraph setup for Elasticsearch. link Authorization within workflows. link Validation errors in event. link Thanks! I hope you enjoyed learning about Godspeed.","content_length":860,"content_tokens":184,"embedding":[]}]},{"title":"Learning Module - Level 1 [Beginner Track] | Godspeed Docs","url":"https://docs.godspeed.systems/tutorial/learning-module/beginner-track","date":"2023-05-09T19:48:21.134Z","content":"Learning Module - Level 1 [Beginner Track] March 26, 2023 · 3 min read Install Godspeed if you have not. The updated and maintained version is available at Godspeed website . Start the service from a remote container Tip: Learn about dev containers in VS Code and its benefits . We would prefer you to go through the getting started section for this. Once your service is in running status as mentioned in getting started section, Open the Swagger doc at localhost:3000/api-docs Hit the Hello world API Export Swagger spec and import the same as Postman collection. From within godspeed gen-api-docs Now come back to the project structure in VS Code . In order to study the basic parts of your project check here . Study about events (docs) What is schema driven development ? Study how to define the schema of the event. FYI, Godspeed events use Swagger/JSON-schema specification. Lets test the schema based validation of inputs by Godspeed. Go to Swagger and give a wrong input to your event. See how it rejects. Now modify the hello world event schema in src/events/hello_world.yaml to add name in query section. Make it required: true. Test the new schema through Swagger or Postman, by sending hit without name and then with a name. The five kinds of inputs you receive from an HTTP event -> params (path), query, headers, body, files (in case of file upload) The kind of inputs you receive from Kafka event - body (only) Studying workflows or functions in Godspeed world (docs) * Do read the documentation on the workflow structure and have some hands-on experience. * Open `src/functions/com/jfs/hello_world.yaml` in VS Code. * Study inline scripting within workflows (docs). Check config/default to see which is the primary language in your project, for inline scripting: js or coffee? For the time being keep it JS (default). Here’s an example of Coffee Script in `src/functions/com/coffee/httpbin_anything_coffee.yaml` You can study that to check the coffee script flavor of programming. It is shorthand for JS. Let's come back to inline scripting with JS for now. * In the first and only task of hello world workflow, we will replace static string Hello World with Hello `<your_name_from_query>` * Replace the string ‘Hello World’ with `<% ”Hello ” + inputs.query.name %>.` * Go to Swagger and hit with your name. You should obtain the following response Hello <your_name> Calling pure JS/TS from within workflows In the previous section you had allowed for name parameters in the query. Now, create a JS file called hello_world.js (or .ts) in the functions/com/biz directory. Write code like module.exports = function helloWorld (text) { return text; } Edit the hello_world.yaml to call fn: com.biz.hello_world and set args: <% ”Hello ” + inputs.query.name %> Go to Swagger Hello World example, and hit with your name. You should see the response Hello <your_name> Now read about the plugins on the documentation. The purpose of plugins is that you can invoke the functions defined in files in that directory, from within the inline JS or CS scripts that you write in the YAML. Test out the example from the documentation. Congratulations! You have now successfully tested events and workflows basics. Before you move to our next section , do inspect the src folder. Feel free to play around by modifying the schema and workflow to your curiosity with everything you have learned so far. See You in Level 2 Track","tokens":798,"length":3424,"chunks":[{"doc_title":"Learning Module - Level 1 [Beginner Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/beginner-track","doc_date":"2023-05-09T19:48:21.134Z","content":"Learning Module - Level 1 [Beginner Track] March 26, 2023 · 3 min read Install Godspeed if you have not. The updated and maintained version is available at Godspeed website  Start the service from a remote container Tip: Learn about dev containers in VS Code and its benefits  We would prefer you to go through the getting started section for this. Once your service is in running status as mentioned in getting started section, Open the Swagger doc at localhost:3000/api-docs Hit the Hello world API Export Swagger spec and import the same as Postman collection. From within godspeed gen-api-docs Now come back to the project structure in VS Code  In order to study the basic parts of your project check here  Study about events (docs) What is schema driven development ? Study how to define the schema of the event. FYI, Godspeed events use Swagger/JSON-schema specification.","content_length":877,"content_tokens":190,"embedding":[]},{"doc_title":"Learning Module - Level 1 [Beginner Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/beginner-track","doc_date":"2023-05-09T19:48:21.134Z","content":"Lets test the schema based validation of inputs by Godspeed. Go to Swagger and give a wrong input to your event. See how it rejects. Now modify the hello world event schema in src/events/hello_world.yaml to add name in query section. Make it required: true. Test the new schema through Swagger or Postman, by sending hit without name and then with a name. The five kinds of inputs you receive from an HTTP event -> params (path), query, headers, body, files (in case of file upload) The kind of inputs you receive from Kafka event - body (only) Studying workflows or functions in Godspeed world (docs) * Do read the documentation on the workflow structure and have some hands-on experience. * Open `src/functions/com/jfs/hello_world.yaml` in VS Code. * Study inline scripting within workflows (docs)","content_length":799,"content_tokens":192,"embedding":[]},{"doc_title":"Learning Module - Level 1 [Beginner Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/beginner-track","doc_date":"2023-05-09T19:48:21.134Z","content":"Check config/default to see which is the primary language in your project, for inline scripting: js or coffee? For the time being keep it JS (default) Here’s an example of Coffee Script in `src/functions/com/coffee/httpbin_anything_coffee.yaml` You can study that to check the coffee script flavor of programming. It is shorthand for JS. Let's come back to inline scripting with JS for now. * In the first and only task of hello world workflow, we will replace static string Hello World with Hello `<your_name_from_query>` * Replace the string ‘Hello World’ with `<% ”Hello ” + inputs.query.name %>.` * Go to Swagger and hit with your name.","content_length":640,"content_tokens":167,"embedding":[]},{"doc_title":"Learning Module - Level 1 [Beginner Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/beginner-track","doc_date":"2023-05-09T19:48:21.134Z","content":"You should obtain the following response Hello <your_name> Calling pure JS/TS from within workflows In the previous section you had allowed for name parameters in the query. Now, create a JS file called hello_world.js (or .ts) in the functions/com/biz directory. Write code like module.exports = function helloWorld (text) { return text; } Edit the hello_world.yaml to call fn: com.biz.hello_world and set args: <% ”Hello ” + inputs.query.name %> Go to Swagger Hello World example, and hit with your name. You should see the response Hello <your_name> Now read about the plugins on the documentation. The purpose of plugins is that you can invoke the functions defined in files in that directory, from within the inline JS or CS scripts that you write in the YAML. Test out the example from the documentation. Congratulations! You have now successfully tested events and workflows basics. Before you move to our next section , do inspect the src folder. Feel free to play around by modifying the schema and workflow to your curiosity with everything you have learned so far. See You in Level 2 Track.","content_length":1099,"content_tokens":251,"embedding":[]}]},{"title":"Learning Module - Level 2 [Intermediate Track] | Godspeed Docs","url":"https://docs.godspeed.systems/tutorial/learning-module/intermediate-track","date":"2023-05-09T19:48:21.293Z","content":"Learning Module - Level 2 [Intermediate Track] March 27, 2023 · 3 min read Before you start with this track, make sure you have completed the previous beginner track. It’s probably a good idea to revise all you have learnt so far. To set up database models and CRUD API. Create a project with PostgreSQL or MongoDB . Go to src/datasources and inspect the .prisma file created with the name of your selected database. You can change the name of that file according to your needs. A sample model is already created for you. Go to prisma.io and study about Prisma ORM , its model and API for more information . When you run Godspeed build, it will create a prisma client for you and also set up the database. If you already have a running instance of dev container, it should automatically detect the new file and do this job for you. (If not, just run the build once, as you did for the first time.) With the database setup and configured model, generate the CRUD API. Run godspeed gen-crud-api from within the terminal of the dev container. (Check documentation of the CLI for more details on what CLI does.) You do not need to run godspeed build, as long as your service is up and running. The auto watch plugin should automatically create events, workflows and deploy the api schema also. Now go to the Swagger doc and see how the new endpoints are now up for your CRUD model. Also check the src folder with new events and new functions. Also check how the schema of events has been picked from the prisma model. Feel free to change the API schema or function workflows of any endpoints and play around with your learnings so far. Study the inbuilt functions of Godspeed from the documentation page. Check the examples in the project and have hands-on experience. Now, study the config/default.yaml thoroughly. You must read the documentation of config and mappings as mentioned in the Getting started/static variables section of documentation. Setup JWT authorization on your endpoints, as per the authentication page in documentation. Check how calls to your APIs are now failing with error 400. Follow the instructions on that page to create a new JWT token and pass that token in the header from your Postman collection. In order to import your Swagger to Postman, go to CLI and use the command to export the swagger documentation, which you later import to Postman. Change the log level of your service by going to config/default.yaml and check if the output log levels have changed. Now,study the custom-environment-variables and read about them in the documentation. Define a custom environment variable through your shell in the dev container. Then use that via config. <var_name> in hello world or somewhere else. Before you move to our next section, do some hands-on. Feel free to play around by modifying the schema and workflow to your curiosity with everything you have learned so far.","tokens":628,"length":2899,"chunks":[{"doc_title":"Learning Module - Level 2 [Intermediate Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/intermediate-track","doc_date":"2023-05-09T19:48:21.293Z","content":"Learning Module - Level 2 [Intermediate Track] March 27, 2023 · 3 min read Before you start with this track, make sure you have completed the previous beginner track. It’s probably a good idea to revise all you have learnt so far. To set up database models and CRUD API. Create a project with PostgreSQL or MongoDB  Go to src/datasources and inspect the .prisma file created with the name of your selected database. You can change the name of that file according to your needs. A sample model is already created for you. Go to prisma.io and study about Prisma ORM , its model and API for more information  When you run Godspeed build, it will create a prisma client for you and also set up the database. If you already have a running instance of dev container, it should automatically detect the new file and do this job for you.","content_length":829,"content_tokens":188,"embedding":[]},{"doc_title":"Learning Module - Level 2 [Intermediate Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/intermediate-track","doc_date":"2023-05-09T19:48:21.293Z","content":"(If not, just run the build once, as you did for the first time.) With the database setup and configured model, generate the CRUD API. Run godspeed gen-crud-api from within the terminal of the dev container. (Check documentation of the CLI for more details on what CLI does.) You do not need to run godspeed build, as long as your service is up and running. The auto watch plugin should automatically create events, workflows and deploy the api schema also. Now go to the Swagger doc and see how the new endpoints are now up for your CRUD model. Also check the src folder with new events and new functions. Also check how the schema of events has been picked from the prisma model. Feel free to change the API schema or function workflows of any endpoints and play around with your learnings so far. Study the inbuilt functions of Godspeed from the documentation page.","content_length":868,"content_tokens":191,"embedding":[]},{"doc_title":"Learning Module - Level 2 [Intermediate Track] | Godspeed Docs","doc_url":"https://docs.godspeed.systems/tutorial/learning-module/intermediate-track","doc_date":"2023-05-09T19:48:21.293Z","content":"Check the examples in the project and have hands-on experience. Now, study the config/default.yaml thoroughly. You must read the documentation of config and mappings as mentioned in the Getting started/static variables section of documentation. Setup JWT authorization on your endpoints, as per the authentication page in documentation. Check how calls to your APIs are now failing with error 400. Follow the instructions on that page to create a new JWT token and pass that token in the header from your Postman collection. In order to import your Swagger to Postman, go to CLI and use the command to export the swagger documentation, which you later import to Postman. Change the log level of your service by going to config/default.yaml and check if the output log levels have changed. Now,study the custom-environment-variables and read about them in the documentation. Define a custom environment variable through your shell in the dev container. Then use that via config. <var_name> in hello world or somewhere else. Before you move to our next section, do some hands-on. Feel free to play around by modifying the schema and workflow to your curiosity with everything you have learned so far.","content_length":1197,"content_tokens":249,"embedding":[]}]},{"title":"Hello World! | Godspeed Docs","url":"https://docs.godspeed.systems/docs/","date":"2023-05-09T19:48:21.423Z","content":"Hello World!","tokens":3,"length":12,"chunks":[{"doc_title":"Hello World! | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/","doc_date":"2023-05-09T19:48:21.423Z","content":"Hello World!","content_length":12,"content_tokens":3,"embedding":[]}]},{"title":"Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/communication/intro","date":"2023-05-09T19:48:21.556Z","content":"On this page Introduction Technologies used Linkerd (service mesh for sync communication) Kafka with CloudEvents format messages (Message bus for async communication) Argo Events (for listening to events from multiple sources and triggering workflows) Salient Features ​ In general, the communication could be one of the following External to API gateway ​ External services will communicate with API gateway synchronously and will follow the request - response cycle. Microservices communication ​ Communication between microservices could either be synchronous or asynchronous. All synchronous communication will happen, either through HTTP or gRPC, via the service mesh (using proxy sidecar). Asynchronous communication will follow event-driven architecture (EDA) through messages/events. The event format will follow the CloudEvents specification (a standard open format. Triggered workflows ​ A workflow is a serverless function which could be triggered by an event or a schedule, being listened to by ArgoEvents. Examples include AI/ML computation, CI/CD workflows, ETLs.","tokens":205,"length":1077,"chunks":[{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/communication/intro","doc_date":"2023-05-09T19:48:21.556Z","content":"On this page Introduction Technologies used Linkerd (service mesh for sync communication) Kafka with CloudEvents format messages (Message bus for async communication) Argo Events (for listening to events from multiple sources and triggering workflows) Salient Features ​ In general, the communication could be one of the following External to API gateway ​ External services will communicate with API gateway synchronously and will follow the request - response cycle. Microservices communication ​ Communication between microservices could either be synchronous or asynchronous. All synchronous communication will happen, either through HTTP or gRPC, via the service mesh (using proxy sidecar) Asynchronous communication will follow event-driven architecture (EDA) through messages/events. The event format will follow the CloudEvents specification (a standard open format. Triggered workflows ​ A workflow is a serverless function which could be triggered by an event or a schedule, being listened to by ArgoEvents. Examples include AI/ML computation, CI/CD workflows, ETLs.","content_length":1075,"content_tokens":205,"embedding":[]}]},{"title":"Technologies used (Default) | Godspeed Docs","url":"https://docs.godspeed.systems/docs/communication/technology-used/intro","date":"2023-05-09T19:48:21.702Z","content":"Technologies used (Default) Linkerd (service mesh for sync communication) Kafka with CloudEvents format messages (Message bus for async communication) Argo Events (for listening to events from multiple sources and triggering workflows)","tokens":44,"length":235,"chunks":[{"doc_title":"Technologies used (Default) | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/communication/technology-used/intro","doc_date":"2023-05-09T19:48:21.702Z","content":"Technologies used (Default) Linkerd (service mesh for sync communication) Kafka with CloudEvents format messages (Message bus for async communication) Argo Events (for listening to events from multiple sources and triggering workflows)","content_length":235,"content_tokens":44,"embedding":[]}]},{"title":"API specs | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","date":"2023-05-09T19:48:21.881Z","content":"On this page API specs Getting Started ​ Importing CRUD module to your project ​ Make sure that you have appropriate model specifications in your project. The CLI will guide the user for the project specific CRUD setup. Importing your project in a microservice ​ When you start Godspeed microservice with your project, it will [autogenerate the REST, Socket and Message bus endpoints] for the functions exported by your project. You can customize your exports in config/global-exports.yaml file. --> It will export the CRUD functions It will also export any other functions you have in your project structure. Universal CRUD API ​ The query interface of these functions is generic, and vendor independent. Yet, there are ways to run SQL and native queries on every vendor. //SQL query to Postgres godspeed.find({ _sql_query: “select * from….”, source: “postgres” //Can be any configured database which supports SQL }) //Mongodb query godspeed.findOrCreate({ _type: “article”, //Godspeed uses the terminology _type for entity type. It translates to collection/index/table names automatically in its pluralized form. _native_query : { tags: [\"technology\", \"low-code\"] }, source: “mongodb” }) All of the functions below can be run in any combination as a function DAG. CRUD API available functions ​ Create ​ Saves the entity in the target database. Params ​ _type: String //One of the types from the model _id: String | Number //Optional. If not given, a UUID is autogenerated. body: Object //A body as per the defined fields of the entity. This will be validated against the respective model Example request { '_type': 'borrower_profile', 'body': { 'english': { 'name': 'Deepti' }, 'hindi': { 'name': 'दीप्ति' }, pan: 'AKJPG810**', pincode: 176057, product: { //While creating an entity, it can also be linked to an existing entity, by their valid relationship _id: 5 //Link to the product with id 5 } } } Response ​ On Success ​ _id: id as specified in the request _type: type of the entity as sent in the request status: status code as per the standards message: Explanatory message On failure ​ //A GSError object status: status code, extending the standards message: Error message stack: Error stack errors: [GSError] Update ​ Params ​ _id: id of the entity field needs to be updated _type: the type of the entity update: the update instructions Update instructions follow the API specified here Example update instructions { _id: 'AVeawbnNW2vhiwtp9F2D', _type: 'event', update: { set: { title: 'Finding Common Ground', x: [1, 2], yString: ['Music'] }, push: { x: 567, yString: ['for', 'life'] }, addToSet: { x: [567], yString: ['for'] }, unset: [ 'y' ] } } Response ​ On Success ​ status: 201 message: Successfully updated data: _id: id as specified in the request _type: type of the entity updated On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Delete ​ Deletes are cascading. Deleting an entity also removes its references (and any denormalized data) in other entities to which the entity is liked. Params ​ _id: id of the entity field needs to be updated _type: the type of the entity Response ​ On Success ​ status: 204 message: Deleted successfully data: _id: id as specified in the request _type: type of the entity as sent in the request On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Get by Id ​ One can retrieve an entity of a given type by its _id. // Fetch borrower profile with selected fields and also some fields(GMV) from its relationship { _type: 'borrower_profile', _id: 'AVeuJeQ9jGz7t7QfUg_M', langs: ['hindi'] returnData: { name: 1, //multi lingual field in DB model spec mid: 1, linkedProduct: { name: 1 } } } // Returns { \"message\": \"Successfully read borrower_profile\", \"status\": 200, \"data\": { \"_type\": \"borrower_profile\", \"_id\": \"AVeuJeQ9jGz7t7QfUg_M\", \"data\": { 'hindi': { 'name': 'दीप्ति' }, 'mid': '87asdf87', 'linkedProduct': { '_id': 5, '_type': 'product' 'data': { 'name': \"Existence\" } } } } } Params ​ _type: type of the entity _id: id of the entity returnData: The data to return including the entity's own properties and those of its related entities. langs: Optional. The retrieved data is to be brought only for these languages. Other language data will not be retrieved. By default, all language data is retrieved. Response ​ On Success ​ status: 200 data: the data including the id, type and the requested returnData message: Successfully retrieved On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Find ​ Find entities matching specified criteria with pagination. // Fetch borrower profile with selected fields and also some fields(GMV) from its relationship { _type: 'borrower_profile', query: { name: 'दीप्ति' }, langs: ['hindi'], returnData: { name: 1, //multi lingual field in DB model spec mid: 1, linkedProduct: { name: 1 } } } // Returns { \"message\": \"Successfully read borrower_profile\", \"status\": 200, \"data\": { \"count\" : 11, \"data\": [ { \"_type\": \"borrower_profile\", \"_id\": \"AVeuJeQ9jGz7t7QfUg_M\", \"data\": { 'hindi': { 'name': 'दीप्ति' }, 'mid': '87asdf87', 'linkedProduct': { '_id': 5, '_type': 'product' 'data': { 'name': \"Existence\" } } } } ] }, } Params ​ _type: A single entity type or multiple types in an array returnData: The data to return (same as get by id above) query: The matching criteria in GS syntax _sql_query: The matching criteria in SQL format _native_query: The matching criteria in native DB format size: The number of matching results wanted offset: The offset for pagination langs: The languages in which data is to be retrieved Note: among the query, _sql_query, _native_query clauses, only one of them must be present, else an error is raised Response ​ On Success ​ status: 200 data: matched entities with their respective id, type and returnData message: Successfully retrieved On failure ​ //A GSError object status: status code, extending the standards message: Telling what happened errors: List of GSError objects explaining errors in details Search ​ It is an extension of 'find' with where clause including additional text search capabilities like match_phrase and autosuggest. Currently works with Elasticsearch backend. Request ​ _type: A single entity type or multiple types in an array returnData: The data to return (same as get by id) query: The matching criteria in GS syntax _native_query: Elasticsearch API _sql_query: Elasticsearch supported SQL syntax size: The number of matching results wanted offset: The offset for pagination langs: The languages in which data is to be retrieved Response ​ On Success ​ _id: id as specified in the request _type: type of the entity as sent in the request status: status code as per the standards On failure ​ //A GSError object status: status code, extending the standards message: Telling what happened errors: List of GSError objects explaining errors in details","tokens":1860,"length":7067,"chunks":[{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"On this page API specs Getting Started ​ Importing CRUD module to your project ​ Make sure that you have appropriate model specifications in your project. The CLI will guide the user for the project specific CRUD setup. Importing your project in a microservice ​ When you start Godspeed microservice with your project, it will [autogenerate the REST, Socket and Message bus endpoints] for the functions exported by your project. You can customize your exports in config/global-exports.yaml file. --> It will export the CRUD functions It will also export any other functions you have in your project structure. Universal CRUD API ​ The query interface of these functions is generic, and vendor independent. Yet, there are ways to run SQL and native queries on every vendor.","content_length":772,"content_tokens":160,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"//SQL query to Postgres godspeed.find({ _sql_query: “select * from….”, source: “postgres” //Can be any configured database which supports SQL }) //Mongodb query godspeed.findOrCreate({ _type: “article”, //Godspeed uses the terminology _type for entity type. It translates to collection/index/table names automatically in its pluralized form. _native_query : { tags: [\"technology\", \"low-code\"] }, source: “mongodb” }) All of the functions below can be run in any combination as a function DAG. CRUD API available functions ​ Create ​ Saves the entity in the target database. Params ​ _type: String //One of the types from the model _id: String | Number //Optional. If not given, a UUID is autogenerated. body: Object //A body as per the defined fields of the entity.","content_length":764,"content_tokens":206,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"This will be validated against the respective model Example request { '_type': 'borrower_profile', 'body': { 'english': { 'name': 'Deepti' }, 'hindi': { 'name': 'दीप्ति' }, pan: 'AKJPG810**', pincode: 176057, product: { //While creating an entity, it can also be linked to an existing entity, by their valid relationship _id: 5 //Link to the product with id 5 } } } Response ​ On Success ​ _id: id as specified in the request _type: type of the entity as sent in the request status: status code as per the standards message: Explanatory message On failure ​ //A GSError object status: status code, extending the standards message: Error message stack: Error stack errors: [GSError] Update ​ Params ​ _id: id of the entity field needs to be updated _type: the type of the entity update: the update instructions Update instructions follow the API specified here Example update instructions { _id: 'AVeawbnNW2vhiwtp9F2D', _type: 'event', update: { set: { title: 'Finding Common Ground', x: [1, 2], yString: ['Music'] }, push: { x: 567, yString: ['for', 'life'] }, addToSet: { x: [567], yString: ['for'] }, unset: [ 'y' ] } } Response ​ On Success ​ status: 201 message: Successfully updated data: _id: id as specified in the request _type: type of the entity updated On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Delete ​ Deletes are cascading.","content_length":1432,"content_tokens":404,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"Deleting an entity also removes its references (and any denormalized data) in other entities to which the entity is liked. Params ​ _id: id of the entity field needs to be updated _type: the type of the entity Response ​ On Success ​ status: 204 message: Deleted successfully data: _id: id as specified in the request _type: type of the entity as sent in the request On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Get by Id ​ One can retrieve an entity of a given type by its _id.","content_length":570,"content_tokens":138,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"// Fetch borrower profile with selected fields and also some fields(GMV) from its relationship { _type: 'borrower_profile', _id: 'AVeuJeQ9jGz7t7QfUg_M', langs: ['hindi'] returnData: { name: 1, //multi lingual field in DB model spec mid: 1, linkedProduct: { name: 1 } } } // Returns { \"message\": \"Successfully read borrower_profile\", \"status\": 200, \"data\": { \"_type\": \"borrower_profile\", \"_id\": \"AVeuJeQ9jGz7t7QfUg_M\", \"data\": { 'hindi': { 'name': 'दीप्ति' }, 'mid': '87asdf87', 'linkedProduct': { '_id': 5, '_type': 'product' 'data': { 'name': \"Existence\" } } } } } Params ​ _type: type of the entity _id: id of the entity returnData: The data to return including the entity's own properties and those of its related entities.","content_length":726,"content_tokens":252,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"langs: Optional. The retrieved data is to be brought only for these languages. Other language data will not be retrieved. By default, all language data is retrieved. Response ​ On Success ​ status: 200 data: the data including the id, type and the requested returnData message: Successfully retrieved On failure ​ //A GSError object status: status code, extending the standards stack: Error stack message: Error message errors: [GSError] Find ​ Find entities matching specified criteria with pagination.","content_length":503,"content_tokens":106,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"// Fetch borrower profile with selected fields and also some fields(GMV) from its relationship { _type: 'borrower_profile', query: { name: 'दीप्ति' }, langs: ['hindi'], returnData: { name: 1, //multi lingual field in DB model spec mid: 1, linkedProduct: { name: 1 } } } // Returns { \"message\": \"Successfully read borrower_profile\", \"status\": 200, \"data\": { \"count\" : 11, \"data\": [ { \"_type\": \"borrower_profile\", \"_id\": \"AVeuJeQ9jGz7t7QfUg_M\", \"data\": { 'hindi': { 'name': 'दीप्ति' }, 'mid': '87asdf87', 'linkedProduct': { '_id': 5, '_type': 'product' 'data': { 'name': \"Existence\" } } } } ] }, } Params ​ _type: A single entity type or multiple types in an array returnData: The data to return (same as get by id above) query: The matching criteria in GS syntax _sql_query: The matching criteria in SQL format _native_query: The matching criteria in native DB format size: The number of matching results wanted offset: The offset for pagination langs: The languages in which data is to be retrieved Note: among the query, _sql_query, _native_query clauses, only one of them must be present, else an error is raised Response ​ On Success ​ status: 200 data: matched entities with their respective id, type and returnData message: Successfully retrieved On failure ​ //A GSError object status: status code, extending the standards message: Telling what happened errors: List of GSError objects explaining errors in details Search ​ It is an extension of 'find' with where clause including additional text search capabilities like match_phrase and autosuggest.","content_length":1557,"content_tokens":435,"embedding":[]},{"doc_title":"API specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/CRUD%20API","doc_date":"2023-05-09T19:48:21.881Z","content":"Currently works with Elasticsearch backend. Request ​ _type: A single entity type or multiple types in an array returnData: The data to return (same as get by id) query: The matching criteria in GS syntax _native_query: Elasticsearch API _sql_query: Elasticsearch supported SQL syntax size: The number of matching results wanted offset: The offset for pagination langs: The languages in which data is to be retrieved Response ​ On Success ​ _id: id as specified in the request _type: type of the entity as sent in the request status: status code as per the standards On failure ​ //A GSError object status: status code, extending the standards message: Telling what happened errors: List of GSError objects explaining errors in details.","content_length":736,"content_tokens":161,"embedding":[]}]},{"title":"Data Federation | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/data-federation","date":"2023-05-09T19:48:22.055Z","content":"On this page Data Federation Introduction ​ Godspeed has in-built support for federating data from multiple sources like database, search engine, warehouse, third party API & another microservice in both sync and async manner. One can execute multiple queries to configured sources within a single query Read only, write only or mixed. i.e. a single instruction can contain a combination of multiple reads and writes. Non transactional or as a distributed transaction. Retry logic, circuit breaker. Notes In case there are multiple independent transactions in the call or queries of any nature, and one is ok for some to fail but some not, then one can specify ignoreError:true in those calls. It is recommended that the developer defines all the API schema on server side when going to production. They should export native functions like federate(shown below) as API contract with caution. The uses of data federation can be ​ Dedicated Backend For Frontend (BFF) service As part of a custom Godspeed service. Sample instruction ​ POST /api/v1/${domain}/${microserviceName}/federate { 'searchResponse': { //The response of the query will come under this key in response from the service \"instruction\": “findAll”, //This instruction has been declared on the server side in the API schema \"params”: { query_gs: { “match_phrase”: {“borrower.city.name”: “patna”}, “exists”: “pincode”, “anyOneOf”: [ {range: {annualIncome”: {\"gte\": 1000000}}}, {“match”: {“hasOwnHouse”: true}}, ] } } }, 'saveBorrowerProfileResponse': { “instruction”: “saveBorrowersProfile”, “ignoreError”: true, \"retry\": { \"count\": 3, \"timeout\": 200 //milliseconds }, \"params”: { “name”: 1, “pan”: “asdfadsf” } } }","tokens":462,"length":1679,"chunks":[{"doc_title":"Data Federation | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/data-federation","doc_date":"2023-05-09T19:48:22.055Z","content":"On this page Data Federation Introduction ​ Godspeed has in-built support for federating data from multiple sources like database, search engine, warehouse, third party API & another microservice in both sync and async manner. One can execute multiple queries to configured sources within a single query Read only, write only or mixed. i.e. a single instruction can contain a combination of multiple reads and writes. Non transactional or as a distributed transaction. Retry logic, circuit breaker. Notes In case there are multiple independent transactions in the call or queries of any nature, and one is ok for some to fail but some not, then one can specify ignoreError:true in those calls. It is recommended that the developer defines all the API schema on server side when going to production. They should export native functions like federate(shown below) as API contract with caution. The uses of data federation can be ​ Dedicated Backend For Frontend (BFF) service As part of a custom Godspeed service.","content_length":1010,"content_tokens":203,"embedding":[]},{"doc_title":"Data Federation | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/data-federation","doc_date":"2023-05-09T19:48:22.055Z","content":"Sample instruction ​ POST /api/v1/${domain}/${microserviceName}/federate { 'searchResponse': { //The response of the query will come under this key in response from the service \"instruction\": “findAll”, //This instruction has been declared on the server side in the API schema \"params”: { query_gs: { “match_phrase”: {“borrower.city.name”: “patna”}, “exists”: “pincode”, “anyOneOf”: [ {range: {annualIncome”: {\"gte\": 1000000}}}, {“match”: {“hasOwnHouse”: true}}, ] } } }, 'saveBorrowerProfileResponse': { “instruction”: “saveBorrowersProfile”, “ignoreError”: true, \"retry\": { \"count\": 3, \"timeout\": 200 //milliseconds }, \"params”: { “name”: 1, “pan”: “asdfadsf” } } }","content_length":667,"content_tokens":259,"embedding":[]}]},{"title":"CRUD Module Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","date":"2023-05-09T19:48:22.232Z","content":"On this page CRUD Module Introduction This introduction covers aspects and benefits of the CRUD API in Godspeed. CRUD API is part of common services available out of box which can be easily plugged into your code using Godspeed SDK DB CRUD is avialable with powerful features like : - Universal API with GSL, native and sql support - SQL/NoSQL databases supported - bi-directional Relationship management across NoSQL, SQL DBs (first of its kind) - in-built support of internalization and localization(first of its kind) - in-built denormalization and data dependency management (first of its kind) In order to use the API, one must first configure the data model Single Universal API for multiple database ​ In Godspeed, one can query multiple databases through single API layer. This generic API decouples The application code from underlying DB. Common params request: ds //datasource {queryType} : // can be one of query_sql, query_native, query_gsl { actual query} response: status_code message _score //total score result [ {_score://indivual score for each match data : { fields value corresponding to returnData } } ] A sample query This query will be sent to the underlying primary database, which is typically a transactional store. /** The default datasource for executing queries is the * primary datastore (as per the configuration) unless specifically mentioned in the * query with the 'source' argument. */ /api/v2/search { type: 'user', where: { name: \"ayush\" //Executes an exact match } } One may want to leverage Elasticsearch for text search or faster reads. If setup as secondary datastore, it will be synced with the primary as per the data model configuration. The following query is targetted specifically to Elasticsearch. But any supported database can be set as secondary, not just Elasticsearch. /api/v2/search { source: 'elasticsearch', //can be 'cache', 'mongodb' or an in memory object etc. query: { type: 'user', where: { age: { gte: 'ayush' }, 'city.name': { matchPhrase: 'delhi' //Executes a text search } } } } Support for native and SQL queries ​ A developer is not limited to use only the generic interface. Infact Godspeed design allows developer to execute SQL queries and native queries both. In case,source is cache or in-memory object store, then the native query or SQL query may or may not be available as the constraint imposed by underlying cache or in-memory object store. /api/v2/search { source: “postgres” //Can be any configured database and supporting SQL _sql_query: { 'select * from user where age > 8 and name='Ayush' } } //OR to a secondary datastore { source: 'mongoDB', _native_query : { tags: [\"technology\", \"low-code\"] }, } Relationship awareness ​ This API is relationship aware, even with NoSQL stores like Elasticsearch and MongoDB. Based on the relationship definitions, the following is managed by this library out of the box. (No code) Bidirectional relationship maintenance. i.e. A single API call establishes relationships from A to B and B to A It is also possible to configure storage of foreign keys only on one side, yet traverse from both sides. (Similar to Graph databases) Denormalisation support is also provided out of the box, across all NoSQL stores support. i.e. MongoDB and Elasticsearch //An example of many to many relationships. An Event has multiple speakers, and a speaker can speak in many events. speakers <> events [event] <> [speaker] Don't store foreign key in speaker //Means only event will have the foreign key of speaker stored in it. //For denormalization, the following rule in schema/denormalisation.txt stores the names of speakers in the event [event] speakers{name} //Whenever a speaker and event are linked, the speaker's name is automatically copied to the event along with the foreign key (speaker's id). //When unlinked, the name of the speaker is also removed along with the foreign key. //When the name of the speaker is changed, the new name reflects across all the events where that speaker is linked. Batching of queries ​ In order to optimize the performance of databases and the microservices, the framework allows to batch the CRUD queries, per database (by query category, i.e. get, create, find, delete, update) . All queries of one type and to one DB (irrespective of their arguments like entity type, where clause) are batched by default, unless specified otherwise. Queries and their reponse are multiplexed and demultiplexed internally so that the client never knows what else went in a batch. All queries including transactions are batched together internally in same sequence as arrival of the queries Error in one query does not affect the result of the other queries. The callers of each get their respective success or failure response as if they had executed the query without any batching in the first place. In config/collect.toml //Project level setting for batching. //Specify the batch size or timeout, for every query category noBatch = false // Can be set to true, in which case batching will not happen by default. [batchSizes] find = 20 create = 20 get = 20 update = 20 delete = 20 [timeouts] //in milli seconds find = 20 create = 20 get = 20 update = 20 delete = 20 When invoking through JS gs.find.collect() //Executes with batching //When exposing the function externally through a microservice, it is batched by default gs.find({ query: {} }) And, batching is optional //When invoking through JS, don't add .collect() to the CRUD function calls gs.find() //When exposing the function externally through a microservice, you can add noBatch: true gs.find({ noBatch: true } You can set default setting of the API to noBatch. In that case you will need to say noBatch: false exlicitly to batch a kind of query. Transactions ​ When writing to a datasource which support transactions, all writes are always transactions. A single write allows to update multiple entities/rows. Dual writes ​ Developer does not need to worry about dual writes for data syncing across multiple types of databases used in the app. Main featues of the dual writes are following: Just configure the federated DB model, write only to your primary. Your secondary databases will get eventually synced, as per your model settings. It is intended for future to also handle transactions spread across multiple databases via Saga pattern.","tokens":1438,"length":6341,"chunks":[{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"On this page CRUD Module Introduction This introduction covers aspects and benefits of the CRUD API in Godspeed. CRUD API is part of common services available out of box which can be easily plugged into your code using Godspeed SDK DB CRUD is avialable with powerful features like : - Universal API with GSL, native and sql support - SQL/NoSQL databases supported - bi-directional Relationship management across NoSQL, SQL DBs (first of its kind) - in-built support of internalization and localization(first of its kind) - in-built denormalization and data dependency management (first of its kind) In order to use the API, one must first configure the data model Single Universal API for multiple database ​ In Godspeed, one can query multiple databases through single API layer. This generic API decouples The application code from underlying DB.","content_length":848,"content_tokens":176,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"Common params request: ds //datasource {queryType} : // can be one of query_sql, query_native, query_gsl { actual query} response: status_code message _score //total score result [ {_score://indivual score for each match data : { fields value corresponding to returnData } } ] A sample query This query will be sent to the underlying primary database, which is typically a transactional store. /** The default datasource for executing queries is the * primary datastore (as per the configuration) unless specifically mentioned in the * query with the 'source' argument. */ /api/v2/search { type: 'user', where: { name: \"ayush\" //Executes an exact match } } One may want to leverage Elasticsearch for text search or faster reads. If setup as secondary datastore, it will be synced with the primary as per the data model configuration.","content_length":833,"content_tokens":197,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"The following query is targetted specifically to Elasticsearch. But any supported database can be set as secondary, not just Elasticsearch. /api/v2/search { source: 'elasticsearch', //can be 'cache', 'mongodb' or an in memory object etc. query: { type: 'user', where: { age: { gte: 'ayush' }, 'city.name': { matchPhrase: 'delhi' //Executes a text search } } } } Support for native and SQL queries ​ A developer is not limited to use only the generic interface. Infact Godspeed design allows developer to execute SQL queries and native queries both. In case,source is cache or in-memory object store, then the native query or SQL query may or may not be available as the constraint imposed by underlying cache or in-memory object store.","content_length":735,"content_tokens":180,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"/api/v2/search { source: “postgres” //Can be any configured database and supporting SQL _sql_query: { 'select * from user where age > 8 and name='Ayush' } } //OR to a secondary datastore { source: 'mongoDB', _native_query : { tags: [\"technology\", \"low-code\"] }, } Relationship awareness ​ This API is relationship aware, even with NoSQL stores like Elasticsearch and MongoDB. Based on the relationship definitions, the following is managed by this library out of the box. (No code) Bidirectional relationship maintenance. i.e. A single API call establishes relationships from A to B and B to A It is also possible to configure storage of foreign keys only on one side, yet traverse from both sides. (Similar to Graph databases) Denormalisation support is also provided out of the box, across all NoSQL stores support. i.e.","content_length":822,"content_tokens":199,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"MongoDB and Elasticsearch //An example of many to many relationships. An Event has multiple speakers, and a speaker can speak in many events. speakers <> events [event] <> [speaker] Don't store foreign key in speaker //Means only event will have the foreign key of speaker stored in it. //For denormalization, the following rule in schema/denormalisation.txt stores the names of speakers in the event [event] speakers{name} //Whenever a speaker and event are linked, the speaker's name is automatically copied to the event along with the foreign key (speaker's id) //When unlinked, the name of the speaker is also removed along with the foreign key. //When the name of the speaker is changed, the new name reflects across all the events where that speaker is linked.","content_length":766,"content_tokens":171,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"Batching of queries ​ In order to optimize the performance of databases and the microservices, the framework allows to batch the CRUD queries, per database (by query category, i.e. get, create, find, delete, update)  All queries of one type and to one DB (irrespective of their arguments like entity type, where clause) are batched by default, unless specified otherwise. Queries and their reponse are multiplexed and demultiplexed internally so that the client never knows what else went in a batch. All queries including transactions are batched together internally in same sequence as arrival of the queries Error in one query does not affect the result of the other queries. The callers of each get their respective success or failure response as if they had executed the query without any batching in the first place. In config/collect.toml //Project level setting for batching.","content_length":883,"content_tokens":186,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"//Specify the batch size or timeout, for every query category noBatch = false // Can be set to true, in which case batching will not happen by default. [batchSizes] find = 20 create = 20 get = 20 update = 20 delete = 20 [timeouts] //in milli seconds find = 20 create = 20 get = 20 update = 20 delete = 20 When invoking through JS gs.find.collect() //Executes with batching //When exposing the function externally through a microservice, it is batched by default gs.find({ query: {} }) And, batching is optional //When invoking through JS, don't add .collect() to the CRUD function calls gs.find() //When exposing the function externally through a microservice, you can add noBatch: true gs.find({ noBatch: true } You can set default setting of the API to noBatch.","content_length":763,"content_tokens":195,"embedding":[]},{"doc_title":"CRUD Module Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/CRUD/intro","doc_date":"2023-05-09T19:48:22.232Z","content":"In that case you will need to say noBatch: false exlicitly to batch a kind of query. Transactions ​ When writing to a datasource which support transactions, all writes are always transactions. A single write allows to update multiple entities/rows. Dual writes ​ Developer does not need to worry about dual writes for data syncing across multiple types of databases used in the app. Main featues of the dual writes are following: Just configure the federated DB model, write only to your primary. Your secondary databases will get eventually synced, as per your model settings. It is intended for future to also handle transactions spread across multiple databases via Saga pattern.","content_length":682,"content_tokens":135,"embedding":[]}]},{"title":"Model Setup | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","date":"2023-05-09T19:48:22.418Z","content":"On this page Model Setup Introduction ​ Based on scaffolding of microservice project, there will be right place to put project configurations for each module and also common settings. The model configuration discussed in this document is a part of the project configuration. The configurations for model/data setup cover not only the entity model with relationships but also all the different databases the project will use, how the data syncs between them and the Single Source of Truth settings for all entities. Setup of the DBs ​ In the configuration/databases/{db.toml} files there will be settings for respective dbs that are included in this project. //elasticsearch.toml name = elasticsearch maxConnections = 200 apiVersion = '7.4' requestTimeout = 90000 node = 'http://localhost:9200' sniffOnStart = true Entities and model ​ We use the word entities (similar to rows in MySQL, or nodes in a Graph) for refering to individual data points of a particular type. Each entity has an id, type, fields like text, date etc. and relationships (akin to foreign keys). The configurations of a field are all declared in one file. Each entity is stored in a separate table/index/colelction depending on the database used. The name of that will be pluralized and autogenerated as {entity._type + ‘s’} For example, for type video, the collection/index/table name will be videos The simple fields of an entity and their settings are defined in configFolder/schema/entities/{entityType}.toml As per the definitions, the schema in the datastores (ES or PG) is generated from here. The migrations also make use of these definitions, to run. Here is a sample config for a sample entity type in TOML format sstDB = postgres //The main DB for this entity as its SST. All writes will happen here first, and later the other DBs will get eventually (automatically) synced through CDC mechanism. [name] type = 'String' enum = ['value1', 'value2'] multiLingual = true [name.postgres] sortable = true //Will create index for this to sort on, in PG [name.elasticsearch] //Create appropriate indices in ES for the queries we wish to do on this field autoSuggestion = true exactMatch = true sortable = true Corresponding document of an Event, when returned from the API will look like shown below. It does not matter which underlying database the information is being fetched from. { \"_type\": \"person\", \"_id\": \"294464\", \"_version\": 4, \"data\": { \"tibetan\": { \"name\": \"ཆེན་པོ་མཆོག་ནས་ནང་ཆོས་ངོ་སྤྲོད་སྩལ་།\", }, \"english\": { \"name\": \"His Holiness the Fourteenth Dalai Lama.\", } } } The data model you set is used to generate the appropriate schema in respective databases. Relationships ​ You must define the relationships of your data model in configFolder/schema/relationships.txt It is compulsory to maintain relationship name both ways, from Entity A to B, and B to A. This is so that one can express Graph traversal from both sides. The format for specifying relationships in relationship file is relationNameFromAToB <> relationNameFromBToA entityTypeA <> entityTypeB //One to one relationNameFromAToB <> relationNameFromBToA [entityTypeA] <> entityTypeB //Many to one relationNameFromAToBs <> relationNameFromBToA entityTypeA <> [entityTypeB] //One to many relationNameFromAsToBs <> relationNameFromBsToAs [entityTypeA] <> [entityTypeB] //many to many As you can see, when an entity type is surrounded by square brackets [], it means cardinality of many Some examples speakers <> events [event] <> [speaker] sessions <> event event <> [session] Example link call es.deep.link({ e1: { _type: ‘event’, _id: ‘674’ }, e2: { _type: session, _id: 4 }, e1ToE2Relation: ‘sessions’ }) .then(console.log) denormalisation ​ We use denormalisation to make it fast Settings configFolder/schema/denormalisation.txt Imagine you have a database composed of events, speakers and persons. And, you wish to do the following two queries. Search events by speakers.person.name Show countwise breakup of search results on events, based on speakers.person.name (like on ecommerce sites) If your tables have only the foreign keys, you will have to do multiple hits to implement such cross table queries. And they will be slow. Depending on your data size, this may take a long long time before the final query result is returned. Also, your database will most probably get under heavy load. With Godspeed you can denormalize based on simple rule setting and achieve the same result with a single hit to the database. By denormalizing (always ensuring latest copy of) the speaker.person.name information within the event object, during index, update, link or unlink calls . For example, here is how ‘event’ may look like in denormalization settings (in the file joins/index.txt) [event] sessions{title, description} speakers.person{name} Based on your configuration the CRUD module works to automatically maintain the denormalised storage of speaker and session data in the event entities. You only need to link or unlink two entities by a relationship. Everything else is taken care by Godspeed. Maintenance of the denormalised state ​ Here are some scenarios in which the automatic denormalization will trigger in our example database. Whenever you update the name of a person, the events where he or she spoke, will also get updated with person’s new name. When you index (store) the event for first time in the database, and it contains speakers ids, the speaker’s name will also get copied inside the event entity as it gets stored/indexed. When the event is linked to a speaker, the speaker’s name will get copied inside the event entity When the event is unlinked from a speaker, the speaker’s id, name etc will get removed from the event entity The Butterfly effect ​ As you just saw, any update can potentially create a ripple update across entire Graph, for maintaining correct data state as per the denormalisation and also the data dependency rules like union and copy (more on the latter below). Since this is handled automatically, it saves the developer from the overhead of maintaining a consistent, denormalised graph state across all updates. Her code doesn’t need to save the updated field value at multiple places in the database- a big overhead, lots of confusing code, more bugs... Instead, she simply declares the behavior just once, in a human readable way. After that she leaves it to Godspeed to do all the internal bookkeeping to upkeep a correct denormalised graph state all the time. In ElasticSearch, MongoDB and other NoSQL stores, we can make use of the JSON style storage and do the joins within one document. In comparison to SQL way of rows, the document way of NoSQL allows for the denormalization easily. Have a look at how the denormalized speakers relationship is stored within an Godspeed event document. { \"_index\": \"events\", \"_type\": \"event\", \"_id\": \"294464\", \"_version\": 4, \"_source\": { \"speakers\": [ { \"_id\": \"c6c35e3b21815a4209054505ac5e1680a954efdf\", \"own\": true, \"data\": { \"person\": { \"_id\": \"1\", \"_version\": 1, \"data\": { \"english\": { \"name\": \"His Holiness the 14th Dalai Lama\" }, \"tibetan\": { \"name\" : \"ང་ས་སྐུ་ཕྲེང་བཅུ་བཞི་\" } } } } } ] } } Data dependency implementation ​ Note: This strategy is perhaps best applied in write less and read more scenarios. In many data models, data of an entity in your graph may depend on the data of other related entities. For ex. if a married woman has a new child, the husband also has a new child. And vice versa. Godspeed gives you an easy way to manage complex data dependencies between related entities of your information graph. As any update is made to any Entity in your Graph, Godspeed checks if any part of the remaining Graph should be updated by this change as per your data model settings. If yes, it updates the entire affected Graph (Butterfly effect). This saves LOTS of lines of code, time and effort in maintaining your inter-dependent data state so that you can move faster with your development goals. For now Godspeed supports two kinds of dependencies - Union from and Copy. Union from Settings are in configFolder/schema/union.toml Union from operation can be used to compute and store distinct values, whether relationships or data values, merged from field values of multiple related entities. This is useful for one to many or many to many relationships. Please look at the following examples to understand. [conference] speakers = '+talks.speaker' #As soon as a talk is linked to a conferece, or an already linked talk gets linked to a speaker, *the talk’s speaker is also linked to the conference as one of its speakers, if not already linked before*. Vice versa happens if the talk is unlinked to its speaker, or the talk is removed from the conference topics = '+talks.topics' #As soon as a talk is linked to an conference, or a topic is set to an already linked talk, the talk’s topic is also added to the conference as one of its topics, if not already there. Vice versa happens if the talk is unliked to the conference, or the topic is removed from the talk. [‘person’] grandChildren = +‘children.children’ #Whenever a person’s child gets a new child, the new child gets added to the person’s grandchildren [‘folder’] fileTypes = ‘+childFolders.fileTypes + childFiles.type’ #Calculate union of all file types existing in the entire folder tree (recursively). Anytime, any file gets added to any child folder in this tree, the type of that file gets unioned with the list of fileTypes of that child folder, and all its parent folders up in the hierarchy. Copy Settings are in configFolder/schema/union.toml Currently the copy functionality is achieved from within the union configuration. This is effective for many to one or one to one relations. For ex. [person] inLaws = \"+spouse.parents\" #This will ensure copy of in laws between husband and wife [file] permissions = \"+folder.permissions\" #Whenever a folder’s permissions are updated the underlying files’ permissions are updated automatically. You can still manually override them, without affecting the folder. But whenever the folder’s permissions are updated again, the file’s permissions will get overwritten. Multi Linguality ​ Settings file: configFolder/common.toml. In that set, supportedLanguages = [‘english’ , ‘tibetan’, ‘thirdLanguage’] If your data is in a single language or is language agnostic, then supportedLanguages = [] The fields which are declared multilingual, are stored like this in the _source of the entities. \"english\": { \"name\": \"His Holiness the 14th Dalai Lama\" }, \"tibetan\": { \"name\": \"ྋགོང་ས་སྐུ་ཕྲེང་བཅུ་བཞི་པ།\" } When creating, updating, searching or getting an entity, you have to specify the full path of every field, including its language. In search and get calls, you specify langs parameter, for the languages in which the data is to be fetched. By default data in all supported languages is fetched.","tokens":2786,"length":10874,"chunks":[{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"On this page Model Setup Introduction ​ Based on scaffolding of microservice project, there will be right place to put project configurations for each module and also common settings. The model configuration discussed in this document is a part of the project configuration. The configurations for model/data setup cover not only the entity model with relationships but also all the different databases the project will use, how the data syncs between them and the Single Source of Truth settings for all entities. Setup of the DBs ​ In the configuration/databases/{db.toml} files there will be settings for respective dbs that are included in this project.","content_length":657,"content_tokens":127,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"//elasticsearch.toml name = elasticsearch maxConnections = 200 apiVersion = '7.4' requestTimeout = 90000 node = 'http://localhost:9200' sniffOnStart = true Entities and model ​ We use the word entities (similar to rows in MySQL, or nodes in a Graph) for refering to individual data points of a particular type. Each entity has an id, type, fields like text, date etc. and relationships (akin to foreign keys) The configurations of a field are all declared in one file. Each entity is stored in a separate table/index/colelction depending on the database used.","content_length":559,"content_tokens":135,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"The name of that will be pluralized and autogenerated as {entity._type + ‘s’} For example, for type video, the collection/index/table name will be videos The simple fields of an entity and their settings are defined in configFolder/schema/entities/{entityType}.toml As per the definitions, the schema in the datastores (ES or PG) is generated from here. The migrations also make use of these definitions, to run. Here is a sample config for a sample entity type in TOML format sstDB = postgres //The main DB for this entity as its SST. All writes will happen here first, and later the other DBs will get eventually (automatically) synced through CDC mechanism.","content_length":660,"content_tokens":163,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"[name] type = 'String' enum = ['value1', 'value2'] multiLingual = true [name.postgres] sortable = true //Will create index for this to sort on, in PG [name.elasticsearch] //Create appropriate indices in ES for the queries we wish to do on this field autoSuggestion = true exactMatch = true sortable = true Corresponding document of an Event, when returned from the API will look like shown below. It does not matter which underlying database the information is being fetched from.","content_length":480,"content_tokens":115,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"{ \"_type\": \"person\", \"_id\": \"294464\", \"_version\": 4, \"data\": { \"tibetan\": { \"name\": \"ཆེན་པོ་མཆོག་ནས་ནང་ཆོས་ངོ་སྤྲོད་སྩལ་།\", }, \"english\": { \"name\": \"His Holiness the Fourteenth Dalai Lama.\", } } } The data model you set is used to generate the appropriate schema in respective databases. Relationships ​ You must define the relationships of your data model in configFolder/schema/relationships.txt It is compulsory to maintain relationship name both ways, from Entity A to B, and B to A. This is so that one can express Graph traversal from both sides.","content_length":551,"content_tokens":229,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"The format for specifying relationships in relationship file is relationNameFromAToB <> relationNameFromBToA entityTypeA <> entityTypeB //One to one relationNameFromAToB <> relationNameFromBToA [entityTypeA] <> entityTypeB //Many to one relationNameFromAToBs <> relationNameFromBToA entityTypeA <> [entityTypeB] //One to many relationNameFromAsToBs <> relationNameFromBsToAs [entityTypeA] <> [entityTypeB] //many to many As you can see, when an entity type is surrounded by square brackets [], it means cardinality of many Some examples speakers <> events [event] <> [speaker] sessions <> event event <> [session] Example link call es.deep.link({ e1: { _type: ‘event’, _id: ‘674’ }, e2: { _type: session, _id: 4 }, e1ToE2Relation: ‘sessions’ }) .then(console.log) denormalisation ​ We use denormalisation to make it fast Settings configFolder/schema/denormalisation.txt Imagine you have a database composed of events, speakers and persons.","content_length":939,"content_tokens":273,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"And, you wish to do the following two queries. Search events by speakers.person.name Show countwise breakup of search results on events, based on speakers.person.name (like on ecommerce sites) If your tables have only the foreign keys, you will have to do multiple hits to implement such cross table queries. And they will be slow. Depending on your data size, this may take a long long time before the final query result is returned. Also, your database will most probably get under heavy load. With Godspeed you can denormalize based on simple rule setting and achieve the same result with a single hit to the database. By denormalizing (always ensuring latest copy of) the speaker.person.name information within the event object, during index, update, link or unlink calls","content_length":775,"content_tokens":164,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"For example, here is how ‘event’ may look like in denormalization settings (in the file joins/index.txt) [event] sessions{title, description} speakers.person{name} Based on your configuration the CRUD module works to automatically maintain the denormalised storage of speaker and session data in the event entities. You only need to link or unlink two entities by a relationship. Everything else is taken care by Godspeed. Maintenance of the denormalised state ​ Here are some scenarios in which the automatic denormalization will trigger in our example database. Whenever you update the name of a person, the events where he or she spoke, will also get updated with person’s new name. When you index (store) the event for first time in the database, and it contains speakers ids, the speaker’s name will also get copied inside the event entity as it gets stored/indexed.","content_length":871,"content_tokens":193,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"When the event is linked to a speaker, the speaker’s name will get copied inside the event entity When the event is unlinked from a speaker, the speaker’s id, name etc will get removed from the event entity The Butterfly effect ​ As you just saw, any update can potentially create a ripple update across entire Graph, for maintaining correct data state as per the denormalisation and also the data dependency rules like union and copy (more on the latter below) Since this is handled automatically, it saves the developer from the overhead of maintaining a consistent, denormalised graph state across all updates. Her code doesn’t need to save the updated field value at multiple places in the database- a big overhead, lots of confusing code, more bugs.. Instead, she simply declares the behavior just once, in a human readable way. After that she leaves it to Godspeed to do all the internal bookkeeping to upkeep a correct denormalised graph state all the time. In ElasticSearch, MongoDB and other NoSQL stores, we can make use of the JSON style storage and do the joins within one document. In comparison to SQL way of rows, the document way of NoSQL allows for the denormalization easily. Have a look at how the denormalized speakers relationship is stored within an Godspeed event document.","content_length":1295,"content_tokens":271,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"{ \"_index\": \"events\", \"_type\": \"event\", \"_id\": \"294464\", \"_version\": 4, \"_source\": { \"speakers\": [ { \"_id\": \"c6c35e3b21815a4209054505ac5e1680a954efdf\", \"own\": true, \"data\": { \"person\": { \"_id\": \"1\", \"_version\": 1, \"data\": { \"english\": { \"name\": \"His Holiness the 14th Dalai Lama\" }, \"tibetan\": { \"name\" : \"ང་ས་སྐུ་ཕྲེང་བཅུ་བཞི་\" } } } } } ] } } Data dependency implementation ​ Note: This strategy is perhaps best applied in write less and read more scenarios.","content_length":460,"content_tokens":209,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"In many data models, data of an entity in your graph may depend on the data of other related entities. For ex. if a married woman has a new child, the husband also has a new child. And vice versa. Godspeed gives you an easy way to manage complex data dependencies between related entities of your information graph. As any update is made to any Entity in your Graph, Godspeed checks if any part of the remaining Graph should be updated by this change as per your data model settings. If yes, it updates the entire affected Graph (Butterfly effect) This saves LOTS of lines of code, time and effort in maintaining your inter-dependent data state so that you can move faster with your development goals. For now Godspeed supports two kinds of dependencies - Union from and Copy.","content_length":776,"content_tokens":162,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"Union from Settings are in configFolder/schema/union.toml Union from operation can be used to compute and store distinct values, whether relationships or data values, merged from field values of multiple related entities. This is useful for one to many or many to many relationships. Please look at the following examples to understand. [conference] speakers = '+talks.speaker' #As soon as a talk is linked to a conferece, or an already linked talk gets linked to a speaker, *the talk’s speaker is also linked to the conference as one of its speakers, if not already linked before*","content_length":581,"content_tokens":128,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"Vice versa happens if the talk is unlinked to its speaker, or the talk is removed from the conference topics = '+talks.topics' #As soon as a talk is linked to an conference, or a topic is set to an already linked talk, the talk’s topic is also added to the conference as one of its topics, if not already there. Vice versa happens if the talk is unliked to the conference, or the topic is removed from the talk. [‘person’] grandChildren = +‘children.children’ #Whenever a person’s child gets a new child, the new child gets added to the person’s grandchildren [‘folder’] fileTypes = ‘+childFolders.fileTypes + childFiles.type’ #Calculate union of all file types existing in the entire folder tree (recursively)","content_length":710,"content_tokens":189,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"Anytime, any file gets added to any child folder in this tree, the type of that file gets unioned with the list of fileTypes of that child folder, and all its parent folders up in the hierarchy. Copy Settings are in configFolder/schema/union.toml Currently the copy functionality is achieved from within the union configuration. This is effective for many to one or one to one relations. For ex. [person] inLaws = \"+spouse.parents\" #This will ensure copy of in laws between husband and wife [file] permissions = \"+folder.permissions\" #Whenever a folder’s permissions are updated the underlying files’ permissions are updated automatically. You can still manually override them, without affecting the folder. But whenever the folder’s permissions are updated again, the file’s permissions will get overwritten. Multi Linguality ​ Settings file: configFolder/common.toml.","content_length":869,"content_tokens":192,"embedding":[]},{"doc_title":"Model Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/model-setup","doc_date":"2023-05-09T19:48:22.418Z","content":"In that set, supportedLanguages = [‘english’ , ‘tibetan’, ‘thirdLanguage’] If your data is in a single language or is language agnostic, then supportedLanguages = [] The fields which are declared multilingual, are stored like this in the _source of the entities. \"english\": { \"name\": \"His Holiness the 14th Dalai Lama\" }, \"tibetan\": { \"name\": \"ྋགོང་ས་སྐུ་ཕྲེང་བཅུ་བཞི་པ།\" } When creating, updating, searching or getting an entity, you have to specify the full path of every field, including its language. In search and get calls, you specify langs parameter, for the languages in which the data is to be fetched. By default data in all supported languages is fetched.","content_length":666,"content_tokens":235,"embedding":[]}]},{"title":"Scaffolding & structure of data config settings | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/scaffolding","date":"2023-05-09T19:48:22.601Z","content":"On this page Introduction The data related settings of any project are all contained in config/data folder, in a nested hierarchy. The configLoader of GS_data reads the config/data path and loads all the configurations in a JSON for use by the gs_data module. Folder location ​ The data config folder is located within config folder of the project, just like all other configurations like exports, telemetry. . //Project root ./src ./actions //This will contain all the API contracts defined in this project by developer (For API schema driven development) ./config ./exports //The exported functions from ../src or imported modules ./data //Here will lie the data specific configurations read by the GS_data module. ./telemetry ... //So on and so forth for other modules Folder structure ​ The folder will contain the following information in heirarchy ./config/data index.(yaml | toml | json) //For common fields like supportedLanguages, defaultPrimaryDB, requestTimeout etc. ./schema ./entities entityA.(yaml | toml) entityB.(yaml | toml) ./relationships definitions.yaml | definitions.toml | definitions.gsl ./definitions // Alternatively from above line `${entityA} <> ${entityB}`.(gsl | yaml) // Keep all the relationships aggregated by the two entities. dataDependencies.gsl // All automatically aggregated fields with functions like union, copy and in future (average, max, min). denormalization.gsl // Copy of current elasticgraph's joins/index.txt ./performance ./batching //Or batching.yaml containing batch size and timeouts for different DBs of each kind elasticsearch.yaml postgres.yaml mongodb.yaml ./databases //Database settings elasticsearch.(yaml | toml) mongodb.(yaml | toml) postgres.(yaml | toml) ./environments dev.env staginv.env production.env Environment variables ​ There will also be variables definable in environment files, i.e. dev.env, staging.env and production.env files. These files will have some predefined variables, some empty variables. They will get their data replaced/filled by devops process as per the business use case.","tokens":483,"length":2065,"chunks":[{"doc_title":"Scaffolding & structure of data config settings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/scaffolding","doc_date":"2023-05-09T19:48:22.601Z","content":"On this page Introduction The data related settings of any project are all contained in config/data folder, in a nested hierarchy. The configLoader of GS_data reads the config/data path and loads all the configurations in a JSON for use by the gs_data module. Folder location ​ The data config folder is located within config folder of the project, just like all other configurations like exports, telemetry.  //Project root ./src ./actions //This will contain all the API contracts defined in this project by developer (For API schema driven development) ./config ./exports //The exported functions from ../src or imported modules ./data //Here will lie the data specific configurations read by the GS_data module. ./telemetry ..","content_length":730,"content_tokens":150,"embedding":[]},{"doc_title":"Scaffolding & structure of data config settings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/scaffolding","doc_date":"2023-05-09T19:48:22.601Z","content":"//So on and so forth for other modules Folder structure ​ The folder will contain the following information in heirarchy ./config/data index.(yaml | toml | json) //For common fields like supportedLanguages, defaultPrimaryDB, requestTimeout etc. ./schema ./entities entityA.(yaml | toml) entityB.(yaml | toml) ./relationships definitions.yaml | definitions.toml | definitions.gsl ./definitions // Alternatively from above line `${entityA} <> ${entityB}`.(gsl | yaml) // Keep all the relationships aggregated by the two entities. dataDependencies.gsl // All automatically aggregated fields with functions like union, copy and in future (average, max, min)","content_length":653,"content_tokens":162,"embedding":[]},{"doc_title":"Scaffolding & structure of data config settings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/scaffolding","doc_date":"2023-05-09T19:48:22.601Z","content":"denormalization.gsl // Copy of current elasticgraph's joins/index.txt ./performance ./batching //Or batching.yaml containing batch size and timeouts for different DBs of each kind elasticsearch.yaml postgres.yaml mongodb.yaml ./databases //Database settings elasticsearch.(yaml | toml) mongodb.(yaml | toml) postgres.(yaml | toml) ./environments dev.env staginv.env production.env Environment variables ​ There will also be variables definable in environment files, i.e. dev.env, staging.env and production.env files. These files will have some predefined variables, some empty variables. They will get their data replaced/filled by devops process as per the business use case.","content_length":677,"content_tokens":171,"embedding":[]}]},{"title":"Technologies used (Default) | Godspeed Docs","url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/technology-used/intro","date":"2023-05-09T19:48:22.738Z","content":"Technologies used (Default) Argo Events Argo Workflows Debezium Kafka","tokens":18,"length":69,"chunks":[{"doc_title":"Technologies used (Default) | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/data-at-flow-and-at-rest/technology-used/intro","doc_date":"2023-05-09T19:48:22.738Z","content":"Technologies used (Default) Argo Events Argo Workflows Debezium Kafka","content_length":69,"content_tokens":18,"embedding":[]}]},{"title":"Development Process | Godspeed Docs","url":"https://docs.godspeed.systems/docs/development-process","date":"2023-05-09T19:48:22.858Z","content":"On this page Development Process Developer culture practises and upskilling ​ Development environment ​ Dockerized Linux or Mac Development practices ​ Sprint planning Properly formatted issue/task creation read more Documentation Coding standards Test Coverage Git ops read more Diversity & cutting edge ​ Everything is better than the others for something Learn and adopt different languages, frameworks & technologies Be quick to discover and adapt new technologies and practices Developer portal & forum ​ Portal ​ Documentation of every product and microservice Coding standards Training modules Docker files and image links Git repository links Other useful links and reading material Recommended: Docusauras Developer forum ​ Informtaion exchange Fast communication Helping each other Recommended stack: Discourse and Slack DevOps Practices: ​ As part of our devops, we have automated our system & infra using Gitops, Kubernetes, ArgoCD, Crossplane. Our system infra automation can easily be integrated with existing or legacy apps. For example: video demo of springboot integration","tokens":197,"length":1089,"chunks":[{"doc_title":"Development Process | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/development-process","doc_date":"2023-05-09T19:48:22.858Z","content":"On this page Development Process Developer culture practises and upskilling ​ Development environment ​ Dockerized Linux or Mac Development practices ​ Sprint planning Properly formatted issue/task creation read more Documentation Coding standards Test Coverage Git ops read more Diversity & cutting edge ​ Everything is better than the others for something Learn and adopt different languages, frameworks & technologies Be quick to discover and adapt new technologies and practices Developer portal & forum ​ Portal ​ Documentation of every product and microservice Coding standards Training modules Docker files and image links Git repository links Other useful links and reading material Recommended: Docusauras Developer forum ​ Informtaion exchange Fast communication Helping each other Recommended stack: Discourse and Slack DevOps Practices: ​ As part of our devops, we have automated our system & infra using Gitops, Kubernetes, ArgoCD, Crossplane. Our system infra automation can easily be integrated with existing or legacy apps. For example: video demo of springboot integration","content_length":1089,"content_tokens":197,"embedding":[]}]},{"title":"COMMON FAQs | Godspeed Docs","url":"https://docs.godspeed.systems/docs/faq","date":"2023-05-09T19:48:23.013Z","content":"16. FAQ On this page COMMON FAQs Followings are collections of commonly asked questions with explanations. In future we will keep adding more questions/use cases/scenarios. 16.1 What is the learning curve of the microservice framework? ​ Our entire effort is to be a low code, easy to learn platform without too many things to learn, while getting big jobs done. A bunch of engineers have already trained and are developing microservices. Based on our data, it takes around 3~5 days for a young intern or engineer to get started on delivering enterprise level microservices. 16.2 What is the development process and quality metrics? ​ All our upgrades go through peer reviews and test coverage (80%). We follow feature based branching. As part of our CI workflow, a developer can't commit/merge to the dev/master branches, unless all the test cases are passing. This ensures continuous sanity checks of our main branches. Documentation and test coverage are integral parts of quality metrics. Code and image vulnerability scans are also followed to ensure security within the code and images. Story/Bug Life Cycle ​ tip Todo - Created by product owner or ticket creator. In Progress - By developer when they start the work. Documentation - By developer when they implement the feature. Code Review - By developer when they finish the documentation. QA - By a peer developer when they are testing the feature. Done - By person merging the pull request. 16.3 How can we adopt new versions of used technology easily and fast? For example, the new Postgres release. ​ Many times, the upgrades work with a simple update in package.json and updating the project . If at all a core framework update is needed, it is done as per the SLA. Security patches, fixes or feature inclusion will be part of the SLA itself. Irrespective of our SLAs, we also take an initiative to proactively support important integrations and upgrades from its side, and make it available to all clientele and potential users. The system will have default support for free and open source software. But based on client requirements, we can provide integrations for paid versions as well based on SLA and priority. If an upgrade has a license cost, it shall be borne by the client should it decide to use it. These changes can be done by the client team itself, because it will have proper documentation, access and right to modify source for its internal uses, and there will be minimum 80% test coverage with test automation, and KT/support by us (latter as per the SLA). 16.4 How easy is it to add new technology in place of an existing one, or add something absolutely new and unique (not existing in the framework)? ​ Since all the implementation is done against the open standards and pluggable interfaces, as long as the new technology is adhering to those standards drop-in replacement will be feasible. If the technology is introduced that does not adhere to the open standards, then some work will be needed to create adapters and avoid vendor lock-ins. But still, the integration will have to be compliant to the interfaces, for them to work, giving a uniformity of implementation and replacement. The modular architecture and modular design is plugin based, allowing for new integrations without much hassle. This can be done by the client itself, or by us, depending on our engagement. 16.5 Which databases are currently supported? What is the roadmap for future support? ​ We currently support Mongodb, Postgres, MySQL, SQLServer, SQLite, MariaDB, CockroachDB, AWS Aurora, Azure SQL via Prisma . We are in the process of adding Elasticsearch in Q2, 2022. 16.6 Does the API handle DB transactions? ​ Yes there is an extensive support for DB transactions . 16.7 How can apps be decoupled or loosely coupled with DBs? ​ This decoupling is possible because of the universal datastore schema, CRUD API and migration process. For more, please refer prisma docs . 16.8 When using Godspeed service alongside SpringBoot, what will be the impact on performance with another hop, versus direct connection with DB from Spring Boot? ​ The performance of an API endpoint depends on the service PLUS DB working together. For example, DB connection pooling and utilization, transaction handling, batching of independent queries, optimization of indexes and queries, denormalization (for cross table queries and aggregations), memoization/caching (for faster read and solving N+1 queries problem), CQRS setup between multiple DBs. Godspeed includes algorithms and best performance practices like the ones mentioned above. We are constantly striving to improve the performance. Next up is the feature of caching of output of workflow tasks and DB queries. 16.9 What is the strategic advantage of making DB queries through Godspeed? ​ First of all, the hop is completely optional. There are a few benefits of using this hop, however, including Become decoupled with the choice of database provider, so that if a DB changes ,the app code does not change. Low code configuration of CRUD service (saves effort of development, QA & maintenance) Data federation across multiple DBs and APIs. One can execute multiple queries to configured sources within a single query. 16.10 How to achieve multi-tenancy in DBs, for a single application? ​ It shall be done in two ways. By having separate DBs for every tenant. This will be costly but will be PCI compliant. It will also provide data isolation if needed for each tenant. By having a tenant_id in every row/document of every table/collection/index in the database.This will be cost effective and easy to maintain but the data across multiple tenants will be in a single database. 16.11 How can we start adopting the Godspeed framework? ​ Start by creating a new microservice in 10 minutes , referring our docs. Migration of existing microservices or monoliths: Generate CRUD APIs and workflows out of the box, by introspecting the database of an existing app, via the CLI. Then start customizing and using it as per your need. If you have existing Open API spec for a service, then you can generate the Godspeed event schema out of the box. (Coming soon) 16.12 How to move out of the Godspeed framework? Can we have a two door exit? I.e. Can we move out of technology and data both? ​ It is possible to opt out of the Godspeed framework without any kind of lock-in in which case all the microservices specific to the client can be developed using some other technology stack. The DBs can be self managed. The data will anyway be hosted on the client’s premise/cloud or its vendor’s cloud. The control of the data is subject to the client’s agreement with their respective cloud vendor, whose hosted database services are being used. But if the client uses self managed DBs, then they are fully in control of their data. This has got nothing to do with Godspeed. The framework comes with no lock-in of any kind and will never do so, as part of our philosophy. 16.13 How will we prevent unified CRUD API from limiting or choking us? ​ The framework, via Prisma, facilitates developers to access full functionality of any database or tool without being limited by the universal API. They shall be able to execute native database queries directly or via the API itself. 16.14 What kind of API standards does the framework support? ​ We currently support REST and planned to support GraphQL and gRPC in Q2, 2022. 16.15 Why Rest first approach ? Why not Graphql first approach? ​ Every existing Graphql server in the industry supports REST/JSON interface, custom DSL and along with it, a Graphql interface (Ex. DGraph, Hasura, Apollo, Postgraphile). We are also going the same route by first being REST/JSON based, custom DSL and then adding Graphql in future. This is primarily because of greater familiarity of the REST standard across industry. At the same time, our REST implementation brings some good concepts to include in the development methodology like the concept of giving power to the frontend team to decide what data they want in response, and to get data from multiple sources in one go. We are including the features of Graphql in our design. The foundation our API interface is the unified event schema which we plan to use to generate GraphQL API (planned for Q2, 2022) Having said that, we would like to add that the Graphql standard specification does not specify a few critical things, like “where clause”, “aggregations”, “filters in joins”, “specifying relationships in model”, search/suggest queries, custom annotations, how to migrate, code first or schema first approach, etc. Every vendor has its own flavour of Graphql implementation/API, and there is no compatibility or out-of-the-box interoperability between implementations from different vendors. If the client also implements Graphql by any vendor, including Godspeed, it will be still having its own unique flavor of implementation, and the concept of data federation will not work for consumers of the API, just out of the box, as it appears to be so in theory of Graphql data federation. It will need developers to write custom resolvers to federate request/response from multiple Graphql services. In short Graphql standard lacks standard and unified implementation across industry. Further, we believe based on our survey that the Graphql ecosystem is complex and difficult to learn and extend for the uninitiated, and most developers even today do not know Grapqhl. Those who know find it complex. It lacks bringing agility for a typical developer team who is more comfortable with REST/JSON. It already took banks years to move from XML to JSON. Expecting third party consumers to consume Graphql is another big ask, a few years ahead of time. Still, it's an new upcoming standard with its own benefits. We wish to roll out our own flavor in 2022. 16.16 How are we doing testing given there is quite a bit of custom DSL in the framework. How do we ensure the correctness? ​ DSL will not get loaded if it's not in the right format. We are planning to add language feature in VSCode for compile time checks. We have automated test suite generation through the event schema and the developer can add testing as a part of CI process. 16.17 How will the upgrades and migrations be done to the framework? ​ We follow a semantic release process using semantic version (semver) with autogenerated Changelog. The developers can change/upgrade the version of the framework for any microservice via the CLI . After this, they can run the test cases and confirm if everything goes well. 16.18 How CRUD APIs will support the paid as well as the non paid features of databases such as MongoDB. For example: MongoDB free vs paid versions will support different features. ​ The framework uses Prisma which already supports paid and non paid features of databases. 16.19 How to ship new models easily? ​ Prisma provides a standardized and widely used migration process, which can be used out of the box.","tokens":2335,"length":10978,"chunks":[{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"16. FAQ On this page COMMON FAQs Followings are collections of commonly asked questions with explanations. In future we will keep adding more questions/use cases/scenarios. 16.1 What is the learning curve of the microservice framework? ​ Our entire effort is to be a low code, easy to learn platform without too many things to learn, while getting big jobs done. A bunch of engineers have already trained and are developing microservices. Based on our data, it takes around 3~5 days for a young intern or engineer to get started on delivering enterprise level microservices. 16.2 What is the development process and quality metrics? ​ All our upgrades go through peer reviews and test coverage (80%) We follow feature based branching. As part of our CI workflow, a developer can't commit/merge to the dev/master branches, unless all the test cases are passing. This ensures continuous sanity checks of our main branches.","content_length":920,"content_tokens":192,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"Documentation and test coverage are integral parts of quality metrics. Code and image vulnerability scans are also followed to ensure security within the code and images. Story/Bug Life Cycle ​ tip Todo - Created by product owner or ticket creator. In Progress - By developer when they start the work. Documentation - By developer when they implement the feature. Code Review - By developer when they finish the documentation. QA - By a peer developer when they are testing the feature. Done - By person merging the pull request. 16.3 How can we adopt new versions of used technology easily and fast? For example, the new Postgres release. ​ Many times, the upgrades work with a simple update in package.json and updating the project  If at all a core framework update is needed, it is done as per the SLA. Security patches, fixes or feature inclusion will be part of the SLA itself.","content_length":883,"content_tokens":182,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"Irrespective of our SLAs, we also take an initiative to proactively support important integrations and upgrades from its side, and make it available to all clientele and potential users. The system will have default support for free and open source software. But based on client requirements, we can provide integrations for paid versions as well based on SLA and priority. If an upgrade has a license cost, it shall be borne by the client should it decide to use it. These changes can be done by the client team itself, because it will have proper documentation, access and right to modify source for its internal uses, and there will be minimum 80% test coverage with test automation, and KT/support by us (latter as per the SLA)","content_length":731,"content_tokens":152,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"16.4 How easy is it to add new technology in place of an existing one, or add something absolutely new and unique (not existing in the framework)? ​ Since all the implementation is done against the open standards and pluggable interfaces, as long as the new technology is adhering to those standards drop-in replacement will be feasible. If the technology is introduced that does not adhere to the open standards, then some work will be needed to create adapters and avoid vendor lock-ins. But still, the integration will have to be compliant to the interfaces, for them to work, giving a uniformity of implementation and replacement. The modular architecture and modular design is plugin based, allowing for new integrations without much hassle. This can be done by the client itself, or by us, depending on our engagement.","content_length":824,"content_tokens":165,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"16.5 Which databases are currently supported? What is the roadmap for future support? ​ We currently support Mongodb, Postgres, MySQL, SQLServer, SQLite, MariaDB, CockroachDB, AWS Aurora, Azure SQL via Prisma  We are in the process of adding Elasticsearch in Q2, 2022. 16.6 Does the API handle DB transactions? ​ Yes there is an extensive support for DB transactions  16.7 How can apps be decoupled or loosely coupled with DBs? ​ This decoupling is possible because of the universal datastore schema, CRUD API and migration process. For more, please refer prisma docs  16.8 When using Godspeed service alongside SpringBoot, what will be the impact on performance with another hop, versus direct connection with DB from Spring Boot? ​ The performance of an API endpoint depends on the service PLUS DB working together.","content_length":817,"content_tokens":186,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"For example, DB connection pooling and utilization, transaction handling, batching of independent queries, optimization of indexes and queries, denormalization (for cross table queries and aggregations), memoization/caching (for faster read and solving N+1 queries problem), CQRS setup between multiple DBs. Godspeed includes algorithms and best performance practices like the ones mentioned above. We are constantly striving to improve the performance. Next up is the feature of caching of output of workflow tasks and DB queries. 16.9 What is the strategic advantage of making DB queries through Godspeed? ​ First of all, the hop is completely optional. There are a few benefits of using this hop, however, including Become decoupled with the choice of database provider, so that if a DB changes ,the app code does not change. Low code configuration of CRUD service (saves effort of development, QA & maintenance) Data federation across multiple DBs and APIs.","content_length":961,"content_tokens":195,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"One can execute multiple queries to configured sources within a single query. 16.10 How to achieve multi-tenancy in DBs, for a single application? ​ It shall be done in two ways. By having separate DBs for every tenant. This will be costly but will be PCI compliant. It will also provide data isolation if needed for each tenant. By having a tenant_id in every row/document of every table/collection/index in the database.This will be cost effective and easy to maintain but the data across multiple tenants will be in a single database. 16.11 How can we start adopting the Godspeed framework? ​ Start by creating a new microservice in 10 minutes , referring our docs. Migration of existing microservices or monoliths: Generate CRUD APIs and workflows out of the box, by introspecting the database of an existing app, via the CLI. Then start customizing and using it as per your need.","content_length":884,"content_tokens":195,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"If you have existing Open API spec for a service, then you can generate the Godspeed event schema out of the box. (Coming soon) 16.12 How to move out of the Godspeed framework? Can we have a two door exit? I.e. Can we move out of technology and data both? ​ It is possible to opt out of the Godspeed framework without any kind of lock-in in which case all the microservices specific to the client can be developed using some other technology stack. The DBs can be self managed. The data will anyway be hosted on the client’s premise/cloud or its vendor’s cloud. The control of the data is subject to the client’s agreement with their respective cloud vendor, whose hosted database services are being used. But if the client uses self managed DBs, then they are fully in control of their data. This has got nothing to do with Godspeed.","content_length":834,"content_tokens":192,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"The framework comes with no lock-in of any kind and will never do so, as part of our philosophy. 16.13 How will we prevent unified CRUD API from limiting or choking us? ​ The framework, via Prisma, facilitates developers to access full functionality of any database or tool without being limited by the universal API. They shall be able to execute native database queries directly or via the API itself. 16.14 What kind of API standards does the framework support? ​ We currently support REST and planned to support GraphQL and gRPC in Q2, 2022. 16.15 Why Rest first approach ? Why not Graphql first approach? ​ Every existing Graphql server in the industry supports REST/JSON interface, custom DSL and along with it, a Graphql interface (Ex. DGraph, Hasura, Apollo, Postgraphile) We are also going the same route by first being REST/JSON based, custom DSL and then adding Graphql in future.","content_length":891,"content_tokens":198,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"This is primarily because of greater familiarity of the REST standard across industry. At the same time, our REST implementation brings some good concepts to include in the development methodology like the concept of giving power to the frontend team to decide what data they want in response, and to get data from multiple sources in one go. We are including the features of Graphql in our design. The foundation our API interface is the unified event schema which we plan to use to generate GraphQL API (planned for Q2, 2022) Having said that, we would like to add that the Graphql standard specification does not specify a few critical things, like “where clause”, “aggregations”, “filters in joins”, “specifying relationships in model”, search/suggest queries, custom annotations, how to migrate, code first or schema first approach, etc.","content_length":842,"content_tokens":182,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"Every vendor has its own flavour of Graphql implementation/API, and there is no compatibility or out-of-the-box interoperability between implementations from different vendors. If the client also implements Graphql by any vendor, including Godspeed, it will be still having its own unique flavor of implementation, and the concept of data federation will not work for consumers of the API, just out of the box, as it appears to be so in theory of Graphql data federation. It will need developers to write custom resolvers to federate request/response from multiple Graphql services. In short Graphql standard lacks standard and unified implementation across industry. Further, we believe based on our survey that the Graphql ecosystem is complex and difficult to learn and extend for the uninitiated, and most developers even today do not know Grapqhl. Those who know find it complex. It lacks bringing agility for a typical developer team who is more comfortable with REST/JSON.","content_length":979,"content_tokens":196,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"It already took banks years to move from XML to JSON. Expecting third party consumers to consume Graphql is another big ask, a few years ahead of time. Still, it's an new upcoming standard with its own benefits. We wish to roll out our own flavor in 2022. 16.16 How are we doing testing given there is quite a bit of custom DSL in the framework. How do we ensure the correctness? ​ DSL will not get loaded if it's not in the right format. We are planning to add language feature in VSCode for compile time checks. We have automated test suite generation through the event schema and the developer can add testing as a part of CI process. 16.17 How will the upgrades and migrations be done to the framework? ​ We follow a semantic release process using semantic version (semver) with autogenerated Changelog. The developers can change/upgrade the version of the framework for any microservice via the CLI","content_length":903,"content_tokens":197,"embedding":[]},{"doc_title":"COMMON FAQs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/faq","doc_date":"2023-05-09T19:48:23.013Z","content":"After this, they can run the test cases and confirm if everything goes well. 16.18 How CRUD APIs will support the paid as well as the non paid features of databases such as MongoDB. For example: MongoDB free vs paid versions will support different features. ​ The framework uses Prisma which already supports paid and non paid features of databases. 16.19 How to ship new models easily? ​ Prisma provides a standardized and widely used migration process, which can be used out of the box.","content_length":488,"content_tokens":103,"embedding":[]}]},{"title":"Application deployment procedure | Godspeed Docs","url":"https://docs.godspeed.systems/docs/infra-and-system/Application","date":"2023-05-09T19:48:23.205Z","content":"On this page Application deployment procedure Prerequisite ​ Kubernetes Cluster ​ ArgoCD, Argo events and argo workflow installed on kubernetes cluster ​ Git repo with CI and application manifests ​ Spring boot application already containerized ​ Docker registry ​ Step 1 ​ Cloning the git repo ​ $ https://github.com/Mindgreppers/demo-k8s-manifests.git Step 2 ​ Create event source with ingress for the webhook and git secret token ​ $ cd demo-k8s-manifests/CI-manifests/ $ kubectl create secret generic git-credentials \\ --from-literal=GIT_TOKEN=TOKEN $ kubectl create -f spring-webhook.yaml Step 3 ​ Create workflow template containing CI steps ​ $ kubectl create -f CI-workflow-templates.yaml Step 4 ​ Create CI/CD for the master branch ​ $ kubectl create -f spring-master-CI.yaml Step 5 ​ Create CI/CD for the non master branch ​ $ kubectl create -f spring-non-master-CI.yaml Step 6 ​ Configure the Argo application to sync the git repo folder with kubernetes ​ $ argocd app create spring-app --repo https://github.com/Mindgreppers/demo-k8s-manifests.git --path spring-app --dest-namespace demo --dest-server https://kubernetes.default.svc --directory-recurse --sync-policy auto Step 7 ​ Add the webhook address in the spring boot git repo under webhooks ​ every push will have a CI trigger the application build. The deployment is done for the master branch only. ​","tokens":382,"length":1371,"chunks":[{"doc_title":"Application deployment procedure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/Application","doc_date":"2023-05-09T19:48:23.205Z","content":"","content_length":0,"content_tokens":0,"embedding":[]},{"doc_title":"Application deployment procedure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/Application","doc_date":"2023-05-09T19:48:23.205Z","content":"On this page Application deployment procedure Prerequisite ​ Kubernetes Cluster ​ ArgoCD, Argo events and argo workflow installed on kubernetes cluster ​ Git repo with CI and application manifests ​ Spring boot application already containerized ​ Docker registry ​ Step 1 ​ Cloning the git repo ​ $ https://github.com/Mindgreppers/demo-k8s-manifests.git Step 2 ​ Create event source with ingress for the webhook and git secret token ​ $ cd demo-k8s-manifests/CI-manifests/ $ kubectl create secret generic git-credentials \\ --from-literal=GIT_TOKEN=TOKEN $ kubectl create -f spring-webhook.yaml Step 3 ​ Create workflow template containing CI steps ​ $ kubectl create -f CI-workflow-templates.yaml Step 4 ​ Create CI/CD for the master branch ​ $ kubectl create -f spring-master-CI.yaml Step 5 ​ Create CI/CD for the non master branch ​ $ kubectl create -f spring-non-master-CI.yaml Step 6 ​ Configure the Argo application to sync the git repo folder with kubernetes ​ $ argocd app create spring-app --repo https://github.com/Mindgreppers/demo-k8s-manifests.git --path spring-app --dest-namespace demo --dest-server https://kubernetes.default.svc --directory-recurse --sync-policy auto Step 7 ​ Add the webhook address in the spring boot git repo under webhooks ​ every push will have a CI trigger the application build. The deployment is done for the master branch only. ​","content_length":1370,"content_tokens":382,"embedding":[]}]},{"title":"Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/infra-and-system/intro","date":"2023-05-09T19:48:23.336Z","content":"On this page Introduction The developer will need to provide abstracted, templated configurations for all the infra and system-level setup. They should not need to learn any underlying technologies for dev ops or automation. The platform will handle the provisioning, securing, configuring, health, scaling, failover and management of the infra and system, in an automated way. The platform will deploy & manage the lifecycle of the following components and functionalities: hardware spanning multiple cloud providers (or on-premise), operating system, network, tools, libraries, data stores, gateways, microservice mesh, message bus, observability, CI/CD, microservices (domain & functional), pipelines & workflows. Technologies used by default Crossplane ArgoCD (delivery) Linkerd (service mesh) OpenTelemetry (for standardized tracing and monitoring via Jaeger and Prometheus) Elasticsearch/Kibana /Fluentd (Log collection, transformation, dashboard, alerts) Jaeger/Elasticsearch (Tracing) Prometheus/Grafana or Elasticsearch APM (metrics collection, monitoring & alerts) Proposed opensource framework for Auth - ORY Kratos or Keycloak (IAM), ORY OATHKEEPER (For gateway authorization) Salient Features ​ Infrastructure as code ​ We will use declarative YAML configuration for Crossplane. for seamless integration with CI/CD pipelines to have a single source for infra configuration. Cloud federation & vendor independence ​ The deployment will be done using the Kubernetes cluster with Crossplane that provides extensive support for cloud providers such as AWS, GCP, Azure and others, along with on-premise. Using this one can not only be vendor-independent but also operate across multiple vendors. Application portability ​ The API centric control plane is directly used by the application teams to deploy their changes easily, with the least involvement of the devOps team. The configurations & APIs hide infrastructure complexity & reduce learning curve, hence empowering the dev teams to develop, deploy or shift auto-managed applications on the fly. Gitops and CI/CD based provisioning and configuring ​ All the dev team needs to make a commit to the git repo. The intended changes(as per the diff in the configuration files and code) should get automatically tested & deployed in production, using CI/CD and canary or blue-green deployment approaches. The pull request will need to be accepted by the stakeholders including but not limited to the product manager, engineering head, QA lead. Every commit to git will trigger unit and integration tests whether for infra, microservice or serverless function deployments. In case of test failures, the latest commit should fail to deploy in production. Deployments will not hinder the ongoing service at any time, ensuring high availability. We will use ArgoCD for the same Observability stack ​ The stack for observability will be provided out of the box, with certain functionality preset and working without developer intervention. Logging ​ There will be provision for centralized logs stored into Elasticsearch via Fluentd, & visualized via Kibana or Grafana. It is to be used by the microservice framework for fundamental request/response/error logging and by developers for business-level logging. Monitoring ​ All the application performance metrics (uptime, CPU, RAM, sync request latency, sync request success/failure) will be automatically stored & monitored centrally via the service mesh. The latency/success/failure metric of the async requests will be stored by the microservice framework. The data will be collected in Prometheus; and the dashboard will be rendered in Grafana. Tracing ​ Jaeger, along with Elasticsearch backend will be used to collect the trace information for every request. Traces will be collected across both sync and async flows. Every incoming HTTP or async request will carry trace information in its headers. The same will be propagated further through the microservice framework when it makes a sync or async hit to another service. In case of sync hits which shall be routed via the service mesh, the mesh will store the tracing information in the tracing backend. Async hit tracing will be the responsibility of the microservice framework itself, when it will consume an async message. The standard event format being used will carry the tracing headers. Alerting ​ Grafana can be used to configure alerts based on the monitored metrics. Authentication ​ ORY Kratos will be used to provide authentication and role management service out of the box. But the system is not confined to using ORY Kratos only. The platform users can use any IAM service they wish to use, and it shall be made to work with the remaining system and microservice framework. Scaffolding ​ A microservice project structure will be auto-generated from the CLI. There will be a scaffolding mechanism through CLI which will generate the project structure for the custom business logic (route, controller, api definition, validation, authorization, data model) of a custom microservice. The framework will also provide a configuration template that can be customized as per the development need. Dashboards available ​ Follwoing dashboards will be available: Service & function dashboard: ​ All the services will be managed using OpenFAAS which provides an infra agnostic way of deploying services and functions using Docker over Kubernetes. Data dashboard: ​ It will allow the admins or team to search, view and edit data in the DB of any microservice. Monitoring dashboard: ​ It will be used to monitor the state of the live production environment. We plan to use Grafana for the same. Workflows dashboard: ​ Monitor the workflows running or erroring out in the system. For example, ETLs, CI/CD, scheduled jobs, triggered workflows. This will be provided by the tool used underneath.For example, ARGO workflow. Analytics dashboard: ​ A dashboard to view & download statistics, graphs and reports will be available. Also other dashboards available in open source can be adopted to work with the same data. Developer workflow in action ​ The following diagram gives an detailed overview of the workflow. A SAMPLE SETUP ​ IMPORTANT NOTE - While Godspeed is providing some standard tools as part of the platform, developers can integrate other tools to cover the same functionality as per the defined API contracts. Because we are using standard protocols like CloudEvents, OpenTelemetry, unified CRUD API, they can integrate any database, cache, message bus, APM/BPM tools, as long as the same API contract is maintained. Most of the tools mentioned below are replaceable as a platform goal. The only exceptions will be tools like Crossplane & Nodejs which are very fundamental to our platform and microservice framework respectively. All the dev team needs to do is to make a commit to the git repo and the intended changes as per the diff in the configuration files and code, should get automagically tested & deployed in production, using CI/CD and canary or blue-green deployment approaches.The pull request will need to be accepted by the stakeholders including but not limited to product manager, engineering head, QA lead. Every commit to git will trigger unit and integration tests whether for infra, microservice or serverless function deployments. In case of test failing, the latest commit should fail to deploy in production. Deployments will not hinder the ongoing service at any time, ensuring high availability. We will use ArgoCD for the same","tokens":1529,"length":7530,"chunks":[{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"On this page Introduction The developer will need to provide abstracted, templated configurations for all the infra and system-level setup. They should not need to learn any underlying technologies for dev ops or automation. The platform will handle the provisioning, securing, configuring, health, scaling, failover and management of the infra and system, in an automated way. The platform will deploy & manage the lifecycle of the following components and functionalities: hardware spanning multiple cloud providers (or on-premise), operating system, network, tools, libraries, data stores, gateways, microservice mesh, message bus, observability, CI/CD, microservices (domain & functional), pipelines & workflows.","content_length":716,"content_tokens":146,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"Technologies used by default Crossplane ArgoCD (delivery) Linkerd (service mesh) OpenTelemetry (for standardized tracing and monitoring via Jaeger and Prometheus) Elasticsearch/Kibana /Fluentd (Log collection, transformation, dashboard, alerts) Jaeger/Elasticsearch (Tracing) Prometheus/Grafana or Elasticsearch APM (metrics collection, monitoring & alerts) Proposed opensource framework for Auth - ORY Kratos or Keycloak (IAM), ORY OATHKEEPER (For gateway authorization) Salient Features ​ Infrastructure as code ​ We will use declarative YAML configuration for Crossplane. for seamless integration with CI/CD pipelines to have a single source for infra configuration.","content_length":669,"content_tokens":157,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"Cloud federation & vendor independence ​ The deployment will be done using the Kubernetes cluster with Crossplane that provides extensive support for cloud providers such as AWS, GCP, Azure and others, along with on-premise. Using this one can not only be vendor-independent but also operate across multiple vendors. Application portability ​ The API centric control plane is directly used by the application teams to deploy their changes easily, with the least involvement of the devOps team. The configurations & APIs hide infrastructure complexity & reduce learning curve, hence empowering the dev teams to develop, deploy or shift auto-managed applications on the fly. Gitops and CI/CD based provisioning and configuring ​ All the dev team needs to make a commit to the git repo. The intended changes(as per the diff in the configuration files and code) should get automatically tested & deployed in production, using CI/CD and canary or blue-green deployment approaches.","content_length":975,"content_tokens":192,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"The pull request will need to be accepted by the stakeholders including but not limited to the product manager, engineering head, QA lead. Every commit to git will trigger unit and integration tests whether for infra, microservice or serverless function deployments. In case of test failures, the latest commit should fail to deploy in production. Deployments will not hinder the ongoing service at any time, ensuring high availability. We will use ArgoCD for the same Observability stack ​ The stack for observability will be provided out of the box, with certain functionality preset and working without developer intervention. Logging ​ There will be provision for centralized logs stored into Elasticsearch via Fluentd, & visualized via Kibana or Grafana. It is to be used by the microservice framework for fundamental request/response/error logging and by developers for business-level logging.","content_length":899,"content_tokens":176,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"Monitoring ​ All the application performance metrics (uptime, CPU, RAM, sync request latency, sync request success/failure) will be automatically stored & monitored centrally via the service mesh. The latency/success/failure metric of the async requests will be stored by the microservice framework. The data will be collected in Prometheus; and the dashboard will be rendered in Grafana. Tracing ​ Jaeger, along with Elasticsearch backend will be used to collect the trace information for every request. Traces will be collected across both sync and async flows. Every incoming HTTP or async request will carry trace information in its headers. The same will be propagated further through the microservice framework when it makes a sync or async hit to another service. In case of sync hits which shall be routed via the service mesh, the mesh will store the tracing information in the tracing backend. Async hit tracing will be the responsibility of the microservice framework itself, when it will consume an async message.","content_length":1025,"content_tokens":201,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"The standard event format being used will carry the tracing headers. Alerting ​ Grafana can be used to configure alerts based on the monitored metrics. Authentication ​ ORY Kratos will be used to provide authentication and role management service out of the box. But the system is not confined to using ORY Kratos only. The platform users can use any IAM service they wish to use, and it shall be made to work with the remaining system and microservice framework. Scaffolding ​ A microservice project structure will be auto-generated from the CLI. There will be a scaffolding mechanism through CLI which will generate the project structure for the custom business logic (route, controller, api definition, validation, authorization, data model) of a custom microservice. The framework will also provide a configuration template that can be customized as per the development need.","content_length":879,"content_tokens":174,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"Dashboards available ​ Follwoing dashboards will be available: Service & function dashboard: ​ All the services will be managed using OpenFAAS which provides an infra agnostic way of deploying services and functions using Docker over Kubernetes. Data dashboard: ​ It will allow the admins or team to search, view and edit data in the DB of any microservice. Monitoring dashboard: ​ It will be used to monitor the state of the live production environment. We plan to use Grafana for the same. Workflows dashboard: ​ Monitor the workflows running or erroring out in the system. For example, ETLs, CI/CD, scheduled jobs, triggered workflows. This will be provided by the tool used underneath.For example, ARGO workflow. Analytics dashboard: ​ A dashboard to view & download statistics, graphs and reports will be available. Also other dashboards available in open source can be adopted to work with the same data.","content_length":910,"content_tokens":196,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"Developer workflow in action ​ The following diagram gives an detailed overview of the workflow. A SAMPLE SETUP ​ IMPORTANT NOTE - While Godspeed is providing some standard tools as part of the platform, developers can integrate other tools to cover the same functionality as per the defined API contracts. Because we are using standard protocols like CloudEvents, OpenTelemetry, unified CRUD API, they can integrate any database, cache, message bus, APM/BPM tools, as long as the same API contract is maintained. Most of the tools mentioned below are replaceable as a platform goal. The only exceptions will be tools like Crossplane & Nodejs which are very fundamental to our platform and microservice framework respectively.","content_length":726,"content_tokens":144,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/intro","doc_date":"2023-05-09T19:48:23.336Z","content":"All the dev team needs to do is to make a commit to the git repo and the intended changes as per the diff in the configuration files and code, should get automagically tested & deployed in production, using CI/CD and canary or blue-green deployment approaches.The pull request will need to be accepted by the stakeholders including but not limited to product manager, engineering head, QA lead. Every commit to git will trigger unit and integration tests whether for infra, microservice or serverless function deployments. In case of test failing, the latest commit should fail to deploy in production. Deployments will not hinder the ongoing service at any time, ensuring high availability. We will use ArgoCD for the same.","content_length":724,"content_tokens":146,"embedding":[]}]},{"title":"Technologies used (Default) | Godspeed Docs","url":"https://docs.godspeed.systems/docs/infra-and-system/technology-used/intro","date":"2023-05-09T19:48:23.489Z","content":"Technologies used (Default) Crossplane ArgoCD (delivery) Linkerd (service mesh) OpenTelemetry (for standardized tracing and monitoring via Jaeger and Prometheus) Elasticsearch / Kibana / Fluentd (Log collection, transformation, dashboard, alerts) Jaeger / Elasticsearch (Tracing) Prometheus / Grafana or Elasticsearch APM (metrics collection, monitoring & alerts) ORY Kratos or Keycloak (IAM) , ORY OATHKEEPER (For gateway authorization)","tokens":110,"length":437,"chunks":[{"doc_title":"Technologies used (Default) | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/infra-and-system/technology-used/intro","doc_date":"2023-05-09T19:48:23.489Z","content":"Technologies used (Default) Crossplane ArgoCD (delivery) Linkerd (service mesh) OpenTelemetry (for standardized tracing and monitoring via Jaeger and Prometheus) Elasticsearch / Kibana / Fluentd (Log collection, transformation, dashboard, alerts) Jaeger / Elasticsearch (Tracing) Prometheus / Grafana or Elasticsearch APM (metrics collection, monitoring & alerts) ORY Kratos or Keycloak (IAM) , ORY OATHKEEPER (For gateway authorization)","content_length":437,"content_tokens":110,"embedding":[]}]},{"title":"Local Dev Setup | Godspeed Docs","url":"https://docs.godspeed.systems/docs/local-development-setup/install%20the%20docker","date":"2023-05-09T19:48:23.630Z","content":"On this page Local Dev Setup Local Debian Setup ​ 1: Update the apt package index and install packages to allow apt to use a repository over HTTPS: ​ $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2: Add Docker’s official GPG key: ​ $ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 3: Adding docker apt repository: ​ $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null 4: Installing docker package using apt: ​ $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io -y 5. Giving permission to user for docker and reboot the system ​ $ sudo usermod -aG docker $USER $ reboot 6: Verifying docker installation and working: ​ $ sudo docker run hello-world Docker install on Ubuntu ​ 1: Update the apt package index and install packages to allow apt to use a repository over HTTPS: ​ $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2: Add Docker’s official GPG key: ​ $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 3: Adding docker apt repository: ​ $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null 4: Installing docker package using apt: ​ $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io -y 5. Giving permission to user for docker and reboot the system ​ $ sudo usermod -aG docker $USER $ reboot 6: Verifying docker installation and working: ​ $ sudo docker run hello-world Install docker on windows ​ To download docker binary Click here ​ Install docker on Mac ​ To download docker binary Click here ​ Install docker compose To download docker binary Click here ​ creating folder under home directory:- example: supports-app copy docker compose file in directory:- version: \"3.0\" services: zookeeper: image: confluentinc/cp-zookeeper:7.0.1 environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - 2181:2181 kafka: image: confluentinc/cp-kafka:7.0.1 depends_on: - zookeeper ports: - 29092:29092 - 9092:9092 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 elasticsearch: container_name: es01 image: elasticsearch:7.4.2 environment: - node.name=es01 - discovery.type=single-node - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - /home/gurjot/elasticsearch/es-data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - elastic restart: always networks: elastic: driver: bridge create a folder under file with name: es-data edit the path of esdata:- ~/support-apps/es-data.","tokens":1042,"length":3327,"chunks":[{"doc_title":"Local Dev Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/local-development-setup/install%20the%20docker","doc_date":"2023-05-09T19:48:23.630Z","content":"","content_length":0,"content_tokens":0,"embedding":[]},{"doc_title":"Local Dev Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/local-development-setup/install%20the%20docker","doc_date":"2023-05-09T19:48:23.630Z","content":"On this page Local Dev Setup Local Debian Setup ​ 1: Update the apt package index and install packages to allow apt to use a repository over HTTPS: ​ $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2: Add Docker’s official GPG key: ​ $ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 3: Adding docker apt repository: ​ $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null 4: Installing docker package using apt: ​ $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io -y 5.","content_length":808,"content_tokens":251,"embedding":[]},{"doc_title":"Local Dev Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/local-development-setup/install%20the%20docker","doc_date":"2023-05-09T19:48:23.630Z","content":"Giving permission to user for docker and reboot the system ​ $ sudo usermod -aG docker $USER $ reboot 6: Verifying docker installation and working: ​ $ sudo docker run hello-world Docker install on Ubuntu ​ 1: Update the apt package index and install packages to allow apt to use a repository over HTTPS: ​ $ sudo apt-get update $ sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 2: Add Docker’s official GPG key: ​ $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 3: Adding docker apt repository: ​ $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null 4: Installing docker package using apt: ​ $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io -y 5.","content_length":965,"content_tokens":286,"embedding":[]},{"doc_title":"Local Dev Setup | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/local-development-setup/install%20the%20docker","doc_date":"2023-05-09T19:48:23.630Z","content":"Giving permission to user for docker and reboot the system ​ $ sudo usermod -aG docker $USER $ reboot 6: Verifying docker installation and working: ​ $ sudo docker run hello-world Install docker on windows ​ To download docker binary Click here ​ Install docker on Mac ​ To download docker binary Click here ​ Install docker compose To download docker binary Click here ​ creating folder under home directory:- example: supports-app copy docker compose file in directory:- version: \"3.0\" services: zookeeper: image: confluentinc/cp-zookeeper:7.0.1 environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - 2181:2181 kafka: image: confluentinc/cp-kafka:7.0.1 depends_on: - zookeeper ports: - 29092:29092 - 9092:9092 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 elasticsearch: container_name: es01 image: elasticsearch:7.4.2 environment: - node.name=es01 - discovery.type=single-node - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - /home/gurjot/elasticsearch/es-data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - elastic restart: always networks: elastic: driver: bridge create a folder under file with name: es-data edit the path of esdata:- ~/support-apps/es-data.","content_length":1552,"content_tokens":505,"embedding":[]}]},{"title":"Authentication & Authorization | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/authen-author","date":"2023-05-09T19:48:23.812Z","content":"12. Authentication & Authorization On this page Authentication & Authorization 12.1 Authentication ​ The framework provides JWT authentication for securely transmitting information among microservices. The user agent should send the JWT in the Authorization header using the Bearer schema. The content of the header should look like the following: Authorization: Bearer <token> 12.1.1 JWT Configuration ​ You can do JWT configuration in Configuration/Environment variables . For example, this is the sample configuration: jwt: issuer: JWT_ISS audience: JWT_AUD secretOrKey: JWT_SECRET You need to export these environment variables in your environment. 12.1.1.1 Access JWT payload in Workflow DSL ​ You can access the complete JWT payload in <% inputs.user %> in workflow DSL as given below: summary : Call an API and transform the tasks : - id : httpbin_step1 description : Hit http bin with some dummy data. It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > jwt_payload : <% inputs.user % > config : url : /anything method : post 12.1.2 Event spec ​ Add authn: true in the event DSL to enable authentication for any event. /v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post: authn: true fn: com.biz.kyc.ckyc.ckyc_initiate on_validation_error: com.jfs.handle_validation_error data: schema: body: required: true content: application/json: schema: type: 'object' required: [] properties: dob: { type : 'string', format : 'date', pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta: type: 'object' params: - name: lender_loan_application_id in: params required: true allow_empty_value: false schema: type: string responses: #Output data defined as per the OpenAPI spec 200: schema: data: required: # default value is false content: application/json: schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id] 12.1.3 Generate JWT ​ Generally, you will get JWT from your authentication service. For testing purposes, you can generate JWT at https://jwt.io/ by providing the iss , aud and secretOrKey to verify signature. Use the encoded token as JWT authentication token. For example, In the above case, the Authorization header should look like: Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJtcy5zYW1wbGUuY29tIiwiYXVkIjoic2FtcGxlLmNvbSJ9._1fpM6VYq1rfKdTEqi8BcPTm8KIm4cNP8VhX0kQOEts 12.1.4 Datasource authentication ​ You can add authentication at datasource level on API datasource . You can define an authn workflow at datasource level which requests to any authentication service for token/authentication then this workflow can return headers, params or statusCodes to the main workflow. Here is the sample spec: Datasource type : api base_url : <% config.httpbin.base_url % > authn : com.jfs.httpbin_auth Here, com.jfs.httpbin_auth is the authentication workflow which gets called for the authentication of any request to this datasource. Sample workflow using the above datasource summary : Call an API and transform the tasks : - id : httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Hit http bin with some dummy data. It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > config : url : /anything method : post Sample authentication workflow com.jfs.httpbin_auth summary : Auth workflow tasks : - id : auth_step1 description : Hit the authn request fn : com.gs.http args : datasource : authapi data : <% inputs.query.username % > config : url : /authenticate method : post - id : auth_step2 description : Transform the response received from authn api fn : com.gs.transform args : headers : Authorization : <% 'Bearer ' + outputs.auth_step1.auth.token % > params : queryid : <% outputs.auth_step1.params.queryid % > statusCodes : <% outputs.auth_step1.status_code % > The authentication workflow should return response in this format: headers : header1 : val1 params : param1 : val1 statusCodes : [ 401 , 403 , ... . ] note The authentication workflow gets called when any request returns the specified statusCodes . 12.2 Authorization ​ The framework provides authorization, to verify if any event/model is authorized to access specific information or is allowed to execute certain actions. 12.2.1 Workflow DSL ​ You can add authorization workflow at the task level in any workflow. The authorization workflow should return allow/deny or json output to the main worklfow. Allow/Deny If authz workflow returns data as true/false, it means the task is allowed/denied to get executed. JSON output If authz workflow returns JSON output then it is merged with args.data of the task for which authz is being executed. Here is the sample spec: Sample workflow calling the authz workflow summary : Call an API tasks : - id : httpbin_step1 description : Hit http bin with some dummy data. It will send back same as response authz : fn : com.jfs.authz args : <% inputs % > fn : com.gs.http args : datasource : httpbin data : <% inputs % > config : url : /anything method : post Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: true } else if outputs.authz_step1.data.code == 201 then { success: true data: where: role: 'USER' } else { success: false data: false } %> The authorization workflow should return response in this format to allow/deny: success : true/false data : true/false/JSON output When data is returned as false i.e. deny then the framework will send 403 Unauthorized response. 12.2.2 Sample DB query call authorization ​ In DB query call, authz workflow can return JSON output with where clause, include clause etc. which will be merged with the args of the main workflow which is doing DB query. Here is the sample spec: Sample workflow calling the authz workflow summary : datastore demo tasks : - id : find_user description : find users authz : fn : com.jfs.auth args : <% inputs % > fn : com.gs.datastore args : datasource : mongo data : include : <% inputs.body.include % > where : <% inputs.body.where % > config : method : user.findMany Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: where: role: 'USER' } else { success: false data: false } %> When authorization workflow com.jfs.authz returns success: true then its data will be merged with the main workflow which is calling the authz workflow. For example, in the above authz workflow, data is returned as: data : where : role : 'USER' This data will be merged with the args.data of the main workflow i.e. args : data : include : <% inputs.body.include % > where : <% inputs.body.where % > # where clause from authz workflow will be merged with this","tokens":1965,"length":7489,"chunks":[{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"12. Authentication & Authorization On this page Authentication & Authorization 12.1 Authentication ​ The framework provides JWT authentication for securely transmitting information among microservices. The user agent should send the JWT in the Authorization header using the Bearer schema. The content of the header should look like the following: Authorization: Bearer <token> 12.1.1 JWT Configuration ​ You can do JWT configuration in Configuration/Environment variables  For example, this is the sample configuration: jwt: issuer: JWT_ISS audience: JWT_AUD secretOrKey: JWT_SECRET You need to export these environment variables in your environment. 12.1.1.1 Access JWT payload in Workflow DSL ​ You can access the complete JWT payload in <% inputs.user %> in workflow DSL as given below: summary : Call an API and transform the tasks : - id : httpbin_step1 description : Hit http bin with some dummy data. It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > jwt_payload : <% inputs.user % > config : url : /anything method : post 12.1.2 Event spec ​ Add authn: true in the event DSL to enable authentication for any event.","content_length":1178,"content_tokens":274,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"/v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post: authn: true fn: com.biz.kyc.ckyc.ckyc_initiate on_validation_error: com.jfs.handle_validation_error data: schema: body: required: true content: application/json: schema: type: 'object' required: [] properties: dob: { type : 'string', format : 'date', pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta: type: 'object' params: - name: lender_loan_application_id in: params required: true allow_empty_value: false schema: type: string responses: #Output data defined as per the OpenAPI spec 200: schema: data: required: # default value is false content: application/json: schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id] 12.1.3 Generate JWT ​ Generally, you will get JWT from your authentication service.","content_length":843,"content_tokens":262,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"For testing purposes, you can generate JWT at https://jwt.io/ by providing the iss , aud and secretOrKey to verify signature. Use the encoded token as JWT authentication token. For example, In the above case, the Authorization header should look like: Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJtcy5zYW1wbGUuY29tIiwiYXVkIjoic2FtcGxlLmNvbSJ9._1fpM6VYq1rfKdTEqi8BcPTm8KIm4cNP8VhX0kQOEts 12.1.4 Datasource authentication ​ You can add authentication at datasource level on API datasource","content_length":511,"content_tokens":190,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"You can define an authn workflow at datasource level which requests to any authentication service for token/authentication then this workflow can return headers, params or statusCodes to the main workflow. Here is the sample spec: Datasource type : api base_url : <% config.httpbin.base_url % > authn : com.jfs.httpbin_auth Here, com.jfs.httpbin_auth is the authentication workflow which gets called for the authentication of any request to this datasource. Sample workflow using the above datasource summary : Call an API and transform the tasks : - id : httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Hit http bin with some dummy data.","content_length":712,"content_tokens":163,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > config : url : /anything method : post Sample authentication workflow com.jfs.httpbin_auth summary : Auth workflow tasks : - id : auth_step1 description : Hit the authn request fn : com.gs.http args : datasource : authapi data : <% inputs.query.username % > config : url : /authenticate method : post - id : auth_step2 description : Transform the response received from authn api fn : com.gs.transform args : headers : Authorization : <% 'Bearer ' + outputs.auth_step1.auth.token % > params : queryid : <% outputs.auth_step1.params.queryid % > statusCodes : <% outputs.auth_step1.status_code % > The authentication workflow should return response in this format: headers : header1 : val1 params : param1 : val1 statusCodes : [ 401 , 403 , ..","content_length":847,"content_tokens":227,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"] note The authentication workflow gets called when any request returns the specified statusCodes  12.2 Authorization ​ The framework provides authorization, to verify if any event/model is authorized to access specific information or is allowed to execute certain actions. 12.2.1 Workflow DSL ​ You can add authorization workflow at the task level in any workflow. The authorization workflow should return allow/deny or json output to the main worklfow. Allow/Deny If authz workflow returns data as true/false, it means the task is allowed/denied to get executed. JSON output If authz workflow returns JSON output then it is merged with args.data of the task for which authz is being executed. Here is the sample spec: Sample workflow calling the authz workflow summary : Call an API tasks : - id : httpbin_step1 description : Hit http bin with some dummy data.","content_length":862,"content_tokens":183,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"It will send back same as response authz : fn : com.jfs.authz args : <% inputs % > fn : com.gs.http args : datasource : httpbin data : <% inputs % > config : url : /anything method : post Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: true } else if outputs.authz_step1.data.code == 201 then { success: true data: where: role: 'USER' } else { success: false data: false } %> The authorization workflow should return response in this format to allow/deny: success : true/false data : true/false/JSON output When data is returned as false i.e. deny then the framework will send 403 Unauthorized response. 12.2.2 Sample DB query call authorization ​ In DB query call, authz workflow can return JSON output with where clause, include clause etc. which will be merged with the args of the main workflow which is doing DB query.","content_length":1214,"content_tokens":317,"embedding":[]},{"doc_title":"Authentication & Authorization | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/authen-author","doc_date":"2023-05-09T19:48:23.812Z","content":"Here is the sample spec: Sample workflow calling the authz workflow summary : datastore demo tasks : - id : find_user description : find users authz : fn : com.jfs.auth args : <% inputs % > fn : com.gs.datastore args : datasource : mongo data : include : <% inputs.body.include % > where : <% inputs.body.where % > config : method : user.findMany Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: where: role: 'USER' } else { success: false data: false } %> When authorization workflow com.jfs.authz returns success: true then its data will be merged with the main workflow which is calling the authz workflow. For example, in the above authz workflow, data is returned as: data : where : role : 'USER' This data will be merged with the args.data of the main workflow i.e. args : data : include : <% inputs.body.include % > where : <% inputs.body.where % > # where clause from authz workflow will be merged with this.","content_length":1306,"content_tokens":349,"embedding":[]}]},{"title":"Caching | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/caching","date":"2023-05-09T19:48:23.978Z","content":"9. Caching On this page Caching Godspeed provides caching of the tasks using redis as cache. You can cache the result of any task in the workflows. 9.1 Specifications ​ 9.1.1 Datasource spec for redis ​ Define a datasource with type 'redis' in src/datasources . Here, redis datasource is defined in src/datasources/redis.yaml type : redis url : redis [ s ] : // [ [ username ] [ : password ] @ ] [ host ] [ : port ] [ /db - number ] 9.1.2 Configuration ​ Define default caching datasource in static configuration log_level : debug lang : coffee server_url : https : //api.example.com : 8443/v1/api caching : redis 9.1.3 Workflow spec ​ Here is the caching spec to write in the workflow. caching : key : <key name which is used to cache result in redis > invalidate : <used to invalidate the cache of some other task. Key name which we want to delete/remove from cache e.g. this field can be used in CRUD types task. While delete operation , invalidate the cache of read or update task > cache_on_failure : <true | false , whether you want to cache the failure result or not. By default , it is false > expires : <timer in seconds , until the cached result is valid > force : <true | false , force flag to specify not to use cache , always trigger task's function. Set it to true if you don't want to use cache > Example Spec summary : workflow to cache task results id : cache_wf tasks : - id : cache_step1 caching : key : cache_step1 invalidate : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'hello' config : url : /anything method : post - id : cache_step2 caching : key : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'cache' config : url : /anything method : post When the workflow is triggered for the first time, then the result of the two tasks are cached in DB with keys cache_step1 and cache_step2 for 60 seconds. If the next call to this workflow occurs within 60 seconds then the cached results will be used, else API call will be triggered. In the cache_step1 , invaldiate spec is defined, which is invalidating/deleting the cached result of the cache_step2 . It means even if cache_step2 is cached, if any calls occurs within 60 seconds then the cache_step1 will delete the cached result of cache_step2 . So, no cache will be used for cache_step2 .","tokens":630,"length":2416,"chunks":[{"doc_title":"Caching | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/caching","doc_date":"2023-05-09T19:48:23.978Z","content":"9. Caching On this page Caching Godspeed provides caching of the tasks using redis as cache. You can cache the result of any task in the workflows. 9.1 Specifications ​ 9.1.1 Datasource spec for redis ​ Define a datasource with type 'redis' in src/datasources  Here, redis datasource is defined in src/datasources/redis.yaml type : redis url : redis [ s ] : // [ [ username ] [ : password ] @ ] [ host ] [ : port ] [ /db - number ] 9.1.2 Configuration ​ Define default caching datasource in static configuration log_level : debug lang : coffee server_url : https : //api.example.com : 8443/v1/api caching : redis 9.1.3 Workflow spec ​ Here is the caching spec to write in the workflow.","content_length":685,"content_tokens":195,"embedding":[]},{"doc_title":"Caching | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/caching","doc_date":"2023-05-09T19:48:23.978Z","content":"caching : key : <key name which is used to cache result in redis > invalidate : <used to invalidate the cache of some other task. Key name which we want to delete/remove from cache e.g. this field can be used in CRUD types task. While delete operation , invalidate the cache of read or update task > cache_on_failure : <true | false , whether you want to cache the failure result or not. By default , it is false > expires : <timer in seconds , until the cached result is valid > force : <true | false , force flag to specify not to use cache , always trigger task's function.","content_length":576,"content_tokens":139,"embedding":[]},{"doc_title":"Caching | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/caching","doc_date":"2023-05-09T19:48:23.978Z","content":"Set it to true if you don't want to use cache > Example Spec summary : workflow to cache task results id : cache_wf tasks : - id : cache_step1 caching : key : cache_step1 invalidate : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'hello' config : url : /anything method : post - id : cache_step2 caching : key : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'cache' config : url : /anything method : post When the workflow is triggered for the first time, then the result of the two tasks are cached in DB with keys cache_step1 and cache_step2 for 60 seconds.","content_length":713,"content_tokens":191,"embedding":[]},{"doc_title":"Caching | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/caching","doc_date":"2023-05-09T19:48:23.978Z","content":"If the next call to this workflow occurs within 60 seconds then the cached results will be used, else API call will be triggered. In the cache_step1 , invaldiate spec is defined, which is invalidating/deleting the cached result of the cache_step2  It means even if cache_step2 is cached, if any calls occurs within 60 seconds then the cache_step1 will delete the cached result of cache_step2  So, no cache will be used for cache_step2 .","content_length":436,"content_tokens":106,"embedding":[]}]},{"title":"Custom Middleware | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/custom-middleware","date":"2023-05-09T19:48:24.124Z","content":"14. Custom Middleware On this page Custom Middleware Godspeed provides usage of application level middleware functions. You can add any custom middleware functions which will have access to the request object (req), the response object (res), and the next middleware function in the application’s request-response cycle. 14.1 How to add custom middleware in Godspeed ​ Step 1: Create an index.js/index.ts file in src/middlewares dierctory in your project. Project structure . ├── config └── src └── middlewares └── index.ts Step 2: index.ts/index.js should be exporting array of middleware functions with signature (req, res, next) index.ts import { uuid } from 'uuidv4' ; function addUuid ( req : any , res : any , next : any ) { // Set data req . body . uuid = uuid ( ) ; // Go to next middleware next ( ) ; } function addTitle ( req : any , res : any , next : any ) { // Set data req . body . title = \"Title from middleware/ts\" ; // Go to next middleware next ( ) ; } export default [ addUuid , addTitle ] ; caution If the current middleware function does not end the request-response cycle, it must call next() to pass control to the next middleware function. Otherwise, the request will be left hanging. Sample req object Here, two properties uuid and title are added in the body of req object. { \"_events\" : { } , \"_eventsCount\" : 1 , \"httpVersionMajor\" : 1 , \"httpVersionMinor\" : 1 , \"httpVersion\" : \"1.1\" , \"complete\" : true , \"rawHeaders\" : [ \"Content-Type\" , \"application/json\" , \"User-Agent\" , \"PostmanRuntime/7.29.2\" , \"Accept\" , \"*/*\" , \"Cache-Control\" , \"no-cache\" , \"Postman-Token\" , \"7ce46b80-61e1-44c4-b91a-8a3c914797e8\" , \"Host\" , \"localhost:4901\" , \"Accept-Encoding\" , \"gzip, deflate, br\" , \"Connection\" , \"keep-alive\" , \"Content-Length\" , \"2\" ] , \"rawTrailers\" : [ ] , \"aborted\" : false , \"upgrade\" : false , \"url\" : \"/test3\" , \"method\" : \"POST\" , \"statusCode\" : null , \"statusMessage\" : null , \"_consuming\" : true , \"_dumped\" : false , \"baseUrl\" : \"\" , \"originalUrl\" : \"/test3\" , \"params\" : { } , \"query\" : { } , \"body\" : { \"uuid\" : \"cfc5fc7f-cfdf-4fe7-99ad-08993f90f570\" , \"title\" : \"Title from middleware/ts\" } , \"_body\" : true , \"id\" : 2 , \"log\" : { } , \"route\" : { \"path\" : \"/test3\" , \"stack\" : [ { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } , { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } ] , \"methods\" : { \"post\" : true } } , \"protocol\" : \"http\" , \"secure\" : false , \"ip\" : \"::ffff:192.168.224.1\" , \"ips\" : [ ] , \"subdomains\" : [ ] , \"path\" : \"/test3\" , \"hostname\" : \"localhost\" , \"host\" : \"localhost\" , \"fresh\" : false , \"stale\" : true , \"xhr\" : false , \"files\" : [ ] }","tokens":925,"length":2750,"chunks":[{"doc_title":"Custom Middleware | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/custom-middleware","doc_date":"2023-05-09T19:48:24.124Z","content":"14. Custom Middleware On this page Custom Middleware Godspeed provides usage of application level middleware functions. You can add any custom middleware functions which will have access to the request object (req), the response object (res), and the next middleware function in the application’s request-response cycle. 14.1 How to add custom middleware in Godspeed ​ Step 1: Create an index.js/index.ts file in src/middlewares dierctory in your project. Project structure  ├── config └── src └── middlewares └── index.ts Step 2: index.ts/index.js should be exporting array of middleware functions with signature (req, res, next) index.ts import { uuid } from 'uuidv4' ; function addUuid ( req : any , res : any , next : any ) { // Set data req  body","content_length":751,"content_tokens":191,"embedding":[]},{"doc_title":"Custom Middleware | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/custom-middleware","doc_date":"2023-05-09T19:48:24.124Z","content":"uuid = uuid ( ) ; // Go to next middleware next ( ) ; } function addTitle ( req : any , res : any , next : any ) { // Set data req  body  title = \"Title from middleware/ts\" ; // Go to next middleware next ( ) ; } export default [ addUuid , addTitle ] ; caution If the current middleware function does not end the request-response cycle, it must call next() to pass control to the next middleware function. Otherwise, the request will be left hanging. Sample req object Here, two properties uuid and title are added in the body of req object.","content_length":541,"content_tokens":136,"embedding":[]},{"doc_title":"Custom Middleware | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/custom-middleware","doc_date":"2023-05-09T19:48:24.124Z","content":"{ \"_events\" : { } , \"_eventsCount\" : 1 , \"httpVersionMajor\" : 1 , \"httpVersionMinor\" : 1 , \"httpVersion\" : \"1.1\" , \"complete\" : true , \"rawHeaders\" : [ \"Content-Type\" , \"application/json\" , \"User-Agent\" , \"PostmanRuntime/7.29.2\" , \"Accept\" , \"*/*\" , \"Cache-Control\" , \"no-cache\" , \"Postman-Token\" , \"7ce46b80-61e1-44c4-b91a-8a3c914797e8\" , \"Host\" , \"localhost:4901\" , \"Accept-Encoding\" , \"gzip, deflate, br\" , \"Connection\" , \"keep-alive\" , \"Content-Length\" , \"2\" ] , \"rawTrailers\" : [ ] , \"aborted\" : false , \"upgrade\" : false , \"url\" : \"/test3\" , \"method\" : \"POST\" , \"statusCode\" : null , \"statusMessage\" : null , \"_consuming\" : true , \"_dumped\" : false , \"baseUrl\" : \"\" , \"originalUrl\" : \"/test3\" , \"params\" : { } , \"query\" : { } , \"body\" : { \"uuid\" : \"cfc5fc7f-cfdf-4fe7-99ad-08993f90f570\" , \"title\" : \"Title from middleware/ts\" } , \"_body\" : true , \"id\" : 2 , \"log\" : { } , \"route\" : { \"path\" : \"/test3\" , \"stack\" : [ { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } , { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } ] , \"methods\" : { \"post\" : true } } , \"protocol\" : \"http\" , \"secure\" : false , \"ip\" : \"::ffff:192.168.224.1\" , \"ips\" : [ ] , \"subdomains\" : [ ] , \"path\" : \"/test3\" , \"hostname\" : \"localhost\" , \"host\" : \"localhost\" , \"fresh\" : false , \"stale\" : true , \"xhr\" : false , \"files\" : [ ] }","content_length":1450,"content_tokens":597,"embedding":[]}]},{"title":"8.2 API datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/api","date":"2023-05-09T19:48:24.264Z","content":"8. Datasources 8.2 API datasource On this page API datasource The API datasource acts as a wrapper around third party APIs. It helps interact with third party APIs or own microservices. It takes OpenAPI schema as its setting, and the datasource can be used in com.gs.http calls out of the box. Following functionality is provided by the framework based on the schema of the datasource Authentication and authorization as per the spec Validation of the input to the http method (must be compliant to the API spec) Validation of the response from the API (must be compliant to the API spec) 8.2.1 API datasource schema defined externally ​ If the OpenAPI spec of the API to consume/connect with is available at a URL, then one can simply refer the url here itself. idfc : schema : https : //raw.githubusercontent.com/Kong/swagger - ui - kong - theme/main/demo/public/specs/httpbin.yaml 8.2.2 API datasource schema defined within the yaml file ​ If there is no OpenAPI spec available for an API, then developer needs to provide details of the API schema in the .yaml file for that datasource. type : api schema : base_url : <% config.httpbin.base_url % > security : - ApiKey : sample - app - ApiToken : <% config.httpbin.api_token % > securitySchemes : ApiKey : type : apiKey in : header name : x - api - key ApiToken : type : apiKey in : header name : Authorization 8.2.3 Headers defined at datasource level ​ Headers defined at datasource level are applicable for all the workflows, which are using this datasource. For example, in below datasource, headers 'name' and 'title' are sent in each workflow which is using this datasource. type: api base_url: <% config.httpbin.base_url %> headers: name: godspeed title: <% inputs.headers['title'] %> 8.2.4 Headers defined at task level ​ Headers defined at task level are applicable for a single task only. You can find the example usage here 8.2.5 Example usage ​ You can find the example usage here","tokens":493,"length":1945,"chunks":[{"doc_title":"8.2 API datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/api","doc_date":"2023-05-09T19:48:24.264Z","content":"8. Datasources 8.2 API datasource On this page API datasource The API datasource acts as a wrapper around third party APIs. It helps interact with third party APIs or own microservices. It takes OpenAPI schema as its setting, and the datasource can be used in com.gs.http calls out of the box. Following functionality is provided by the framework based on the schema of the datasource Authentication and authorization as per the spec Validation of the input to the http method (must be compliant to the API spec) Validation of the response from the API (must be compliant to the API spec) 8.2.1 API datasource schema defined externally ​ If the OpenAPI spec of the API to consume/connect with is available at a URL, then one can simply refer the url here itself. idfc : schema : https : //raw.githubusercontent.com/Kong/swagger - ui - kong - theme/main/demo/public/specs/httpbin.yaml 8.2.2 API datasource schema defined within the yaml file ​ If there is no OpenAPI spec available for an API, then developer needs to provide details of the API schema in the .yaml file for that datasource.","content_length":1088,"content_tokens":261,"embedding":[]},{"doc_title":"8.2 API datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/api","doc_date":"2023-05-09T19:48:24.264Z","content":"type : api schema : base_url : <% config.httpbin.base_url % > security : - ApiKey : sample - app - ApiToken : <% config.httpbin.api_token % > securitySchemes : ApiKey : type : apiKey in : header name : x - api - key ApiToken : type : apiKey in : header name : Authorization 8.2.3 Headers defined at datasource level ​ Headers defined at datasource level are applicable for all the workflows, which are using this datasource. For example, in below datasource, headers 'name' and 'title' are sent in each workflow which is using this datasource. type: api base_url: <% config.httpbin.base_url %> headers: name: godspeed title: <% inputs.headers['title'] %> 8.2.4 Headers defined at task level ​ Headers defined at task level are applicable for a single task only. You can find the example usage here 8.2.5 Example usage ​ You can find the example usage here.","content_length":855,"content_tokens":233,"embedding":[]}]},{"title":"8.7 AWS as datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/aws","date":"2023-05-09T19:48:24.394Z","content":"8. Datasources 8.7 AWS as datasource On this page Introduction The framework supports AWS as a datasource. It helps in interacting with AWS, to use various AWS services and methods. 8.7.1 Example spec ​ The datasources for AWS are defined in src/datasources . Here, AWS datasource is defined in aws_s3.yaml . . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── aws_s3.yaml ├── events ├── functions └── mappings Sample configuration in aws_s3.yaml type: aws common: credentials: accessKeyId: 'AKIA4KQJJFGY2KPNNOEMmnbv' secretAccessKey: '+pf5xyyPSUfBNn0V9ZIH0oPVzARBvxoehR+mpzigcdfg' region: \"ap-south-1\" services: S3: config: {} 8.7.2 com.gs.aws workflow ​ Refer here for com.gs.aws workflow.","tokens":237,"length":701,"chunks":[{"doc_title":"8.7 AWS as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/aws","doc_date":"2023-05-09T19:48:24.394Z","content":"8. Datasources 8.7 AWS as datasource On this page Introduction The framework supports AWS as a datasource. It helps in interacting with AWS, to use various AWS services and methods. 8.7.1 Example spec ​ The datasources for AWS are defined in src/datasources  Here, AWS datasource is defined in aws_s3.yaml","content_length":305,"content_tokens":78,"embedding":[]},{"doc_title":"8.7 AWS as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/aws","doc_date":"2023-05-09T19:48:24.394Z","content":"├── config └── src ├── datasources │ └── httpbin.yaml │ ├── aws_s3.yaml ├── events ├── functions └── mappings Sample configuration in aws_s3.yaml type: aws common: credentials: accessKeyId: 'AKIA4KQJJFGY2KPNNOEMmnbv' secretAccessKey: '+pf5xyyPSUfBNn0V9ZIH0oPVzARBvxoehR+mpzigcdfg' region: \"ap-south-1\" services: S3: config: {} 8.7.2 com.gs.aws workflow ​ Refer here for com.gs.aws workflow.","content_length":390,"content_tokens":159,"embedding":[]}]},{"title":"8.3 Datastore as datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","date":"2023-05-09T19:48:24.529Z","content":"8. Datasources 8.3 Datastore as datasource On this page Introduction The framework takes the approach of schema driven development. It supports multiple kinds of SQL and NoSQL datastores. The developer only needs to specify or generate the schema for a datastore, with authorization policies. The CRUD events and workflows are automatically generated from the schema itself. Shall the developer need to use these within other workflows, they can do that as well. Currently supported datastores Postgres (via Prisma) Mysql (via Prisma) Mongodb (via Prisma) Elasticsearch (via Elasticgraph, our inhouse implementation providing bunch of exciting features over Elasticsearch, including relationship management and joins.) The integration supports Model declaration (For both relational and non-relational stores) Schema generation from existing database Universal, autogenerated CRUD API. Validation of the CRUD requests Authorization mechanism at the entity, column, row and ownership levels Automatic caching based on configuration. 8.3.1 Schema specification ​ The framework extends Prisma specification for specifying the schema of any datastore. This can be generated from an existing database or manually created by the developer. The schema is present as {datastore_name}.prisma file in the src/datasources folder. Sample Schema generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique name String? } 8.3.2 CLI Commands ​ Any Prisma CLI command can be executed from godspeed CLI using godspeed prisma <command> . For example, $ godspeed prisma db pull --schema=./src/datasources/mongo_pull.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Prisma schema loaded from src/datasources/mongo_pull.prisma Environment variables loaded from .env Datasource \"db\" ✔ Introspected 6 models and wrote them into src/datasources/mongo_pull.prisma in 81ms *** WARNING *** Could not determine the types for the following fields. - Model \"Post\", field: \"slug\" - Model \"Profile\", field: \"userId\" - Model \"User\", field: \"email\" Run prisma generate to generate Prisma Client. note Please make sure that godspeed prisma <command> is executed inside from devcontainer/project root directory. 8.3.3 Prisma Datastore Setup ​ The framework has inbuilt feature of setting up datastore automatically whenever a new {datastore_name}.prisma file is created in the src/datasources folder. In case, you are getting any error in the datastore setup, then you can refer to below section for manual setup: During the project setup, if you have not specified the type of datastore you just added, then you will have to execute godspeed update in project root directory, outside the dev container. This will deploy the container for this datastore in the dev container environment. Model setup ​ Prisma model setup is done using prisma generate and db push commands. Step 1: godspeed prisma generate ​ $ godspeed prisma generate --schema=./src/datasources/mongo2.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo2.prisma ✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo2 in 111ms You can now start using Prisma Client in your code. Reference: https://pris.ly/d/client import { PrismaClient } from './src/datasources/generated-clients/mongo2' const prisma = new PrismaClient() Step 2: godspeed prisma db push ​ $ godspeed prisma db push --schema=./src/datasources/mongo.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo.prisma Datasource \"db\" The database is already in sync with the Prisma schema. ✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo in 149ms 8.3.4 Auto generating CRUD APIs from data store models ​ Developer can generate CRUD APIs for all the models in a datastore. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each model in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{modelName} and /functions/com/gs/{datasourceName}/{modelName} folders respectively. godspeed gen-crud-api 8.3.5 Sample datastore CRUD task ​ Please find an example here 8.3.6 Prisma encryption of fields ​ You can apply encryption on String type fields in Prisma. Be default, the encryption algorithm used is AES-GCM with 256 bit keys. 8.3.6.1 Specification ​ In your prisma schema, add /// @encrypted to the fields you want to encrypts. For example, email field in below schema: generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique /// @encrypted name String? } 8.3.6.2 Configuration ​ You can specify prisma_secret in environment configuration For example, this is the sample configuration, set PRISMA_SECRET as env variable: prisma_secret : PRISMA_SECRET # secret used to generate hash of prisma fields","tokens":1662,"length":5947,"chunks":[{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"8. Datasources 8.3 Datastore as datasource On this page Introduction The framework takes the approach of schema driven development. It supports multiple kinds of SQL and NoSQL datastores. The developer only needs to specify or generate the schema for a datastore, with authorization policies. The CRUD events and workflows are automatically generated from the schema itself. Shall the developer need to use these within other workflows, they can do that as well. Currently supported datastores Postgres (via Prisma) Mysql (via Prisma) Mongodb (via Prisma) Elasticsearch (via Elasticgraph, our inhouse implementation providing bunch of exciting features over Elasticsearch, including relationship management and joins.) The integration supports Model declaration (For both relational and non-relational stores) Schema generation from existing database Universal, autogenerated CRUD API. Validation of the CRUD requests Authorization mechanism at the entity, column, row and ownership levels Automatic caching based on configuration. 8.3.1 Schema specification ​ The framework extends Prisma specification for specifying the schema of any datastore. This can be generated from an existing database or manually created by the developer. The schema is present as {datastore_name}.prisma file in the src/datasources folder.","content_length":1317,"content_tokens":270,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"Sample Schema generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique name String? } 8.3.2 CLI Commands ​ Any Prisma CLI command can be executed from godspeed CLI using godspeed prisma <command>","content_length":449,"content_tokens":129,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"For example, $ godspeed prisma db pull --schema=./src/datasources/mongo_pull.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Prisma schema loaded from src/datasources/mongo_pull.prisma Environment variables loaded from .env Datasource \"db\" ✔ Introspected 6 models and wrote them into src/datasources/mongo_pull.prisma in 81ms *** WARNING *** Could not determine the types for the following fields.","content_length":570,"content_tokens":207,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"- Model \"Post\", field: \"slug\" - Model \"Profile\", field: \"userId\" - Model \"User\", field: \"email\" Run prisma generate to generate Prisma Client. note Please make sure that godspeed prisma <command> is executed inside from devcontainer/project root directory. 8.3.3 Prisma Datastore Setup ​ The framework has inbuilt feature of setting up datastore automatically whenever a new {datastore_name}.prisma file is created in the src/datasources folder. In case, you are getting any error in the datastore setup, then you can refer to below section for manual setup: During the project setup, if you have not specified the type of datastore you just added, then you will have to execute godspeed update in project root directory, outside the dev container. This will deploy the container for this datastore in the dev container environment. Model setup ​ Prisma model setup is done using prisma generate and db push commands.","content_length":916,"content_tokens":213,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"Step 1: godspeed prisma generate ​ $ godspeed prisma generate --schema=./src/datasources/mongo2.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo2.prisma ✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo2 in 111ms You can now start using Prisma Client in your code.","content_length":563,"content_tokens":211,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"Reference: https://pris.ly/d/client import { PrismaClient } from './src/datasources/generated-clients/mongo2' const prisma = new PrismaClient() Step 2: godspeed prisma db push ​ $ godspeed prisma db push --schema=./src/datasources/mongo.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo.prisma Datasource \"db\" The database is already in sync with the Prisma schema.","content_length":623,"content_tokens":228,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo in 149ms 8.3.4 Auto generating CRUD APIs from data store models ​ Developer can generate CRUD APIs for all the models in a datastore. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each model in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{modelName} and /functions/com/gs/{datasourceName}/{modelName} folders respectively. godspeed gen-crud-api 8.3.5 Sample datastore CRUD task ​ Please find an example here 8.3.6 Prisma encryption of fields ​ You can apply encryption on String type fields in Prisma. Be default, the encryption algorithm used is AES-GCM with 256 bit keys. 8.3.6.1 Specification ​ In your prisma schema, add /// @encrypted to the fields you want to encrypts.","content_length":872,"content_tokens":234,"embedding":[]},{"doc_title":"8.3 Datastore as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/datastore","doc_date":"2023-05-09T19:48:24.529Z","content":"For example, email field in below schema: generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique /// @encrypted name String? } 8.3.6.2 Configuration ​ You can specify prisma_secret in environment configuration For example, this is the sample configuration, set PRISMA_SECRET as env variable: prisma_secret : PRISMA_SECRET # secret used to generate hash of prisma fields.","content_length":626,"content_tokens":171,"embedding":[]}]},{"title":"8.5 Elasticgraph as datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","date":"2023-05-09T19:48:24.681Z","content":"8. Datasources 8.5 Elasticgraph as datasource On this page Introduction The framework supports elasticgraph as a datasource. It supports elasticsearch as datastore. In addition, you can use various features of elasticgraph like deep graph search algorithms, joins, aggregations, multi-lingual support. 8.5.1 Folder Structure ​ The datasources for elasticgraph are defined in src/datasources . Here, elasticgraph1.yaml and elasticgraph2.yaml are defined in datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── elasticgraph1.yaml │ ├── elasticgraph2.yaml ├── events ├── functions └── mappings 8.5.2 Datasource DSL ​ elasticgraph1.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg1/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch elasticgraph2.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg2/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch 8.5.3 Configuration files of elasticgraph ​ All the configuration files of elasticgraph datasources should be defined in src/datasources/eg_config/ directory. Sample strucutre of config files under schema_backend path. . ├── eg1 │ ├── collect.toml │ ├── common.toml │ ├── config.toml │ ├── custom.toml │ ├── elasticsearch.toml │ ├── joins │ │ └── search.txt │ └── schema │ ├── aggregation.toml │ ├── dependencies.toml │ ├── entities │ │ ├── credit_card.toml │ │ └── user.toml │ ├── entitiesInfo.toml │ ├── relationships.txt │ ├── suggestions.toml │ └── union.toml └── eg2 ├── collect.toml ├── common.toml ├── config.toml ├── custom.toml ├── elasticsearch.toml ├── joins │ └── search.txt └── schema ├── aggregation.toml ├── dependencies.toml ├── entities │ ├── credit_card.toml │ └── user.toml ├── entitiesInfo.toml ├── relationships.txt ├── suggestions.toml └── union.toml 8.5.4 Elasticgraph Setup ​ The framework has inbuilt feature of setting up elasticgraph model automatically whenever a new configuration is added in src/datasources/eg_config/ directory. In case, you are getting any error in the setup, then you can refer execute below step for manual setup: During the project setup, if you have not selected elasticsearch, then you will have to execute godspeed update in project root directory, outside the dev container. This will add elasticsearch in the dev container environment. Step 1: godspeed eg-push ​ $ godspeed eg-push _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 eg-push > for f in src/datasources/eg_config/*; do echo ${f}; node ../gs_service/elasticgraph/lib/mappingGenerator/reIndexer.js ${f} all; done src/datasources/eg_config/eg1 8.5.5 Auto generating CRUD APIs for elasticgraph ​ Developer can generate CRUD APIs for all the entities in src/datasources/eg_config/ directory. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each entity in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{entityName} and /functions/com/gs/eg/{datasourceName}/{entityName} folders respectively. $ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs Events and Workflows are generated for elasticgraph.yaml","tokens":1125,"length":3833,"chunks":[{"doc_title":"8.5 Elasticgraph as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","doc_date":"2023-05-09T19:48:24.681Z","content":"8. Datasources 8.5 Elasticgraph as datasource On this page Introduction The framework supports elasticgraph as a datasource. It supports elasticsearch as datastore. In addition, you can use various features of elasticgraph like deep graph search algorithms, joins, aggregations, multi-lingual support. 8.5.1 Folder Structure ​ The datasources for elasticgraph are defined in src/datasources  Here, elasticgraph1.yaml and elasticgraph2.yaml are defined in datasources.","content_length":467,"content_tokens":108,"embedding":[]},{"doc_title":"8.5 Elasticgraph as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","doc_date":"2023-05-09T19:48:24.681Z","content":"├── config └── src ├── datasources │ └── httpbin.yaml │ ├── elasticgraph1.yaml │ ├── elasticgraph2.yaml ├── events ├── functions └── mappings 8.5.2 Datasource DSL ​ elasticgraph1.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg1/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch elasticgraph2.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg2/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch 8.5.3 Configuration files of elasticgraph ​ All the configuration files of elasticgraph datasources should be defined in src/datasources/eg_config/ directory. Sample strucutre of config files under schema_backend path.","content_length":882,"content_tokens":225,"embedding":[]},{"doc_title":"8.5 Elasticgraph as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","doc_date":"2023-05-09T19:48:24.681Z","content":"├── eg1 │ ├── collect.toml │ ├── common.toml │ ├── config.toml │ ├── custom.toml │ ├── elasticsearch.toml │ ├── joins │ │ └── search.txt │ └── schema │ ├── aggregation.toml │ ├── dependencies.toml │ ├── entities │ │ ├── credit_card.toml │ │ └── user.toml │ ├── entitiesInfo.toml │ ├── relationships.txt │ ├── suggestions.toml │ └── union.toml └── eg2 ├── collect.toml ├── common.toml ├── config.toml ├── custom.toml ├── elasticsearch.toml ├── joins │ └── search.txt └── schema ├── aggregation.toml ├── dependencies.toml ├── entities │ ├── credit_card.toml │ └── user.toml ├── entitiesInfo.toml ├── relationships.txt ├── suggestions.toml └── union.toml 8.5.4 Elasticgraph Setup ​ The framework has inbuilt feature of setting up elasticgraph model automatically whenever a new configuration is added in src/datasources/eg_config/ directory. In case, you are getting any error in the setup, then you can refer execute below step for manual setup: During the project setup, if you have not selected elasticsearch, then you will have to execute godspeed update in project root directory, outside the dev container. This will add elasticsearch in the dev container environment.","content_length":1170,"content_tokens":312,"embedding":[]},{"doc_title":"8.5 Elasticgraph as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","doc_date":"2023-05-09T19:48:24.681Z","content":"Step 1: godspeed eg-push ​ $ godspeed eg-push _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 eg-push > for f in src/datasources/eg_config/*; do echo ${f}; node ../gs_service/elasticgraph/lib/mappingGenerator/reIndexer.js ${f} all; done src/datasources/eg_config/eg1 8.5.5 Auto generating CRUD APIs for elasticgraph ​ Developer can generate CRUD APIs for all the entities in src/datasources/eg_config/ directory. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each entity in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{entityName} and /functions/com/gs/eg/{datasourceName}/{entityName} folders respectively.","content_length":896,"content_tokens":312,"embedding":[]},{"doc_title":"8.5 Elasticgraph as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph","doc_date":"2023-05-09T19:48:24.681Z","content":"$ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs Events and Workflows are generated for elasticgraph.yaml.","content_length":407,"content_tokens":171,"embedding":[]}]},{"title":"8.6 Extensible datasources | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources","date":"2023-05-09T19:48:24.826Z","content":"8. Datasources 8.6 Extensible datasources On this page Introduction The framework provides feature to extend datasources where you can add new datasources with any customized type as per your business logic. 8.6.1 Datasource definition ​ You can define your datasource in yaml file inside src/datasources directory. For example, newDatasource.yaml is defined in the datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── newDatasource.yaml ├── events ├── functions └── mappings The three keys in yaml type , loadFn and executeFn are mandatory to define any new datasource which is not provided by the framework as core datasources. You can define other key/vaue pairs as per your need. Below is a sample of newDatasource.yaml type: sample loadFn: com.sample.loader executeFn: com.sample.execute client_url: https://sample.com client_id: sample123 type ​ It defines the type of the datasource like api, soap, datastore, etc. loadFn ​ It defines the load function which loads the client for the datasource. The developer must define the load function in the workflows as mentioned in the below project structure. The loadFn can be a js/ts function which takes the datasource yaml as an input and return an object that contains client. . ├── config └── src ├── datasources ├── events ├── functions │ └── com │ └── sample │ ├── loader.ts │ └── execute.ts └── mappings A sample of loader.ts export default async function(args:{[key:string]:any;}) { const ds = { ...args, client: new SampleClient(args) }; return ds; } executeFn ​ It defines the execute function which gets executed in the workflow. The developer must define the execute function in the workflows as mentioned in the above project structure. The executeFn can be a js/ts function which takes the workflow args as input and return status/output. export default async function(args:{[key:string]:any;}) { if(args.datasource) { const client = args.datasource.client; const data = args.data; if (!Array.isArray(args.data)) { data = [args.data]; } . . . . . . . . . . } else { return { success: false, code: 500, data: 'datasource not found in the workflow' }; } } 8.6.2 Example spec for the event ​ /sample_helloworld.http.post : id : sample_event fn : com.jfs.sample_helloworld body : description : The body of the query required : true content : application/json : # For ex. application/json application/xml schema : type : object properties : name : type : string required : [ name ] 8.6.3 Example spec for the workflow ​ summary : hello world tasks : - id : helloworld_step1 fn : com.sample.execute args : datasource : newDatasource data : <% inputs % > config : method : sample","tokens":701,"length":2677,"chunks":[{"doc_title":"8.6 Extensible datasources | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources","doc_date":"2023-05-09T19:48:24.826Z","content":"8. Datasources 8.6 Extensible datasources On this page Introduction The framework provides feature to extend datasources where you can add new datasources with any customized type as per your business logic. 8.6.1 Datasource definition ​ You can define your datasource in yaml file inside src/datasources directory. For example, newDatasource.yaml is defined in the datasources.  ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── newDatasource.yaml ├── events ├── functions └── mappings The three keys in yaml type , loadFn and executeFn are mandatory to define any new datasource which is not provided by the framework as core datasources. You can define other key/vaue pairs as per your need.","content_length":722,"content_tokens":183,"embedding":[]},{"doc_title":"8.6 Extensible datasources | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources","doc_date":"2023-05-09T19:48:24.826Z","content":"Below is a sample of newDatasource.yaml type: sample loadFn: com.sample.loader executeFn: com.sample.execute client_url: https://sample.com client_id: sample123 type ​ It defines the type of the datasource like api, soap, datastore, etc. loadFn ​ It defines the load function which loads the client for the datasource. The developer must define the load function in the workflows as mentioned in the below project structure. The loadFn can be a js/ts function which takes the datasource yaml as an input and return an object that contains client.","content_length":546,"content_tokens":135,"embedding":[]},{"doc_title":"8.6 Extensible datasources | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources","doc_date":"2023-05-09T19:48:24.826Z","content":"├── config └── src ├── datasources ├── events ├── functions │ └── com │ └── sample │ ├── loader.ts │ └── execute.ts └── mappings A sample of loader.ts export default async function(args:{[key:string]:any;}) { const ds = { ...args, client: new SampleClient(args) }; return ds; } executeFn ​ It defines the execute function which gets executed in the workflow. The developer must define the execute function in the workflows as mentioned in the above project structure. The executeFn can be a js/ts function which takes the workflow args as input and return status/output.","content_length":570,"content_tokens":145,"embedding":[]},{"doc_title":"8.6 Extensible datasources | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources","doc_date":"2023-05-09T19:48:24.826Z","content":"export default async function(args:{[key:string]:any;}) { if(args.datasource) { const client = args.datasource.client; const data = args.data; if (!Array.isArray(args.data)) { data = [args.data]; }           } else { return { success: false, code: 500, data: 'datasource not found in the workflow' }; } } 8.6.2 Example spec for the event ​ /sample_helloworld.http.post : id : sample_event fn : com.jfs.sample_helloworld body : description : The body of the query required : true content : application/json : # For ex. application/json application/xml schema : type : object properties : name : type : string required : [ name ] 8.6.3 Example spec for the workflow ​ summary : hello world tasks : - id : helloworld_step1 fn : com.sample.execute args : datasource : newDatasource data : <% inputs % > config : method : sample.","content_length":823,"content_tokens":240,"embedding":[]}]},{"title":"8.1 Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/intro","date":"2023-05-09T19:48:24.977Z","content":"8. Datasources 8.1 Introduction On this page Datasources Any kind of entity which provides read and write mechanism for data is considered a datasource. For example, an API, a SQL or NoSQL datastore which includes RDBMS, key value stores, document stores etc. The settings for each datasource lies in src/datasources directory. 8.1.1 Datasource types ​ Currently supported types API Datastores (SQL/NoSQL) Postgres Mysql Mongodb Kafka Elasticsearch Upcoming S3 File system","tokens":116,"length":472,"chunks":[{"doc_title":"8.1 Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/intro","doc_date":"2023-05-09T19:48:24.977Z","content":"8. Datasources 8.1 Introduction On this page Datasources Any kind of entity which provides read and write mechanism for data is considered a datasource. For example, an API, a SQL or NoSQL datastore which includes RDBMS, key value stores, document stores etc. The settings for each datasource lies in src/datasources directory. 8.1.1 Datasource types ​ Currently supported types API Datastores (SQL/NoSQL) Postgres Mysql Mongodb Kafka Elasticsearch Upcoming S3 File system","content_length":472,"content_tokens":116,"embedding":[]}]},{"title":"8.4 Kafka as datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/kafka","date":"2023-05-09T19:48:25.095Z","content":"8. Datasources 8.4 Kafka as datasource On this page Introduction The framework supports kafka as a datasource. It helps in interacting with kafka, to send/receive events on a kafka message bus. 8.4.1 Example spec ​ The datasources for kafka are defined in src/datasources . Here, two kafka clients kafka1.yaml and kafka2.yaml are defined in datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── kafka2.yaml ├── events ├── functions └── mappings Sample configuration in kafka1.yaml type: kafka client_id: my_service brokers: [ \"kafka:9092\" ]","tokens":184,"length":579,"chunks":[{"doc_title":"8.4 Kafka as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/kafka","doc_date":"2023-05-09T19:48:25.095Z","content":"8. Datasources 8.4 Kafka as datasource On this page Introduction The framework supports kafka as a datasource. It helps in interacting with kafka, to send/receive events on a kafka message bus. 8.4.1 Example spec ​ The datasources for kafka are defined in src/datasources . Here, two kafka clients kafka1.yaml and kafka2.yaml are defined in datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── kafka2.yaml ├── events ├── functions └── mappings Sample configuration in kafka1.yaml type: kafka client_id: my_service brokers: [ \"kafka:9092\" ]","content_length":579,"content_tokens":184,"embedding":[]}]},{"title":"8.8 Redis as datasource | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/datasources/redis","date":"2023-05-09T19:48:25.229Z","content":"8. Datasources 8.8 Redis as datasource On this page Introduction The framework supports Redis as a datasource. It helps to utilize redis in different ways. 8.8.1 Example spec ​ The datasources for Redis are defined in src/datasources . Here, Redis datasource is defined in redis.yaml . . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── redis.yaml ├── events ├── functions └── mappings Sample configuration in redis.yaml type: redis url: redis[s]://[[username][:password]@][host][:port][/db-number] For full redis configuration, Refer redis node client documentation .","tokens":160,"length":579,"chunks":[{"doc_title":"8.8 Redis as datasource | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/datasources/redis","doc_date":"2023-05-09T19:48:25.229Z","content":"8. Datasources 8.8 Redis as datasource On this page Introduction The framework supports Redis as a datasource. It helps to utilize redis in different ways. 8.8.1 Example spec ​ The datasources for Redis are defined in src/datasources . Here, Redis datasource is defined in redis.yaml . . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── redis.yaml ├── events ├── functions └── mappings Sample configuration in redis.yaml type: redis url: redis[s]://[[username][:password]@][host][:port][/db-number] For full redis configuration, Refer redis node client documentation .","content_length":579,"content_tokens":160,"embedding":[]}]},{"title":"intro | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/developer-manual/intro","date":"2023-05-09T19:48:25.364Z","content":"On this page intro Work of the developer Creating Data services or module services like Document or Notification service ​ The developer will use scaffolding provided by the platform to setup a new microservice project. In the configuration template, (s)he will configure the microservice for the required functionality, data model & performance tunings. Once done, (s)he can migrate the DBs and run the microservice, using the CLI during the local development, and using the GitOps process for staging/production deployment. Simple settings will provide out of the box functionalities like JWT token based authorization, APIs for multi db CRUD, analytics, search, suggest, document, notifications, event publishing/subscription, observability. This can be used to run standalone domain agnostic functional services like notification service! Or to run a separate microservice which does only the job of data federation and nothing else (Such a service is called backend for frontend aka BFF) Creating domain gateway, orchestrators & servicese ​ If for a domain microservice like Lead Origination System, there is any custom validations, business logic, event consumers, REST routes, orchestration flows etc that need to be written, the developer needs to add those within the microservice project, using the respective interface and scaffolding structure provided by the framework. This way a developer can get a custom microservice on top of the fundamental microservice framework capabilities. They can mix and match capabilities provided out of the box with custom capabilities added on top by them.","tokens":306,"length":1603,"chunks":[{"doc_title":"intro | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/developer-manual/intro","doc_date":"2023-05-09T19:48:25.364Z","content":"On this page intro Work of the developer Creating Data services or module services like Document or Notification service ​ The developer will use scaffolding provided by the platform to setup a new microservice project. In the configuration template, (s)he will configure the microservice for the required functionality, data model & performance tunings. Once done, (s)he can migrate the DBs and run the microservice, using the CLI during the local development, and using the GitOps process for staging/production deployment. Simple settings will provide out of the box functionalities like JWT token based authorization, APIs for multi db CRUD, analytics, search, suggest, document, notifications, event publishing/subscription, observability.","content_length":744,"content_tokens":145,"embedding":[]},{"doc_title":"intro | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/developer-manual/intro","doc_date":"2023-05-09T19:48:25.364Z","content":"This can be used to run standalone domain agnostic functional services like notification service! Or to run a separate microservice which does only the job of data federation and nothing else (Such a service is called backend for frontend aka BFF) Creating domain gateway, orchestrators & servicese ​ If for a domain microservice like Lead Origination System, there is any custom validations, business logic, event consumers, REST routes, orchestration flows etc that need to be written, the developer needs to add those within the microservice project, using the respective interface and scaffolding structure provided by the framework. This way a developer can get a custom microservice on top of the fundamental microservice framework capabilities. They can mix and match capabilities provided out of the box with custom capabilities added on top by them.","content_length":858,"content_tokens":161,"embedding":[]}]},{"title":"Events | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/events","date":"2023-05-09T19:48:25.519Z","content":"6. Events On this page Events A microservice can be configured to consume events from variety of event sources , like HTTP, gRpc, GraphQl, S3 etc. The event schema, for each event source, closely follows the OpenAPI specification. It includes The name/topic/URL of the event The event source and other information for the source (for ex. group_id in case of Kafka events) The event handler workflow Validation (input and output) Examples of input and output The response of the event is flexible for the developer to change as per the requirement. 6.1 Event types ​ Currently supported http.{method_type} For example, post or get Kafka salesforce cron Planned Webhook S3 gRPC GraphQL Websocket 6.2 Event schema & examples for supported sources ​ All event declarations are stored in the src/events folder, in YAML files. 6.2.1 JSON schema validation ​ The framework provides request and response schema validation out of the box. Request schema validation ​ Sample spec for request schema. body: content: application/json: schema: type: 'object' required: [] properties: dob: type: 'string' format : 'date' pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" If request schema validation fails, then status code 400 is returned. Response schema validation ​ Sample spec for response schema. responses: #Output data defined as per the OpenAPI spec 200: description: content: application/json: # For ex. application/json application/xml schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id] examples: # <string, ExampleObject> example1: summary: description: value: application_id: PRM20478956N external_value: If response schema validation fails, then status code 500 is returned. 6.2.2 HTTP event ​ For an HTTP event, the headers, query, params and body data are captured in a standard format, and made available in the inputs object for use in the workflows . The inputs (event) object has following properties: - query: `<%inputs.query.var_name%>` # present in case of http events - params: `<%inputs.params.path_param%>` # present in case of http events - headers: `<%inputs.headers.some_header_key%>` # present in case of http events - body: `<%inputs.body.key%>` # Present for all events except for http events which don't have a body. For ex. http.get - files: `<%input.files%>` # Any files uploaded via HTTP event. Not present in other kind of events Example spec for HTTP event ​ /v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post : #Adding .http.post after #the endpoint exposes the endpoint as REST via the POST method (in this example) fn : com.biz.kyc.ckyc.ckyc_initiate #The event handler written in ckyc_initiate.yml, and # kept in src/workflows/com/biz/kyc/ckyc folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs/ folder (in this example) body : required : true content : application/json : schema : type : 'object' required : [ ] properties : dob : { type : 'string' , format : 'date' , pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta : type : 'object' params : - name : lender_loan_application_id in : params # same as open api spec: one of cookie, path, query, header required : true allow_empty_value : false schema : type : string responses : #Output data defined as per the OpenAPI spec 200 : description : required : # default value is false content : application/json : # For ex. application/json application/xml schema : type : object properties : application_id : type : string additionalProperties : false required : [ application_id ] examples : # <string, ExampleObject> example1 : summary : description : value : application_id : PRM20478956N external_value : encoding : 400 : description : required : # default value is false content : application/json : # For ex. application/json application/xml schema : type : object properties : lender_response_code : type : string examples : # <string, ExampleObject> example1 : summary : description : value : lender_response_code : E001 external_value : encoding : Example workflow consuming an HTTP event ​ summary : Simply returning query & body data of an http.post event id : some_unique_id tasks : - id : step1 fn : com.gs.return args : <%inputs.body% > # Evaluation of dynamic values happens via <% %>. The type of scripting can be coffee/js. # Here we are returning the body of the HTTP post event. Example workflow (on_validation_error handler) handling json schema validation error ​ summary : Handle json scehma validation error id : error_handler tasks : - id : erorr_step1 fn : com.gs.kafka args : datasource : kafka1 data : # publish the event and validation error to kafka on a topic value : event : <% inputs.event % > validation_error : <% inputs.validation_error % > config : topic : kafka_error_handle method : publish 6.2.3 Kafka event ​ A kafka event is specified as {topic_name}.{datasourceName}.{group_id} in the kafka event specification . The group_id represents identifier for all the consumers of the group. Only one consumer of the group will consume a message. This is useful for microservices, when a single services runs in multiple K8s pods. Each pod is part of the same group. This ensures the message is eventually consumed by any one of the pods. The message body of a kafka event is captured and represented as inputs.body for consumption in the handler workflow . Datasource for kafka ​ The datasources for kafka are defined in src/datasources . Refer Kafka as datasource for more information. Example spec for kafka event ​ kafka-consumer1.kafka1.kafka_proj : # This event will be triggered whenever # a new message arrives on the topic_name id : /kafkaWebhook fn : com.jfs.publish_kafka #The event handler written in publish_kafka.yml, and # kept in src/workflows/com/jfs folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs folder (in this example) body : description : The body of the query content : application/json : # For ex. application/json application/xml schema : type : object properties : name : type : string required : [ name ] Example workflow consuming a kafka event ​ summary : Handle kafka event id : some_unique_id tasks : - id : step1 summary : Publish an event with this data fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : publish - producer1 data : value : <% inputs % > # Here we are publishing an event data to another topic Refer com.gs.kafka native function to publish an event on kafka. 6.2.4 Salesforce event ​ A salesforce event is specified as {topic_name}.salesforce.{datasource_name} topic_name is salaesforce event topic datasource_name is name of the salesforce datasource filename Prerequisite: For using salesforce , You need to enable redis datasource. You can enable redis while creating a new godspeed project or run godspeed update on an existing project. in config/default.yaml add a property as caching: redis . Where redis is datasource name. If your redis type datasource name is redis1.yaml , then caching: redis1 will be the correct configuration. Example of salesforce datasource, eg: src/datasources/salaeforce.yaml type : salesforce connection : # Please Check https:#jsforce.github.io/document/ #1. Username and Password Login # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com' #2. Username and Password Login (OAuth2 Resource Owner Password Credential) oauth2 : # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com', clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<callback URI is here>' #3. Session ID serverUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' sessionId : '<your Salesforce session ID is here>' #4. Access Token instanceUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' #5. Access Token with Refresh Token oauth2 : clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<your Salesforce OAuth2 redirect URI is here>' instanceUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' refreshToken : '<your Salesforce OAuth2 refresh token is here>' username : <% config.salesforce_username % > password : <% config.salesforce_password % > Example of salesforce event: { topic_name } .salesforce. { datasourceName } id : /salesforcetopic fn : com.jfs.handle_title_events on_validation_error : com.jfs.handle_validation_error body : description : The body of the query content : application/json : schema : type : object properties : name : type : string required : [ name ] 6.2.5 CRON event ​ A CRON event will allow you to run events at scheduled time / interval. A CRON event is specified as {schedule_expression.cron.timezone} schedule_expression You can refer crontab to generate schedule. timezone Refer this wikipedia to get the timezone format. Here is an example of a CRON event which run every minute. every_minute_cron.yaml \"* * * * *.cron.Asia/Kolkata\" : fn : com.every_minute and corresponding function is com/every_minute.yaml summary : this workflow will be running every minute tasks : - id : print description : print every fn : com.gs.log args : level : info data : HELLO from CRON","tokens":2575,"length":9766,"chunks":[{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"6. Events On this page Events A microservice can be configured to consume events from variety of event sources , like HTTP, gRpc, GraphQl, S3 etc. The event schema, for each event source, closely follows the OpenAPI specification. It includes The name/topic/URL of the event The event source and other information for the source (for ex. group_id in case of Kafka events) The event handler workflow Validation (input and output) Examples of input and output The response of the event is flexible for the developer to change as per the requirement. 6.1 Event types ​ Currently supported http.{method_type} For example, post or get Kafka salesforce cron Planned Webhook S3 gRPC GraphQL Websocket 6.2 Event schema & examples for supported sources ​ All event declarations are stored in the src/events folder, in YAML files.","content_length":820,"content_tokens":186,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"6.2.1 JSON schema validation ​ The framework provides request and response schema validation out of the box. Request schema validation ​ Sample spec for request schema. body: content: application/json: schema: type: 'object' required: [] properties: dob: type: 'string' format : 'date' pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" If request schema validation fails, then status code 400 is returned. Response schema validation ​ Sample spec for response schema. responses: #Output data defined as per the OpenAPI spec 200: description: content: application/json: # For ex.","content_length":567,"content_tokens":140,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"application/json application/xml schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id] examples: # <string, ExampleObject> example1: summary: description: value: application_id: PRM20478956N external_value: If response schema validation fails, then status code 500 is returned. 6.2.2 HTTP event ​ For an HTTP event, the headers, query, params and body data are captured in a standard format, and made available in the inputs object for use in the workflows","content_length":519,"content_tokens":119,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"The inputs (event) object has following properties: - query: `<%inputs.query.var_name%>` # present in case of http events - params: `<%inputs.params.path_param%>` # present in case of http events - headers: `<%inputs.headers.some_header_key%>` # present in case of http events - body: `<%inputs.body.key%>` # Present for all events except for http events which don't have a body. For ex. http.get - files: `<%input.files%>` # Any files uploaded via HTTP event.","content_length":460,"content_tokens":141,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"Not present in other kind of events Example spec for HTTP event ​ /v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post : #Adding .http.post after #the endpoint exposes the endpoint as REST via the POST method (in this example) fn : com.biz.kyc.ckyc.ckyc_initiate #The event handler written in ckyc_initiate.yml, and # kept in src/workflows/com/biz/kyc/ckyc folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs/ folder (in this example) body : required : true content : application/json : schema : type : 'object' required : [ ] properties : dob : { type : 'string' , format : 'date' , pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta : type : 'object' params : - name : lender_loan_application_id in : params # same as open api spec: one of cookie, path, query, header required : true allow_empty_value : false schema : type : string responses : #Output data defined as per the OpenAPI spec 200 : description : required : # default value is false content : application/json : # For ex. application/json application/xml schema : type : object properties : application_id : type : string additionalProperties : false required : [ application_id ] examples : # <string, ExampleObject> example1 : summary : description : value : application_id : PRM20478956N external_value : encoding : 400 : description : required : # default value is false content : application/json : # For ex.","content_length":1536,"content_tokens":426,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"application/json application/xml schema : type : object properties : lender_response_code : type : string examples : # <string, ExampleObject> example1 : summary : description : value : lender_response_code : E001 external_value : encoding : Example workflow consuming an HTTP event ​ summary : Simply returning query & body data of an http.post event id : some_unique_id tasks : - id : step1 fn : com.gs.return args : <%inputs.body% > # Evaluation of dynamic values happens via <% %> The type of scripting can be coffee/js. # Here we are returning the body of the HTTP post event.","content_length":581,"content_tokens":140,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"Example workflow (on_validation_error handler) handling json schema validation error ​ summary : Handle json scehma validation error id : error_handler tasks : - id : erorr_step1 fn : com.gs.kafka args : datasource : kafka1 data : # publish the event and validation error to kafka on a topic value : event : <% inputs.event % > validation_error : <% inputs.validation_error % > config : topic : kafka_error_handle method : publish 6.2.3 Kafka event ​ A kafka event is specified as {topic_name}.{datasourceName}.{group_id} in the kafka event specification  The group_id represents identifier for all the consumers of the group. Only one consumer of the group will consume a message. This is useful for microservices, when a single services runs in multiple K8s pods. Each pod is part of the same group. This ensures the message is eventually consumed by any one of the pods. The message body of a kafka event is captured and represented as inputs.body for consumption in the handler workflow  Datasource for kafka ​ The datasources for kafka are defined in src/datasources  Refer Kafka as datasource for more information.","content_length":1119,"content_tokens":278,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"Example spec for kafka event ​ kafka-consumer1.kafka1.kafka_proj : # This event will be triggered whenever # a new message arrives on the topic_name id : /kafkaWebhook fn : com.jfs.publish_kafka #The event handler written in publish_kafka.yml, and # kept in src/workflows/com/jfs folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs folder (in this example) body : description : The body of the query content : application/json : # For ex.","content_length":578,"content_tokens":168,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"application/json application/xml schema : type : object properties : name : type : string required : [ name ] Example workflow consuming a kafka event ​ summary : Handle kafka event id : some_unique_id tasks : - id : step1 summary : Publish an event with this data fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : publish - producer1 data : value : <% inputs % > # Here we are publishing an event data to another topic Refer com.gs.kafka native function to publish an event on kafka.","content_length":539,"content_tokens":136,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"6.2.4 Salesforce event ​ A salesforce event is specified as {topic_name}.salesforce.{datasource_name} topic_name is salaesforce event topic datasource_name is name of the salesforce datasource filename Prerequisite: For using salesforce , You need to enable redis datasource. You can enable redis while creating a new godspeed project or run godspeed update on an existing project. in config/default.yaml add a property as caching: redis  Where redis is datasource name. If your redis type datasource name is redis1.yaml , then caching: redis1 will be the correct configuration. Example of salesforce datasource, eg: src/datasources/salaeforce.yaml type : salesforce connection : # Please Check https:#jsforce.github.io/document/ #1.","content_length":733,"content_tokens":191,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"Username and Password Login # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com' #2. Username and Password Login (OAuth2 Resource Owner Password Credential) oauth2 : # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com', clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<callback URI is here>' #3. Session ID serverUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' sessionId : '<your Salesforce session ID is here>' #4.","content_length":644,"content_tokens":185,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"Access Token instanceUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' #5. Access Token with Refresh Token oauth2 : clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<your Salesforce OAuth2 redirect URI is here>' instanceUrl : '<your Salesforce server URL (e.g.","content_length":437,"content_tokens":130,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' refreshToken : '<your Salesforce OAuth2 refresh token is here>' username : <% config.salesforce_username % > password : <% config.salesforce_password % > Example of salesforce event: { topic_name } .salesforce. { datasourceName } id : /salesforcetopic fn : com.jfs.handle_title_events on_validation_error : com.jfs.handle_validation_error body : description : The body of the query content : application/json : schema : type : object properties : name : type : string required : [ name ] 6.2.5 CRON event ​ A CRON event will allow you to run events at scheduled time / interval.","content_length":678,"content_tokens":194,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/events","doc_date":"2023-05-09T19:48:25.519Z","content":"A CRON event is specified as {schedule_expression.cron.timezone} schedule_expression You can refer crontab to generate schedule. timezone Refer this wikipedia to get the timezone format. Here is an example of a CRON event which run every minute. every_minute_cron.yaml \"* * * * *.cron.Asia/Kolkata\" : fn : com.every_minute and corresponding function is com/every_minute.yaml summary : this workflow will be running every minute tasks : - id : print description : print every fn : com.gs.log args : level : info data : HELLO from CRON.","content_length":534,"content_tokens":142,"embedding":[]}]},{"title":"Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/intro","date":"2023-05-09T19:48:25.688Z","content":"2. Introduction On this page Introduction Every microservice in the Godspeed framework has three fundamental abstractions, and the developer needs to work with just these three. Events : Events trigger workflows. Events are generated by event sources like REST endpoints, gRPC, message bus, webhooks, websockets, S3, and more... Workflows : Workflows are triggered by events. They not only perform business logic but also provide orchestration over datasources and microservices, and data/API federation. They will use datasources to store or retrieve data, join across various datasources, transform data, emit events and send responses. The framework provides a YAML dsl with some inbuilt workflows . If YAML does not suffice for any particular case, developers can currently put JS/TS workflows alongside YAML workflows and use them. Coming in future : Support for other languages. Datasources : Datasources are locations where data can be stored or read from. For example API datasource (another microservice or third party), datastores (RDBMS, document, key-value), file system, S3 storage, etc. A microservice can use multiple datasources. The framework provides abstractions for Authn/Authz making it easy for the developer to express the same in a low code manner. These abstractions allow the developer to focus purely on their business logic. 99.9% - 100% of typical functionality needed by the developer is covered by the framework's YAML-based DSL. Devs can forget about the low-level stuff they typically need to do - which accounts for 90% of the work in typical app dev scenario. The framework aims to handle all the low level functionality and saves developer's effort to do the same. For example creating controllers for endpoints, endpoint authentication/authorization, input validation, auto-telemetry with distributed context, setting up DB client and authorizing DB access, authentication of third party API, key management, creating Swagger docs or Postman collection, creating basic test suite based on documentation, etc. There is a standard project structure which will give the developer a kickstart to their project and also reference code/declarations, for the kind of stuff they can do using the framework. 2.1 Developer's work ​ The developer will use the CLI provided by the framework to setup a new microservice project and start developing. (S)he will configure the events, datasources, and workflows for the required functionality, along with mappings, environment variables, and common configurations, like for telemetry. To configure the datasources, For datastores: they will either define the db schema or autogenerate it from the existing database using the CLI. For APIs: they will need to define the APIs OpenAPI schema or provide the url for the same. Salient Features ​ Note Some of the features mentioned here are in the product roadmap and planned for upcoming releases. Schema driven development The developer has to specify the API and data schema to start the development. YAML based DSL and configurations We have YAML based DSL which makes it much easier and succinct to express policies, business logic, and configurations. Code is shorter and easier to comprehend than programming, even for new learners. This DSL can be further customized by developers to add custom requirements. Multi datastore support The same model configuration & unified CRUD API (including full-text search and autosuggest) will provide interfaces with multiple kinds of datastores (SQL or NoSQL). The API is aimed to provide validation, relationship management, transactions, denormalization, and multilingual support. Each integration will support the possible functionality as per the nature of the store. Data validation The framework provides validation of third party API requests & responses, datastore queries, and its own API endpoints request and response. The developer only needs to specify the schema of third party API, own microservice API, and datastore model. Rest is taken care of by the framework. In case of more complex validation scenarios, where customer journeys may require conditional validation of incoming requests based on some attributes (in the database or the query {i.e. subject, object, environment, payload}), the developer can add such rules to the application logic as part of the workflows. Authentication The microservice framework authenticates every incoming request and extracts the user role and other info, for further processing, based on a valid JWT token. An IAM provider like ORY Kratos can be integrated into the platform for providing identity service. It will generate a JWT token which will include user id, information, and roles. This token is consumed by the microservices for user validation. Authorization ( Planned ) Each microservice will do the job of authorization for any request. Developers will write authorization rules for every microservice in simple configuration files. This will cover not only API endpoint access but also fine grained data access from datastores. This will integrate with third party Authz services in a pluggable way, with abstractions. Distributed transactions ( Planned ) Each domain’s orchestrator is able to use the Saga pattern to ensure distributed transactions across multiple microservices. Autogenerated documentation The framework provides autogenerated documentation using CLI. Autogenerated CRUD API ( Planned ) The framework provides autogenereated CRUD APIs from database model. Generated API's can be extended by the developers as per their needs. Autogenerated test suite The framework provides autogenerated test suite for APIs using CLI. Multiple languages support In case YAML is not enough for a corner case, developers can write custom business logic in any language. If written in JS/TS, they can place the code within the same microservice project. Other language support will also work in the same way, and is planned for the future. Observability The framework provides automatic observability support with correlation, for modern distributed systems, via the OpenTelemetry spec. For the same, it will work in conjunction with the microservice mesh used. The developer can extend that to include customized observability. This can integrate with any tools that support OpenTelemetry. Logging The inbuilt logging mechanism will log both sync request/response cycle or async events, for both success and failure scenarios. Monitoring The framework allows the developer to monitor custom business metrics, along with application level metrics like latency, success, and failures. Tracing Every incoming sync & async request will carry trace information in its headers. The same is propagated further through the microservice framework when it makes a sync or async hit to another service.","tokens":1356,"length":6826,"chunks":[{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"2. Introduction On this page Introduction Every microservice in the Godspeed framework has three fundamental abstractions, and the developer needs to work with just these three. Events : Events trigger workflows. Events are generated by event sources like REST endpoints, gRPC, message bus, webhooks, websockets, S3, and more.. Workflows : Workflows are triggered by events. They not only perform business logic but also provide orchestration over datasources and microservices, and data/API federation. They will use datasources to store or retrieve data, join across various datasources, transform data, emit events and send responses. The framework provides a YAML dsl with some inbuilt workflows  If YAML does not suffice for any particular case, developers can currently put JS/TS workflows alongside YAML workflows and use them. Coming in future : Support for other languages.","content_length":882,"content_tokens":186,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"Datasources : Datasources are locations where data can be stored or read from. For example API datasource (another microservice or third party), datastores (RDBMS, document, key-value), file system, S3 storage, etc. A microservice can use multiple datasources. The framework provides abstractions for Authn/Authz making it easy for the developer to express the same in a low code manner. These abstractions allow the developer to focus purely on their business logic. 99.9% - 100% of typical functionality needed by the developer is covered by the framework's YAML-based DSL. Devs can forget about the low-level stuff they typically need to do - which accounts for 90% of the work in typical app dev scenario. The framework aims to handle all the low level functionality and saves developer's effort to do the same.","content_length":815,"content_tokens":181,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"For example creating controllers for endpoints, endpoint authentication/authorization, input validation, auto-telemetry with distributed context, setting up DB client and authorizing DB access, authentication of third party API, key management, creating Swagger docs or Postman collection, creating basic test suite based on documentation, etc. There is a standard project structure which will give the developer a kickstart to their project and also reference code/declarations, for the kind of stuff they can do using the framework. 2.1 Developer's work ​ The developer will use the CLI provided by the framework to setup a new microservice project and start developing. (S)he will configure the events, datasources, and workflows for the required functionality, along with mappings, environment variables, and common configurations, like for telemetry. To configure the datasources, For datastores: they will either define the db schema or autogenerate it from the existing database using the CLI.","content_length":1000,"content_tokens":196,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"For APIs: they will need to define the APIs OpenAPI schema or provide the url for the same. Salient Features ​ Note Some of the features mentioned here are in the product roadmap and planned for upcoming releases. Schema driven development The developer has to specify the API and data schema to start the development. YAML based DSL and configurations We have YAML based DSL which makes it much easier and succinct to express policies, business logic, and configurations. Code is shorter and easier to comprehend than programming, even for new learners. This DSL can be further customized by developers to add custom requirements. Multi datastore support The same model configuration & unified CRUD API (including full-text search and autosuggest) will provide interfaces with multiple kinds of datastores (SQL or NoSQL) The API is aimed to provide validation, relationship management, transactions, denormalization, and multilingual support. Each integration will support the possible functionality as per the nature of the store.","content_length":1032,"content_tokens":198,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"Data validation The framework provides validation of third party API requests & responses, datastore queries, and its own API endpoints request and response. The developer only needs to specify the schema of third party API, own microservice API, and datastore model. Rest is taken care of by the framework. In case of more complex validation scenarios, where customer journeys may require conditional validation of incoming requests based on some attributes (in the database or the query {i.e. subject, object, environment, payload}), the developer can add such rules to the application logic as part of the workflows. Authentication The microservice framework authenticates every incoming request and extracts the user role and other info, for further processing, based on a valid JWT token. An IAM provider like ORY Kratos can be integrated into the platform for providing identity service. It will generate a JWT token which will include user id, information, and roles.","content_length":974,"content_tokens":192,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"This token is consumed by the microservices for user validation. Authorization ( Planned ) Each microservice will do the job of authorization for any request. Developers will write authorization rules for every microservice in simple configuration files. This will cover not only API endpoint access but also fine grained data access from datastores. This will integrate with third party Authz services in a pluggable way, with abstractions. Distributed transactions ( Planned ) Each domain’s orchestrator is able to use the Saga pattern to ensure distributed transactions across multiple microservices. Autogenerated documentation The framework provides autogenerated documentation using CLI. Autogenerated CRUD API ( Planned ) The framework provides autogenereated CRUD APIs from database model. Generated API's can be extended by the developers as per their needs. Autogenerated test suite The framework provides autogenerated test suite for APIs using CLI.","content_length":960,"content_tokens":187,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/intro","doc_date":"2023-05-09T19:48:25.688Z","content":"Multiple languages support In case YAML is not enough for a corner case, developers can write custom business logic in any language. If written in JS/TS, they can place the code within the same microservice project. Other language support will also work in the same way, and is planned for the future. Observability The framework provides automatic observability support with correlation, for modern distributed systems, via the OpenTelemetry spec. For the same, it will work in conjunction with the microservice mesh used. The developer can extend that to include customized observability. This can integrate with any tools that support OpenTelemetry. Logging The inbuilt logging mechanism will log both sync request/response cycle or async events, for both success and failure scenarios. Monitoring The framework allows the developer to monitor custom business metrics, along with application level metrics like latency, success, and failures. Tracing Every incoming sync & async request will carry trace information in its headers. The same is propagated further through the microservice framework when it makes a sync or async hit to another service.","content_length":1153,"content_tokens":216,"embedding":[]}]},{"title":"Introduction to Godspeed CLI | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","date":"2023-05-09T19:48:25.853Z","content":"4. CLI On this page Godspeed CLI The CLI is the primary way to interact with your Godspeed project from the command line. It provides a bunch of useful functionalities during the project development lifecycle. 4.1 Functionality ​ Outside the dev container ​ Creating a new project environment with dev container setup, which includes the folder structure, all the databases, message bus, cache, etc. Open up an existing project in the dev container, add/update a container in the dev environment, based on updated settings. List the versions of gs_service. Change the version of gs_service. Inside the dev container ​ All Prisma commands including DB push, pull or migration. OAS 3 documentation file generation. Test suite/Postman collection generation. Running test suite. 4.2 Installation ​ npm install -g @mindgrep/godspeed Once Godspeed CLI is installed, the godspeed command can be called from command line. When called without arguments, it displays its help and command usage. $ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command 4.3 Options ​ --version (-v) ​ The --version option outputs information about your current godspeed version. $ godspeed -v _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| 0.0.26 --help (-h) ​ The --help option displays help and command usage. $ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command 4.4 Commands: Outside the dev container ​ create ​ The create command creates project structure for any microservice. When called without arguments, it creates project structure with examples. $ godspeed create my_service _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| projectDir: /home/gurjot/cli-test/my_service projectTemplateDir undefined project created Do you need mongodb? [y/n] [default: n] n Do you need postgresdb? [y/n] [default: n] y Please enter name of the postgres database [default: test] Do you need kafka? [y/n] [default: n] n Do you need elastisearch? [y/n] [default: n] n Please enter host port on which you want to run your service [default: 3000] 3100 Fetching release version information... Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13 Enter your version [default: latest] 1.0.13 Selected version 1.0.13 . . . . . . . . Options ​ $ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <existing_project_directory> existing project template dir -h, --help display help for command update ​ The update can be executed in the following cases: If you want to launch an existing project (i.e. copied from local/cloned from repo) instead of creating a new one, then execute godspeed update command before launching the project. If you want to reloads the containers with updated project settings. For example, if you have not selected any database during the project creation and you want to include any database in the project later on, then execute godspeed update with the required settings. If there is any change in gs_service image of standard tags (e.g. latest, stable) and you want to fetch the latest code for the same tag, then execute godspeed update command. It fetches the new docker image itself. Please note that the command should be executed from inside the project root directory. note Whenever you update your project using godspeed update and open the project in VScode dev container after update, then it is mandatory to do godspeed build inside dev container for the first time. $ godspeed update _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Do you need postgresdb? [y/n] [default: n] Do you need kafka? [y/n] [default: n] Do you need elastisearch? [y/n] [default: n] Please enter host port on which you want to run your service [default: 3000] Fetching release version information... Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.2 1.0.3 1.0.4 dev stable Enter your version [default: latest] Selected version latest Removing dev_test_devcontainer_node_1 ... . . . . . . . . . . Step 1/9 : FROM adminmindgrep/gs_service:latest latest: Pulling from adminmindgrep/gs_service 824b15f81d65: Already exists 325d38bcb229: Already exists d6d638bf61bf: Already exists 55daac95cedf: Already exists 4c701498752d: Already exists a48b0ae49665: Pulling fs layer 4c393fb6deac: Pulling fs layer 4f4fb700ef54: Pulling fs layer 8992963a9530: Pulling fs layer 4f4fb700ef54: Verifying Checksum 4f4fb700ef54: Download complete 4c393fb6deac: Verifying Checksum 4c393fb6deac: Download complete 8992963a9530: Verifying Checksum 8992963a9530: Download complete a48b0ae49665: Verifying Checksum a48b0ae49665: Download complete a48b0ae49665: Pull complete 4c393fb6deac: Pull complete 4f4fb700ef54: Pull complete 8992963a9530: Pull complete Digest: sha256:7195b3c921f1278153c911e6e77cbcfb385a84c435bfcb7b8272ffcf9a3278ee Status: Downloaded newer image for adminmindgrep/gs_service:latest ---> 988917710d1a Step 2/9 : ARG USERNAME=node ---> Running in c70404bb4f3e Removing intermediate container c70404bb4f3e ---> 47a7406b2473 Step 3/9 : ARG USER_UID=1000 ---> Running in 51e68336d8d8 Removing intermediate container 51e68336d8d8 ---> ce913f6898bb Step 4/9 : ARG USER_GID=$USER_UID ---> Running in 7cf1c1f2a3ec Removing intermediate container 7cf1c1f2a3ec ---> 91f045b32e0f Step 5/9 : USER root ---> Running in f338d755a032 Removing intermediate container f338d755a032 ---> fa9898eb4c23 Step 6/9 : RUN sudo groupmod --gid $USER_GID $USERNAME && usermod --uid $USER_UID --gid $USER_GID $USERNAME && chown -R $USER_UID:$USER_GID /workspace/development ---> Running in eba3659fb919 Removing intermediate container eba3659fb919 ---> 414f34560b0d Step 7/9 : USER node ---> Running in 23818c5f4882 Removing intermediate container 23818c5f4882 ---> 1bd65323ae91 Step 8/9 : RUN sudo npm i -g @mindgrep/godspeed ---> Running in a66cb062390d . . . . . . . . . . godspeed update dev_test is done. versions ​ The versions command lists all the versions available of gs_service. $ godspeed versions _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13 version ​ The version command helps to change the version of gs_service for any microservice. Execute the command from inside the project root directory. $ godspeed version 1.0.13 _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Generating prisma modules Starting test1_devcontainer_postgres_1 ... Starting test1_devcontainer_postgres_1 ... done Creating test1_devcontainer_node_run ... Creating test1_devcontainer_node_run ... done Environment variables loaded from .env . . . . . . . . . . help ​ The help command displays help and usage for any command. $ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <projectTemplateDir> local project template dir -h, --help display help for command 4.5 Commands: Inside the dev container ​ prisma ​ You can run all the prisma commands in your project root directory inside the dev container. This command is useful for db migration and introspection. Read more here . $ godspeed prisma <prisma command with args> build ​ You can build the complete project using this command. It is the first command which you need to run whenever you open your project in VScode Dev container. Refer Open in Dev container godspeed build dev ​ You can run your project using dev command. godspeed dev gen-api-docs ​ You can get OAS 3 documentation generated automatically by executing this command in your project root directory inside the dev container. $ godspeed gen-api-docs _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-api-docs > node ../gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529346164] INFO (GS-logger/7684 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529346190] DEBUG (GS-logger/7684 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529346289] INFO (GS-logger/7684 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved! gen-test-suite ​ You can get test suite/postman collection generated automatically by executing this command in your project root directory inside the dev container. Now, you can import the collection in postman directly. godspeed gen-test-suite _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-test-suite > npm run gen-api-docs && mkdir -p tests && openapi2postmanv2 -s docs/api-doc.yaml -o tests/test-suite.json -p -O folderStrategy=Tags,includeAuthInfoInExample=false > proj_upd@1.0.0 gen-api-docs > node ../gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529443249] INFO (GS-logger/8145 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529443273] DEBUG (GS-logger/8145 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529443374] INFO (GS-logger/8145 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved! Input file: /workspace/development/app/docs/api-doc.yaml Writing to file: true /workspace/development/app/tests/test-suite.json { result: true, output: [ { type: 'collection', data: [Object] } ] } Conversion successful, collection written to file gen-crud-api ​ You can get CRUD API generated automatically for datastores and elasticgraph datasources by executing this command in your project root directory inside the dev container. $ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs (x) elasticgraph.yaml ( ) For all ( ) Cancel test ​ You can run the test suite generated in above command from the following two ways: Postman: Import the collection in postman and run the test suite. CLI: You can use below command to run the test suite from CLI. Please make sure your service is up and running before running the test suite. godspeed test _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 test > newman run tests/test-suite.json newman Godspeed: Sample Microservice → Call another (sub) workflow from main workflow POST http://localhost:3000/another_workflow?bank_id=<string> [200 OK, 630B, 2.6s] . . . . . . . . help ​ The help command displays help and usage for any command. Click here to know more","tokens":4875,"length":14605,"chunks":[{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"4. CLI On this page Godspeed CLI The CLI is the primary way to interact with your Godspeed project from the command line. It provides a bunch of useful functionalities during the project development lifecycle. 4.1 Functionality ​ Outside the dev container ​ Creating a new project environment with dev container setup, which includes the folder structure, all the databases, message bus, cache, etc. Open up an existing project in the dev container, add/update a container in the dev environment, based on updated settings. List the versions of gs_service. Change the version of gs_service. Inside the dev container ​ All Prisma commands including DB push, pull or migration. OAS 3 documentation file generation. Test suite/Postman collection generation. Running test suite. 4.2 Installation ​ npm install -g @mindgrep/godspeed Once Godspeed CLI is installed, the godspeed command can be called from command line. When called without arguments, it displays its help and command usage.","content_length":983,"content_tokens":206,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command 4.3 Options ​ --version (-v) ​ The --version option outputs information about your current godspeed version.","content_length":687,"content_tokens":212,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed -v _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| 0.0.26 --help (-h) ​ The --help option displays help and command usage.","content_length":299,"content_tokens":136,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command 4.4 Commands: Outside the dev container ​ create ​ The create command creates project structure for any microservice. When called without arguments, it creates project structure with examples.","content_length":770,"content_tokens":223,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed create my_service _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| projectDir: /home/gurjot/cli-test/my_service projectTemplateDir undefined project created Do you need mongodb? [y/n] [default: n] n Do you need postgresdb? [y/n] [default: n] y Please enter name of the postgres database [default: test] Do you need kafka? [y/n] [default: n] n Do you need elastisearch? [y/n] [default: n] n Please enter host port on which you want to run your service [default: 3000] 3100 Fetching release version information..","content_length":686,"content_tokens":253,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13 Enter your version [default: latest] 1.0.13 Selected version 1.0.13","content_length":248,"content_tokens":113,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"Options ​ $ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <existing_project_directory> existing project template dir -h, --help display help for command update ​ The update can be executed in the following cases: If you want to launch an existing project (i.e.","content_length":576,"content_tokens":193,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"copied from local/cloned from repo) instead of creating a new one, then execute godspeed update command before launching the project. If you want to reloads the containers with updated project settings. For example, if you have not selected any database during the project creation and you want to include any database in the project later on, then execute godspeed update with the required settings. If there is any change in gs_service image of standard tags (e.g. latest, stable) and you want to fetch the latest code for the same tag, then execute godspeed update command. It fetches the new docker image itself. Please note that the command should be executed from inside the project root directory. note Whenever you update your project using godspeed update and open the project in VScode dev container after update, then it is mandatory to do godspeed build inside dev container for the first time.","content_length":906,"content_tokens":185,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed update _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Do you need postgresdb? [y/n] [default: n] Do you need kafka? [y/n] [default: n] Do you need elastisearch? [y/n] [default: n] Please enter host port on which you want to run your service [default: 3000] Fetching release version information.. Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.2 1.0.3 1.0.4 dev stable Enter your version [default: latest] Selected version latest Removing dev_test_devcontainer_node_1 ..","content_length":691,"content_tokens":256,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"Step 1/9 : FROM adminmindgrep/gs_service:latest latest: Pulling from adminmindgrep/gs_service 824b15f81d65: Already exists 325d38bcb229: Already exists d6d638bf61bf: Already exists 55daac95cedf: Already exists 4c701498752d: Already exists a48b0ae49665: Pulling fs layer 4c393fb6deac: Pulling fs layer 4f4fb700ef54: Pulling fs layer 8992963a9530: Pulling fs layer 4f4fb700ef54: Verifying Checksum 4f4fb700ef54: Download complete 4c393fb6deac: Verifying Checksum 4c393fb6deac: Download complete 8992963a9530: Verifying Checksum 8992963a9530: Download complete a48b0ae49665: Verifying Checksum a48b0ae49665: Download complete a48b0ae49665: Pull complete 4c393fb6deac: Pull complete 4f4fb700ef54: Pull complete 8992963a9530: Pull complete Digest: sha256:7195b3c921f1278153c911e6e77cbcfb385a84c435bfcb7b8272ffcf9a3278ee Status: Downloaded newer image for adminmindgrep/gs_service:latest ---> 988917710d1a Step 2/9 : ARG USERNAME=node ---> Running in c70404bb4f3e Removing intermediate container c70404bb4f3e ---> 47a7406b2473 Step 3/9 : ARG USER_UID=1000 ---> Running in 51e68336d8d8 Removing intermediate container 51e68336d8d8 ---> ce913f6898bb Step 4/9 : ARG USER_GID=$USER_UID ---> Running in 7cf1c1f2a3ec Removing intermediate container 7cf1c1f2a3ec ---> 91f045b32e0f Step 5/9 : USER root ---> Running in f338d755a032 Removing intermediate container f338d755a032 ---> fa9898eb4c23 Step 6/9 : RUN sudo groupmod --gid $USER_GID $USERNAME && usermod --uid $USER_UID --gid $USER_GID $USERNAME && chown -R $USER_UID:$USER_GID /workspace/development ---> Running in eba3659fb919 Removing intermediate container eba3659fb919 ---> 414f34560b0d Step 7/9 : USER node ---> Running in 23818c5f4882 Removing intermediate container 23818c5f4882 ---> 1bd65323ae91 Step 8/9 : RUN sudo npm i -g @mindgrep/godspeed ---> Running in a66cb062390d godspeed update dev_test is done. versions ​ The versions command lists all the versions available of gs_service.","content_length":1938,"content_tokens":691,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed versions _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13 version ​ The version command helps to change the version of gs_service for any microservice.","content_length":439,"content_tokens":213,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"Execute the command from inside the project root directory. $ godspeed version 1.0.13 _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Generating prisma modules Starting test1_devcontainer_postgres_1 .. Starting test1_devcontainer_postgres_1 .. done Creating test1_devcontainer_node_run .. Creating test1_devcontainer_node_run .. done Environment variables loaded from .env help ​ The help command displays help and usage for any command.","content_length":602,"content_tokens":202,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <projectTemplateDir> local project template dir -h, --help display help for command 4.5 Commands: Inside the dev container ​ prisma ​ You can run all the prisma commands in your project root directory inside the dev container.","content_length":590,"content_tokens":195,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"This command is useful for db migration and introspection. Read more here  $ godspeed prisma <prisma command with args> build ​ You can build the complete project using this command. It is the first command which you need to run whenever you open your project in VScode Dev container. Refer Open in Dev container godspeed build dev ​ You can run your project using dev command. godspeed dev gen-api-docs ​ You can get OAS 3 documentation generated automatically by executing this command in your project root directory inside the dev container.","content_length":544,"content_tokens":112,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed gen-api-docs _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-api-docs > node ../gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529346164] INFO (GS-logger/7684 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529346190] DEBUG (GS-logger/7684 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529346289] INFO (GS-logger/7684 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved! gen-test-suite ​ You can get test suite/postman collection generated automatically by executing this command in your project root directory inside the dev container. Now, you can import the collection in postman directly.","content_length":1440,"content_tokens":529,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"godspeed gen-test-suite _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-test-suite > npm run gen-api-docs && mkdir -p tests && openapi2postmanv2 -s docs/api-doc.yaml -o tests/test-suite.json -p -O folderStrategy=Tags,includeAuthInfoInExample=false > proj_upd@1.0.0 gen-api-docs > node ../gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529443249] INFO (GS-logger/8145 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529443273] DEBUG (GS-logger/8145 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529443374] INFO (GS-logger/8145 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved! Input file: /workspace/development/app/docs/api-doc.yaml Writing to file: true /workspace/development/app/tests/test-suite.json { result: true, output: [ { type: 'collection', data: [Object] } ] } Conversion successful, collection written to file gen-crud-api ​ You can get CRUD API generated automatically for datastores and elasticgraph datasources by executing this command in your project root directory inside the dev container.","content_length":1850,"content_tokens":670,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"$ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs (x) elasticgraph.yaml ( ) For all ( ) Cancel test ​ You can run the test suite generated in above command from the following two ways: Postman: Import the collection in postman and run the test suite. CLI: You can use below command to run the test suite from CLI. Please make sure your service is up and running before running the test suite.","content_length":691,"content_tokens":237,"embedding":[]},{"doc_title":"Introduction to Godspeed CLI | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/introduction-cli","doc_date":"2023-05-09T19:48:25.853Z","content":"godspeed test _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 test > newman run tests/test-suite.json newman Godspeed: Sample Microservice → Call another (sub) workflow from main workflow POST http://localhost:3000/another_workflow?bank_id=<string> [200 OK, 630B, 2.6s] help ​ The help command displays help and usage for any command. Click here to know more.","content_length":541,"content_tokens":206,"embedding":[]}]},{"title":"Mappings | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/mappings","date":"2023-05-09T19:48:26.039Z","content":"10. Mappings On this page Mappings Mappings is a global object which will be available in your microservice. You can define anything in the mappings i.e. key/value pair map, array, etc. You can access these mappings inside your workflows at any time. 10.1 Project structure ​ Mappings are present in src/mappings directory. The default format is yaml and you can store mappings in the nested directories also. The nested directories are also accessible in the same mappings object. . ├── config └── src └── mappings └── index.yaml └── generate.yaml 10.2 Sample mappings ​ This is a sample mapping which is accessible in the workflows inside mappings object using mappings.Gender and mappings.generate.genId index.yaml Gender : Male : M Female : F Others : O generate.yaml genId : 12345 Note If the file name is index.yaml then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the mappings object like mappings.Gender . However, for other file names you need to mention the file name while accessing the mappings object like mappings.generate.genId Smaple workflow accessing mappings object: - id: httpbinCof_step1 description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' gender: <% mappings.Gender[inputs.body.Gender] %> id: <% mappings.generate.genId %> config: url : /anything method: post 10.3 Use mappings constants in other mapping files ​ You can use mapping constants in other mapping files using coffee/js scripting. For example, you have mapping files index.yaml , relations.json and reference.yaml . Use the mappings from first two files as reference in the third file as follows: index.yaml Gender : Male : M Female : F Others : O relations.json { \"id\" : 1 , \"title\" : \"Hello World\" , \"completed\" : false } reference.yaml NewGender : <% mappings.Gender.Others % > title : <% mappings.relations.title % >","tokens":512,"length":2003,"chunks":[{"doc_title":"Mappings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/mappings","doc_date":"2023-05-09T19:48:26.039Z","content":"10. Mappings On this page Mappings Mappings is a global object which will be available in your microservice. You can define anything in the mappings i.e. key/value pair map, array, etc. You can access these mappings inside your workflows at any time. 10.1 Project structure ​ Mappings are present in src/mappings directory. The default format is yaml and you can store mappings in the nested directories also. The nested directories are also accessible in the same mappings object.","content_length":481,"content_tokens":108,"embedding":[]},{"doc_title":"Mappings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/mappings","doc_date":"2023-05-09T19:48:26.039Z","content":"├── config └── src └── mappings └── index.yaml └── generate.yaml 10.2 Sample mappings ​ This is a sample mapping which is accessible in the workflows inside mappings object using mappings.Gender and mappings.generate.genId index.yaml Gender : Male : M Female : F Others : O generate.yaml genId : 12345 Note If the file name is index.yaml then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the mappings object like mappings.Gender  However, for other file names you need to mention the file name while accessing the mappings object like mappings.generate.genId Smaple workflow accessing mappings object: - id: httpbinCof_step1 description: Hit http bin with some dummy data.","content_length":739,"content_tokens":187,"embedding":[]},{"doc_title":"Mappings | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/mappings","doc_date":"2023-05-09T19:48:26.039Z","content":"It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' gender: <% mappings.Gender[inputs.body.Gender] %> id: <% mappings.generate.genId %> config: url : /anything method: post 10.3 Use mappings constants in other mapping files ​ You can use mapping constants in other mapping files using coffee/js scripting. For example, you have mapping files index.yaml , relations.json and reference.yaml Use the mappings from first two files as reference in the third file as follows: index.yaml Gender : Male : M Female : F Others : O relations.json { \"id\" : 1 , \"title\" : \"Hello World\" , \"completed\" : false } reference.yaml NewGender : <% mappings.Gender.Others % > title : <% mappings.relations.title % >","content_length":775,"content_tokens":217,"embedding":[]}]},{"title":"Plugins | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/plugins","date":"2023-05-09T19:48:26.173Z","content":"11. Plugins On this page Plugins Plugins are small js/ts functions to enhance the workflows capabilities. You can write any piece of code in the plugin and can access it inside your workflows at any time. 11.1 Project structure ​ Plugins are present in src/plugins directory. The default format is js/ts and you can store plugins in the nested directories also. . ├── config └── src └── plugins └── index.ts └── time └── epoch.ts └── epoch └── convertEpoch.ts 11.2 Sample plugins ​ These are the sample plugins file which export plugin functions named randomInt and convertEpochToDate . plugins/index.ts export function randomInt ( min : number , max : number ) { return Math . floor ( Math . random ( ) * ( max - min + 1 ) ) + min ; } plugins/time/epoch.ts import format from 'date-fns/format' ; export function convertEpochToDate ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } plugins/epoch/convertEpoch.ts import format from 'date-fns/format' ; export default function convertEpoch ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } Note If the file name is index.ts then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the plugin e.g. randomInt . For other file names you need to mention the file name using underscore notation while accessing the plugins function inside your workflow e.g. time_epoch_convertEpochToDate If it's a default import then you don't need to mention the plugin function name e.g. epoch_convertEpoch 11.3 Sample workflow using plugins ​ You can use these plugins in your workflows as given below: - id: httpbinCof_step1 description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' id: <% 'UID-' + randomInt(1,9) %> date: <% time_epoch_convertEpochToDate(inputs.body.datetimestamp) %> default_date: <% epoch_convertEpoch(inputs.body.datetimestamp) %> config: url : /anything method: post","tokens":598,"length":2180,"chunks":[{"doc_title":"Plugins | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/plugins","doc_date":"2023-05-09T19:48:26.173Z","content":"11. Plugins On this page Plugins Plugins are small js/ts functions to enhance the workflows capabilities. You can write any piece of code in the plugin and can access it inside your workflows at any time. 11.1 Project structure ​ Plugins are present in src/plugins directory. The default format is js/ts and you can store plugins in the nested directories also.  ├── config └── src └── plugins └── index.ts └── time └── epoch.ts └── epoch └── convertEpoch.ts 11.2 Sample plugins ​ These are the sample plugins file which export plugin functions named randomInt and convertEpochToDate  plugins/index.ts export function randomInt ( min : number , max : number ) { return Math  floor ( Math","content_length":687,"content_tokens":170,"embedding":[]},{"doc_title":"Plugins | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/plugins","doc_date":"2023-05-09T19:48:26.173Z","content":"random ( ) * ( max - min + 1 ) ) + min ; } plugins/time/epoch.ts import format from 'date-fns/format' ; export function convertEpochToDate ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } plugins/epoch/convertEpoch.ts import format from 'date-fns/format' ; export default function convertEpoch ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } Note If the file name is index.ts then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the plugin e.g.","content_length":687,"content_tokens":198,"embedding":[]},{"doc_title":"Plugins | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/plugins","doc_date":"2023-05-09T19:48:26.173Z","content":"randomInt  For other file names you need to mention the file name using underscore notation while accessing the plugins function inside your workflow e.g. time_epoch_convertEpochToDate If it's a default import then you don't need to mention the plugin function name e.g. epoch_convertEpoch 11.3 Sample workflow using plugins ​ You can use these plugins in your workflows as given below: - id: httpbinCof_step1 description: Hit http bin with some dummy data.","content_length":457,"content_tokens":108,"embedding":[]},{"doc_title":"Plugins | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/plugins","doc_date":"2023-05-09T19:48:26.173Z","content":"It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' id: <% 'UID-' + randomInt(1,9) %> date: <% time_epoch_convertEpochToDate(inputs.body.datetimestamp) %> default_date: <% epoch_convertEpoch(inputs.body.datetimestamp) %> config: url : /anything method: post.","content_length":341,"content_tokens":122,"embedding":[]}]},{"title":"Scaffolding | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/scaffolding","date":"2023-05-09T19:48:26.329Z","content":"Scaffolding To be done","tokens":6,"length":22,"chunks":[{"doc_title":"Scaffolding | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/scaffolding","doc_date":"2023-05-09T19:48:26.329Z","content":"Scaffolding To be done","content_length":22,"content_tokens":6,"embedding":[]}]},{"title":"3.5 Auto watch and build | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/auto-watch","date":"2023-05-09T19:48:26.462Z","content":"3. Setup 3.5 Auto watch and build Auto watch and build The framework provides auto watch/build feature to detect the changes in you project files. This feature is only applicable when you are working inside dev container. note Please make sure VS code 'Run On Save' plugin is installed in your dev container environment. Here is the list of files which are being watched inside the dev container. src/**/*.yaml|yml|js|json src/**/*.ts src/**/*.prisma src/**/*.toml *.prisma files These files are being watched for Datastore as datasources During any datastore setup via Prisma in the dev container, you don't need to setup anything explicitily, the watch feature automatically takes care of setting up the datastores. Refer Prisma Datastore Setup for more information. *.toml files These files are being watched for configuration files of Elasticgraph as datasource . If there is any change in *.toml file then auto watch reindexes all the elasticgraph datasources configuration inside src/datasources/eg_config/ directory.","tokens":229,"length":1023,"chunks":[{"doc_title":"3.5 Auto watch and build | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/auto-watch","doc_date":"2023-05-09T19:48:26.462Z","content":"3. Setup 3.5 Auto watch and build Auto watch and build The framework provides auto watch/build feature to detect the changes in you project files. This feature is only applicable when you are working inside dev container. note Please make sure VS code 'Run On Save' plugin is installed in your dev container environment. Here is the list of files which are being watched inside the dev container. src/**/*.yaml|yml|js|json src/**/*.ts src/**/*.prisma src/**/*.toml *.prisma files These files are being watched for Datastore as datasources During any datastore setup via Prisma in the dev container, you don't need to setup anything explicitily, the watch feature automatically takes care of setting up the datastores. Refer Prisma Datastore Setup for more information. *.toml files These files are being watched for configuration files of Elasticgraph as datasource If there is any change in *.toml file then auto watch reindexes all the elasticgraph datasources configuration inside src/datasources/eg_config/ directory.","content_length":1020,"content_tokens":228,"embedding":[]}]},{"title":"3.3.2 Environment variables | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/env-vars","date":"2023-05-09T19:48:26.597Z","content":"3. Setup 3.3 Configuration 3.3.2 Environment variables On this page Environment variables The environment variables are defined in yaml files under config/custom-environment-variables.yaml file. The default directory structure is given as below: ├── config │ ├── custom-environment-variables.yaml note Any configuration which includes secrets or passwords is recommended to be defined using environment variables only. custom-environment-variables.yaml ​ This is a sample for custom environment variables where these variables gets values from environment variables set in the environment. my_datasource: base_url: MY_DATASOURCE_BASE_URL api_key: MY_DATASOURCE_API_KEY api_token: MY_DATASOURCE_API_TOKEN kafka: brokers: __name: KAFKA_BROKERS __format: json client_id: KAFKA_CLIENT_ID jwt: issuer: JWT_ISS audience: JWT_AUD secretOrKey: JWT_SECRET prisma_secret: PRISMA_SECRET For example, MY_DATASOURCE_BASE_URL is defined as an environment variable. To specify its value, you need to export this variable in the environment as given below: $ export MY_DATASOURCE_BASE_URL=https://httpbin.org/ After exporting the environment variable, you can access this variable in your project by using scripting <% config.my_datasource.base_url %>","tokens":322,"length":1235,"chunks":[{"doc_title":"3.3.2 Environment variables | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/env-vars","doc_date":"2023-05-09T19:48:26.597Z","content":"3. Setup 3.3 Configuration 3.3.2 Environment variables On this page Environment variables The environment variables are defined in yaml files under config/custom-environment-variables.yaml file. The default directory structure is given as below: ├── config │ ├── custom-environment-variables.yaml note Any configuration which includes secrets or passwords is recommended to be defined using environment variables only. custom-environment-variables.yaml ​ This is a sample for custom environment variables where these variables gets values from environment variables set in the environment.","content_length":589,"content_tokens":113,"embedding":[]},{"doc_title":"3.3.2 Environment variables | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/env-vars","doc_date":"2023-05-09T19:48:26.597Z","content":"my_datasource: base_url: MY_DATASOURCE_BASE_URL api_key: MY_DATASOURCE_API_KEY api_token: MY_DATASOURCE_API_TOKEN kafka: brokers: __name: KAFKA_BROKERS __format: json client_id: KAFKA_CLIENT_ID jwt: issuer: JWT_ISS audience: JWT_AUD secretOrKey: JWT_SECRET prisma_secret: PRISMA_SECRET For example, MY_DATASOURCE_BASE_URL is defined as an environment variable. To specify its value, you need to export this variable in the environment as given below: $ export MY_DATASOURCE_BASE_URL=https://httpbin.org/ After exporting the environment variable, you can access this variable in your project by using scripting <% config.my_datasource.base_url %>","content_length":644,"content_tokens":209,"embedding":[]}]},{"title":"3.3.1 Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/intro","date":"2023-05-09T19:48:26.747Z","content":"3. Setup 3.3 Configuration 3.3.1 Introduction On this page 3.3.1 Introduction The configuration variables as well as their values are defined in yaml files under config/ directory. These variables can be replaced as per the business use cases. The default directory structure is given as below: ├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml File Naming and Load Order ​ The configuration files under config/ directory can have specific naming conventions and load order. Please refer File Name and Load Order for more information.","tokens":115,"length":554,"chunks":[{"doc_title":"3.3.1 Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/intro","doc_date":"2023-05-09T19:48:26.747Z","content":"3. Setup 3.3 Configuration 3.3.1 Introduction On this page 3.3.1 Introduction The configuration variables as well as their values are defined in yaml files under config/ directory. These variables can be replaced as per the business use cases. The default directory structure is given as below: ├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml File Naming and Load Order ​ The configuration files under config/ directory can have specific naming conventions and load order. Please refer File Name and Load Order for more information.","content_length":554,"content_tokens":115,"embedding":[]}]},{"title":"3.3.3 Static variables | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/static-vars","date":"2023-05-09T19:48:26.867Z","content":"3. Setup 3.3 Configuration 3.3.3 Static variables On this page Static variables The static variables as well as their values are defined in yaml files under config/ directory. These variables can be replaced as per the business use cases. The default directory structure is given as below: ├── config │ ├── default.yaml note Any configuration which includes secrets or passwords is recommended to be defined using environment variables only. Avoid using static variables for secrets and passwords. default.yaml ​ This file contains some predefined variables. Below is a sample file which defines the static variables used in Godspeed. log_level : debug lang : coffee redact : [ ] # fields to hide. Sample: ['ns', 'req.headers'] server_url : https : //api.example.com : 8443/v1/api httpbin : # sample api datasource url base_url : https : //httpbin.org log_level is the minimum log level to log. Log messages with a lower limit will not get logged. The default value is 'info'. The available levels are 'fatal', 'error', 'warn', 'info', 'debug', 'trace' or 'silent'. lang is the language used for scripting in the workflows. The default value is 'coffee'. The available values are 'coffee' or 'js'. Refer Coffee/JS scripting for more information. redact is the list of fields, the values for which, you want to hide from the logs. The default value is blank. Refer Logs field masking for more information. server_url is the custom server url which you want to use as Servers in swagger specs/auto generated documentation. Refer Custom Server URL","tokens":354,"length":1544,"chunks":[{"doc_title":"3.3.3 Static variables | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/static-vars","doc_date":"2023-05-09T19:48:26.867Z","content":"3. Setup 3.3 Configuration 3.3.3 Static variables On this page Static variables The static variables as well as their values are defined in yaml files under config/ directory. These variables can be replaced as per the business use cases. The default directory structure is given as below: ├── config │ ├── default.yaml note Any configuration which includes secrets or passwords is recommended to be defined using environment variables only. Avoid using static variables for secrets and passwords. default.yaml ​ This file contains some predefined variables. Below is a sample file which defines the static variables used in Godspeed. log_level : debug lang : coffee redact : [ ] # fields to hide.","content_length":697,"content_tokens":140,"embedding":[]},{"doc_title":"3.3.3 Static variables | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/configuration/static-vars","doc_date":"2023-05-09T19:48:26.867Z","content":"Sample: ['ns', 'req.headers'] server_url : https : //api.example.com : 8443/v1/api httpbin : # sample api datasource url base_url : https : //httpbin.org log_level is the minimum log level to log. Log messages with a lower limit will not get logged. The default value is 'info' The available levels are 'fatal', 'error', 'warn', 'info', 'debug', 'trace' or 'silent' lang is the language used for scripting in the workflows. The default value is 'coffee' The available values are 'coffee' or 'js' Refer Coffee/JS scripting for more information. redact is the list of fields, the values for which, you want to hide from the logs. The default value is blank. Refer Logs field masking for more information. server_url is the custom server url which you want to use as Servers in swagger specs/auto generated documentation. Refer Custom Server URL.","content_length":842,"content_tokens":215,"embedding":[]}]},{"title":"3.1 Getting started | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/getting-started","date":"2023-05-09T19:48:27.015Z","content":"3. Setup 3.1 Getting started On this page Getting started Hereby is a step by step guide on running your first project. The setup is independent of the OS you are running it on. info You can also refer to tutorial on Getting Started with Godspeed . 3.1.1 Glossary ​ gs_service : The framework code version. During this setup, you will be asked to select the version of gs_service. Remote containers/Dev containers : Refer VSCode Remote containers for more information. 3.1.2 Pre-requisites ​ Please ensure you have the following in your machine NVM, with Node LTS installed (Currently 16+) Visual Studio Code LTS, with the following plugins installed: Remote Containers Run on Save Refer Run On Save Godspeed Extension Pack Docker-desktop should be up and running. On Linux systems, please ensure that docker compose plugin is installed. You can verify it by executing docker compose version command. Refer Install Compose plugin for more information. Git Hardware recommendations RAM: 8GB Hard Disk: SSD tip Depending your setup, you may need to run the above command using administrator privileges On Windows machines, sometimes Docker-desktop doesn't start. Make sure you have WSL installed with Ubuntu 18.04, for Docker to work fine. 3.1.3 Steps to get started ​ Step1: Install the Godspeed CLI ​ npm install -g @mindgrep/godspeed Step 2: Setting up a project on your local machine ​ note If you are creating a new project then follow section 2.1 OR If you are setting up a project from any existing git repository then follow section 2.2 2.1 Create a new project ​ godspeed create my_test_project During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements. By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. 2.2 Setting up a project from an existing GIT repository ​ Clone the git repository on your local machine. cd <your git repo> godspeed update During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements. By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. Step3: cd to your project ​ cd <your project directory> Step4: Start Visual Studio from the project directory ​ code . Step 5: Open in Dev container ​ Again click on the dev container tray icon. If this is your first time, click on Open folder in Dev Container . Else for every other time, click on Re-open in Dev Container Step 6: Building the project ​ godspeed build Step 7: Start the service for local development in watch mode ​ godspeed dev tip With the dev container running, we have auto watch and auto build enabled when you make changes to your project files. You don't need to run build manually everytime you make changes. 3.1.4 Time to start the development ​ If you have successfully reached here, then it is time to start the development of your project!","tokens":670,"length":3075,"chunks":[{"doc_title":"3.1 Getting started | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/getting-started","doc_date":"2023-05-09T19:48:27.015Z","content":"3. Setup 3.1 Getting started On this page Getting started Hereby is a step by step guide on running your first project. The setup is independent of the OS you are running it on. info You can also refer to tutorial on Getting Started with Godspeed  3.1.1 Glossary ​ gs_service : The framework code version. During this setup, you will be asked to select the version of gs_service. Remote containers/Dev containers : Refer VSCode Remote containers for more information. 3.1.2 Pre-requisites ​ Please ensure you have the following in your machine NVM, with Node LTS installed (Currently 16+) Visual Studio Code LTS, with the following plugins installed: Remote Containers Run on Save Refer Run On Save Godspeed Extension Pack Docker-desktop should be up and running. On Linux systems, please ensure that docker compose plugin is installed. You can verify it by executing docker compose version command.","content_length":899,"content_tokens":195,"embedding":[]},{"doc_title":"3.1 Getting started | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/getting-started","doc_date":"2023-05-09T19:48:27.015Z","content":"Refer Install Compose plugin for more information. Git Hardware recommendations RAM: 8GB Hard Disk: SSD tip Depending your setup, you may need to run the above command using administrator privileges On Windows machines, sometimes Docker-desktop doesn't start. Make sure you have WSL installed with Ubuntu 18.04, for Docker to work fine. 3.1.3 Steps to get started ​ Step1: Install the Godspeed CLI ​ npm install -g @mindgrep/godspeed Step 2: Setting up a project on your local machine ​ note If you are creating a new project then follow section 2.1 OR If you are setting up a project from any existing git repository then follow section 2.2 2.1 Create a new project ​ godspeed create my_test_project During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements.","content_length":839,"content_tokens":189,"embedding":[]},{"doc_title":"3.1 Getting started | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/getting-started","doc_date":"2023-05-09T19:48:27.015Z","content":"By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. 2.2 Setting up a project from an existing GIT repository ​ Clone the git repository on your local machine. cd <your git repo> godspeed update During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements. By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. Step3: cd to your project ​ cd <your project directory> Step4: Start Visual Studio from the project directory ​ code  Step 5: Open in Dev container ​ Again click on the dev container tray icon. If this is your first time, click on Open folder in Dev Container","content_length":824,"content_tokens":177,"embedding":[]},{"doc_title":"3.1 Getting started | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/getting-started","doc_date":"2023-05-09T19:48:27.015Z","content":"Else for every other time, click on Re-open in Dev Container Step 6: Building the project ​ godspeed build Step 7: Start the service for local development in watch mode ​ godspeed dev tip With the dev container running, we have auto watch and auto build enabled when you make changes to your project files. You don't need to run build manually everytime you make changes. 3.1.4 Time to start the development ​ If you have successfully reached here, then it is time to start the development of your project!","content_length":506,"content_tokens":108,"embedding":[]}]},{"title":"3.2 Project structure | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/scaffolding","date":"2023-05-09T19:48:27.168Z","content":"3. Setup 3.2 Project structure On this page Introduction The project root folder gets created in current folder under the projectName which is used in godspeed create command using godspeed CLI. The project contains two folders: src/ and config/ . Click here for more information on godspeed create command. 3.2.1 Scaffolding & Project structure ​ Project Structure with no examples ​ The project contains blank structure with no examples/templates when it is created using godspeed create -n command option. Refer command here for more information. . ├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml │ ├── index.yaml │ └── telemetry ├── package.json └── src ├── datasources ├── events ├── functions └── mappings Project Structure with examples ​ The project contains following heirarchy with examples when it is created without using godspeed create -n command option. Refer command here for more information. . ├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml │ ├── index.yaml │ └── telemetry │ └── index.yaml ├── package.json └── src ├── datasources │ └── httpbin.yaml ├── events │ ├── call_another_workflow.yaml │ ├── document.yaml │ ├── helloworld.yaml │ ├── httpbin_anything.yaml │ ├── run_tasks_in_parallel.yaml │ ├── sum.yaml │ └── switch_case.yaml ├── functions │ └── com │ └── biz │ ├── call_another_wf.yaml │ ├── documents │ │ └── upload_file.yaml │ ├── helloworld.yaml │ ├── httpbin_anything.yaml │ ├── run_tasks_in_parallel.yaml │ ├── sub_wf.yaml │ ├── sum.js │ ├── sum_workflow.yaml │ └── switch_case.yaml └── mappings └── index.yaml","tokens":436,"length":1589,"chunks":[{"doc_title":"3.2 Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/scaffolding","doc_date":"2023-05-09T19:48:27.168Z","content":"3. Setup 3.2 Project structure On this page Introduction The project root folder gets created in current folder under the projectName which is used in godspeed create command using godspeed CLI. The project contains two folders: src/ and config/  Click here for more information on godspeed create command. 3.2.1 Scaffolding & Project structure ​ Project Structure with no examples ​ The project contains blank structure with no examples/templates when it is created using godspeed create -n command option. Refer command here for more information.  ├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml │ ├── index.yaml │ └── telemetry ├── package.json └── src ├── datasources ├── events ├── functions └── mappings Project Structure with examples ​ The project contains following heirarchy with examples when it is created without using godspeed create -n command option. Refer command here for more information.","content_length":929,"content_tokens":196,"embedding":[]},{"doc_title":"3.2 Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/scaffolding","doc_date":"2023-05-09T19:48:27.168Z","content":"├── config │ ├── custom-environment-variables.yaml │ ├── default.yaml │ ├── index.yaml │ └── telemetry │ └── index.yaml ├── package.json └── src ├── datasources │ └── httpbin.yaml ├── events │ ├── call_another_workflow.yaml │ ├── document.yaml │ ├── helloworld.yaml │ ├── httpbin_anything.yaml │ ├── run_tasks_in_parallel.yaml │ ├── sum.yaml │ └── switch_case.yaml ├── functions │ └── com │ └── biz │ ├── call_another_wf.yaml │ ├── documents │ │ └── upload_file.yaml │ ├── helloworld.yaml │ ├── httpbin_anything.yaml │ ├── run_tasks_in_parallel.yaml │ ├── sub_wf.yaml │ ├── sum.js │ ├── sum_workflow.yaml │ └── switch_case.yaml └── mappings └── index.yaml.","content_length":656,"content_tokens":242,"embedding":[]}]},{"title":"3.4 Tests | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/setup/tests","date":"2023-05-09T19:48:27.293Z","content":"3. Setup 3.4 Tests Introduction Godspeed provides a facility to auto-generate and run test suite using CLI. Click on auto-generate test suite for more information on generating the test suite and run test suite for more information on running the test suite.","tokens":54,"length":258,"chunks":[{"doc_title":"3.4 Tests | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/setup/tests","doc_date":"2023-05-09T19:48:27.293Z","content":"3. Setup 3.4 Tests Introduction Godspeed provides a facility to auto-generate and run test suite using CLI. Click on auto-generate test suite for more information on generating the test suite and run test suite for more information on running the test suite.","content_length":258,"content_tokens":54,"embedding":[]}]},{"title":"5. Swagger Specs | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/swagger-specs","date":"2023-05-09T19:48:27.442Z","content":"5. Swagger Specs On this page Introduction You can access autogenerated Swagger API specifications at <domain name>/api-docs url. For example, http://localhost:3000/api-docs Godspeed also provides a facility to auto-generate OAS 3 documentation using CLI. 5.1 CLI command to generate documentation ​ You can generate OAS3 documentation using godspeed gen-api-docs CLI command. 5.2 Custom Server URL ​ You can add custom server URL for API documentation in static configuration By adding the custom server url, your autogenerated documentation or swagger specs will have this url set in the Servers . server_url: https://api.example.com:8443/v1/api For example,","tokens":157,"length":660,"chunks":[{"doc_title":"5. Swagger Specs | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/swagger-specs","doc_date":"2023-05-09T19:48:27.442Z","content":"5. Swagger Specs On this page Introduction You can access autogenerated Swagger API specifications at <domain name>/api-docs url. For example, http://localhost:3000/api-docs Godspeed also provides a facility to auto-generate OAS 3 documentation using CLI. 5.1 CLI command to generate documentation ​ You can generate OAS3 documentation using godspeed gen-api-docs CLI command. 5.2 Custom Server URL ​ You can add custom server URL for API documentation in static configuration By adding the custom server url, your autogenerated documentation or swagger specs will have this url set in the Servers . server_url: https://api.example.com:8443/v1/api For example,","content_length":660,"content_tokens":157,"embedding":[]}]},{"title":"Technologies used (Default) | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/technology-used/intro","date":"2023-05-09T19:48:27.573Z","content":"Technologies used (Default) Nodejs Express (HTTP server) Webassembly (multiple language support in Nodejs) Temporal (microservice orchestration and distributed transactions)","tokens":34,"length":173,"chunks":[{"doc_title":"Technologies used (Default) | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/technology-used/intro","doc_date":"2023-05-09T19:48:27.573Z","content":"Technologies used (Default) Nodejs Express (HTTP server) Webassembly (multiple language support in Nodejs) Temporal (microservice orchestration and distributed transactions)","content_length":173,"content_tokens":34,"embedding":[]}]},{"title":"Workflows | Godspeed Docs","url":"https://docs.godspeed.systems/docs/microservices/workflows","date":"2023-05-09T19:48:27.791Z","content":"7. Workflows On this page Workflows Workflows is where the actual computation and flow orchestration happens. The framework supports a YAML based DSL to write workflows and tasks containing the business logic. These workflows can be attached to the events as their handlers, or called from within another workflow. The framework exposes CoffeeScript /JS based expressions for evaluation of dynamic variables or transformation of data from inputs of event, or outputs of previous tasks. Default language for transformations (coffee/js) can be configured in configuration 7.1 The structure of workflows ​ A workflow has the following attributes summary - the title description - more details id - Recommended for better logging visibility on_error - Default error handling if any tasks fails. tasks - the tasks (workflows or sub-workflows) to be run in series (sequence, or one by one). The tasks invoke other workflows written in YAML or JS/TS. Other languages support is planned. summary : Hello world description : Hello world example which invokes the com.gs.return workflow id : hello_world # needed for better logging visibility on_error : continue : false response : success : false code : 500 data : \"Default error\" tasks : # tasks to be run in sequence (default is sequence) - id : step1 ## id of this task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below. fn : com.gs.return args : 'Hello World!' # com.gs.return takes its return value as `args`. Hence the args key. 7.2 The tasks within workflows ​ A workflow has one or more tasks associated with it. A task has the following attributes id - Needed for better logging visibility. It is compulsory for a task. Importantly, this is also used to access the output of this task in subsequent tasks in the outputs.{task_id} path, as shown in example below . summary - the title description - more details fn - The handler to be run in this task. It can be one of the framework functions , control functions (like parallel, sequential, switch), developer written functions , or another workflow. You can also use scripting in dynamic evaluation of a function name as given in below example. Refer Coffee/JS scripting for more information. summary : Call an API and transform the tasks : - id : transform_fn_step1 description : find fn name fn : com.gs.transform args : | <js% if (inputs.body.fn == 'sum') { return 'com.jfs.sum_workflow' } else { return 'com.jfs.helloworld' } %> - id : call_fn_step2 description : call fn returned in transform_fn_step1 fn : <% outputs.transform_fn_step1.data % > args : name : <% inputs.body.name % > args - Every handler fn has its own argument structure, which is kept in the args key. For example, id : httpbin_step1 fn : com.gs.http args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post headers : <% inputs.headers % > on_error - What to do if this task fails? on_error : #You can find sample usage of this in the examples below. Just search on_error in this page. continue : false # Whether the next task should be executed, in case this task fails. by default continue is true. response : <%Coffee/JS expression% > | String # If specified, the output of `response` is returned as the output of this task. If not specified, the error output is the default output of the failed task. tasks : # If specified, the tasks are executed in series/sequence. The output of the last task in these tasks is the default output of the failed task. - id : transform_error fn : com.gs.transform args : <% outputs.httpbin_step1 % > - id : publish_error fn : com.gs.kafka args : datasource : kafka1 data : value : <% outputs.transform_error.message % > config : topic : publish - producer1 The only exception to this is control functions like series, parallel, switch, which don't take the args , for the sake of more readability. retry - Retry logic helps to handle transient failures, internal server errors, and network errors with support for constant, exponential and random types. Currently applied only for com.gs.http workflow. retry : max_attempts : 5 type : constant interval : PT15m retry : max_attempts : 5 type : exponential interval : PT15s retry : max_attempts : 5 type : random min_interval : PT5s max_interval : PT10s Example of multiple task with arguments ​ summary : Workflow with switch - case and transform task id : example_switch_functionality_id description : | Run two tasks in series. Both take different arguments. First one is switch case task. Second is transform task which consumes the output of step1 and shapes the final output of this workflow. tasks : # tasks to be run in sequence (default is sequence) - id : step1_switch ## id of this switch task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below. fn : com.gs.switch # Switch workflow takes `value` and `cases` as arguments. The cases object specifies another task for every case. value : <%inputs.body.condition% > # Evaluation of dynamic values happens via <% %> cases : FIRST : id : 1st fn : com.gs.return args : \"'case - 1'\" SECOND : id : 2nd fn : com.gs.return args : \"'case - 2'\" THIRD : id : 3rd fn : com.gs.return args : \"'case - 3'\" defaults : id : default fn : com.gs.return args : <%inputs.body.default_return_val% > #coffee/js script for dyanmic evaluation. Wrapped in <% %>. Same as that used elsewhere in workflows for dynamic calculations and variable substitutions. For ex. as used in com.gs.transform and com.gs.return - id : step2 fn : com.gs.transform args : | #coffee for dyanmic evaluation. Wrapped in <% %> <coffee% { code : 200 , data : outputs [ '1st' ] } % > 7.3 Location and fully qualified name (id) of workflows and functions ​ All the workflows and functions are to be kept in the src/functions folder. Their directory tree path, followed by the file name becomes the workflow's fully qualified name or id, by which it can be referenced in the events or within other workflows. The JS function shown below will be available in workflows under the F.Q.N. com.biz.custom_function . Similarly, com.biz.create_hdfc_account , com.biz.create_parallel etc. are accessible as handlers from within other workflow tasks or events. 7.4 Referencing a workflow within an event or another workflow ​ A workflow task references and invokes other workflows written in either YAML or JS/TS, via the fn key. In future, other languages will also be supported. An event definition references the handler yaml workflows by their fully qualified name, via the same fn key. 7.5 Use of Coffee/JS for scripting ​ The framework provides coffee/js for Transformations in com.gs.transform and com.gs.return Dynamic evaluation or workflow or task variables, event variables, datasource variables. You will find its code in <% %> within various examples in this page below. Define language at global level ​ Default language for transformations (coffee/js) is configured in static configuration Define language at workflow level ​ Global configuration for language is overridden by defining specific language inside <coffee/js% %>. For example, - id: httpbinCof_step2 fn: com.gs.transform args: | <coffee% if outputs.httpbinCof_step1.data.json.code == 200 then { code: 200, success: true, data: outputs.httpbinCof_step1.data.json, headers: outputs.httpbinCof_step1.data.headers } else { code: 500, success: false, message: 'error in httpbinCof_step1' } %> - id: step1 # the response of this will be accessible within the parent step key, under the step1 sub key description: upload documents fn: com.gs.http args: datasource: httpbin params: data: | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> Built-in Javascript modules ​ You can use build-in javascript modules in inline scripting. Only synchronous methods of build-in modules are allowed in inline scripting. For example, summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : # fs is used directly in scripting in Body - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject 7.6 Inbuilt functions ​ The framework provides the following inbuilt functions 7.6.1 com.gs.http ​ Send HTTP events to other APIs in Axios compatible format. Example 1 summary : agreement esign id : agreement_esign tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : agreement esign fn : com.gs.http params : # query params to be sent in the request id : 123 args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : true - id : step2 fn : com.gs.transform args : | <%if outputs.step1.data.success then outputs.step1.data else { code: outputs.step1.code, success : false, data: { error_data: outputs.step1.data['error'], uuid: outputs.step1.data.uuid, status_code_error: outputs.step1.data.status_code_error, event: outputs.step1.data.event } }%> Example 2 summary : upload documents id : upload_documents tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload documents fn : com.gs.http args : datasource : httpbin params : data : | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : false response : <%'Some error happened in saving' + inputs.body.entity_type% > - id : step2 fn : com.gs.transform args : <% delete outputs.step1.headers; outputs.step1 % > 7.6.2 com.gs.kafka ​ Publish events on Kafka. summary : Publishing incoming event data to a Kafka topic id : push_to_kafka tasks : - id : step1 summary : Publish an event with input event's data , adding to_process = true fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : kyc_initiate_recieved group_id : kyc_domain data : # Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. value : <% inputs % > # Your message content. Evaluation of dynamic values happens via <% %>. The type of scripting is coffee. key : # Optional - Used for partitioning. partition : # Optional - Which partition to send the message to. timestamp : # Optional - The timestamp of when the message was created. headers : # Optional - Metadata to associate with your message. Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. 7.6.3 com.gs.datastore ​ The datastore function allows CRUD access to any supported datastore in a format extending Prisma API . summary : Create and read data tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Create entity from REST input data (POST request) fn : com.gs.datastore args : datasource : mongo # Which ds to use. data : <% inputs.body + { extra_field : its_value } % > config : method : <% inputs.params.entity_type % > .create - id : step2 # the response of this will be accessible within the parent step key, under the step1 sub key description : test again fn : com.gs.datastore args : datasource : mongo # Adding this knows which ds/model we are talking about here. config : # Similar approach as Axios method : <% inputs.params.entity_type % > .findMany 7.6.4 com.gs.elasticgraph ​ The elasticgraph function allows CRUD access to elasticsearch datastore . summary : eg tasks : - id : create_entity1 description : create_entity1 fn : com.gs.elasticgraph args : datasource : elasticgraph1 data : index : <% inputs.params.entity_type + 's' % > type : '_doc' body : <% inputs.body % > config : method : index on_error : continue : false 7.6.5 com.gs.transform ​ This function allows to transform data from one format to another using coffee/js scripting. summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : code : 200 data : <% outputs.step1_switch.data % > 7.6.6 com.gs.series ​ control flow function Executes the tasks in series. By default every top level workflow executes its task in series. But when invoking subworkflows if you need, you can explicitly use series workflow. Its syntax is same as parallel. summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.series tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.7 com.gs.parallel ​ control flow function Executes the child tasks in parallel. Syntax is same as com.gs.series summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : 3rd fn : com.gs.return args : | 'parallel task3' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.8 com.gs.switch ​ control flow function The classic switch-case flow execution The args of switch-flow are value and cases . value takes a coffee/js expression to be evaluated during runtime. Every case has a task associated with it. The task can invoke another function or a workflow. summary : create loan application for lender tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.switch value : <%inputs.headers [ 'lender' ] % > cases : httpbin : - id : 1st fn : com.biz.loan_application.httpbin_create_loan_application args : <%inputs% > 7.6.9 com.gs.each_sequential ​ control flow function The classic for-each flow execution The args is list of values in value field along with associated tasks. For each value in value tasks are executed sequentially. The final output each_sequential is the array of status of the last executed task of each iteration. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > on_error handling You can add on_error at task level as well as at each_sequential loop level. See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the loop else it continues the next tasks. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > 7.6.10 com.gs.each_parallel ​ The args is list of values in value field along with associated tasks. For each value in value tasks are executed in parallel. The final output each_parallel is the array of status of the last executed task of each iteration. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > on_error handling You can add on_error at task level as well as at each_parallel loop level. See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the execution for the next tasks in tasks for current task_value in value list. For example, in the below workflow, if each_task1 step of task_value 1 gets failed then each_task2 will not get executed on continue false. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String - id : each_task2 fn : com.gs.transform args : <% 'each_task2 ' + task_value % > on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > 7.6.11 com.gs.return ​ return statement The classic return statement It returns from the current function to the function caller. The function stops executing when the return statement is called. summary : Multiplexing create loan for hdfc api calls id : helloworld tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.return args : | <coffee% 'Hello ' + inputs.query.name %> 7.6.12 com.gs.log ​ It logs the intermediate inputs/outputs during the workflow execution in pino logging format. The args are level and data . level takes any value from the Pino log levels and data takes a coffee/js expression to be evaluated during runtime or anything (like string, number, etc.) which you want to get logged during the workflow execution. summary : Summing x + y description : Here we sum two hardcoded x and y values. Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.jfs.sum args : x : 1 y : 2 - id : sum_step2 description : log the output in logs fn : com.gs.log args : level : info # log levels: info, debug, error, warn, fatal, silent, trace data : <% outputs.sum_step1 % > - id : sum_step3 description : return the response fn : com.gs.transform args : <% outputs.sum_step1 % > 7.6.13 com.gs.dynamic_fn ​ It executes the workflow whose name is dynamically returned as the output of its task list. The tasks of this function should return a string output which will be the name of the workflow to be executed. Event DSL '/sum.http.get' : fn : com.jfs.sum_dynamic summary : A workflow to sum x and y description : This workflow sums two integers params : - name : x in : query required : true allow_empty_value : false schema : type : string - name : y in : query required : true allow_empty_value : false schema : type : string com.jfs.sum_dynamic.yaml summary : Dynamic function to call com.jfs.sum_workflow.yaml description : This function dynamically is taking workflow name and executing it at the runtime. tasks : - id : sum_dynamic_step1 description : add two numbers fn : com.gs.dynamic_fn tasks : # the tasks should return a string value which will the name of the workflow to be executed. # For example, in below task list, final workflow name will be `com.jfs.sum_workflow` - id : get_wf_name_step1 fn : com.gs.transform args : com.jfs.sum_workflow - id : get_wf_name_step2 # this task is returning a workflow name dynamically fn : com.gs.transform args : <% outputs.get_wf_name_step1.data % > com.jfs.sum_workflow.yaml summary : Summing x + y description : Here we sum two hardcoded x and y values. Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.gs.return args : | <% +inputs.query.x + +inputs.query.y %> 7.6.14 com.gs.aws ​ Interacts with AWS to use its various services and methods. params is the list of params to the AWS service methods. We are using AWS v3 style services. Please refer AWS S3 for AWS S3 methods. summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject 7.6.15 com.gs.redis ​ Developer can read / write to redis datasource using standard redis client functions. summary : demonstration of redis functions id : accessing_redis tasks : - id : store_value_to_key description : Writing user info in redis with key user fn : com.gs.redis args : config : method : set data : key : user value : Adam - id : retrieve_user_set_in_previous_task description : Retriving user from redis fn : com.gs.redis args : config : method : get data : key : user 7.6.16 com.gs.if, com.gs.elif, com.gs.else ​ control flow function The classic if-else flow execution The args are condition and tasks . condition takes a coffee/js expression to be evaluated during runtime. The tasks can invoke another function or a workflow. summary : Returning hello world tasks : - id : if fn : com.gs.if condition : <% inputs.query.status == 'Hello' % > tasks : - id : step1 description : Return hello world fn : com.gs.return args : 'Hello!' - id : elif1 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hell' % > tasks : - id : step2 description : Return hello world fn : com.gs.return args : 'Hell!' - id : elif2 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hel' % > tasks : - id : step3 description : Return hello world fn : com.gs.return args : 'Hel!' - id : else description : Return hello world fn : com.gs.else tasks : - id : step4 description : Return hello world fn : com.gs.return args : 'Hi!' 7.7 Developer written functions ​ Developer can write functions in JS/TS and kept in src/functions folder at a path, which becomes its fully qualified name. Other languages support is planned. Once it is written, the function can be invoked from within any workflow or sub-workflow, with its fully qualified name and argument structure. summary : Custom workflow invocation id : custom_function tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : custom_fn fn : com.biz.custom_function # Can be JS/TS workflow in src/com/xyz directory with filename being custom.{js|ts} args : arg1 : 'hello world' arg2 : 'hello again' 7.8 Headers defined at workflow level ​ Headers defined at workflow level are applicable for a single workflow only. You can find the example usage here 7.9 File Upload feature ​ The framework provides file upload feature to upload files. Here is the sample event and workflow spec to upload any file. Event Spec /document.http.post : fn : com.biz.documents.upload_file id : '/sendDocuments' summary : upload document description : upload document on httpbin data : schema : body : required : false content : multipart/form-data : schema : type : object properties : fileName : type : string format : binary 7.9.1 Workflow spec to upload files with same file key ​ summary : upload file id : upload_file tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload docfileuments fn : com.gs.http args : datasource : httpbin params : file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M Note If file_key is same for all the files then you can use above workflow DSL. In case you have different file_keys for multiple files then you can directly use <% inputs.file_obj %> as given in the below section 6.9.2 7.9.2 Workflow spec to upload multiple files with different file keys ​ summary : upload multiple documents tasks : - id : upload_multiple_files_step1 description : upload multiple documents fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : <% inputs.file_obj % > config : url : /anything method : post 7.9.3 Workflow spec to upload file directly from URL ​ summary : upload document from url tasks : - id : upload_url_step1 description : upload document from url fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : sample : url : https : //s3.ap - south - 1.amazonaws.com/sample.pdf method : get config : url : /anything method : post headers : Content-Type : 'multipart/form-data'","tokens":6808,"length":25880,"chunks":[{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"7. Workflows On this page Workflows Workflows is where the actual computation and flow orchestration happens. The framework supports a YAML based DSL to write workflows and tasks containing the business logic. These workflows can be attached to the events as their handlers, or called from within another workflow. The framework exposes CoffeeScript /JS based expressions for evaluation of dynamic variables or transformation of data from inputs of event, or outputs of previous tasks. Default language for transformations (coffee/js) can be configured in configuration 7.1 The structure of workflows ​ A workflow has the following attributes summary - the title description - more details id - Recommended for better logging visibility on_error - Default error handling if any tasks fails. tasks - the tasks (workflows or sub-workflows) to be run in series (sequence, or one by one) The tasks invoke other workflows written in YAML or JS/TS. Other languages support is planned.","content_length":978,"content_tokens":196,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Hello world description : Hello world example which invokes the com.gs.return workflow id : hello_world # needed for better logging visibility on_error : continue : false response : success : false code : 500 data : \"Default error\" tasks : # tasks to be run in sequence (default is sequence) - id : step1 ## id of this task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below. fn : com.gs.return args : 'Hello World!' # com.gs.return takes its return value as `args` Hence the args key. 7.2 The tasks within workflows ​ A workflow has one or more tasks associated with it. A task has the following attributes id - Needed for better logging visibility. It is compulsory for a task.","content_length":747,"content_tokens":177,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Importantly, this is also used to access the output of this task in subsequent tasks in the outputs.{task_id} path, as shown in example below  summary - the title description - more details fn - The handler to be run in this task. It can be one of the framework functions , control functions (like parallel, sequential, switch), developer written functions , or another workflow. You can also use scripting in dynamic evaluation of a function name as given in below example. Refer Coffee/JS scripting for more information.","content_length":522,"content_tokens":107,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Call an API and transform the tasks : - id : transform_fn_step1 description : find fn name fn : com.gs.transform args : | <js% if (inputs.body.fn == 'sum') { return 'com.jfs.sum_workflow' } else { return 'com.jfs.helloworld' } %> - id : call_fn_step2 description : call fn returned in transform_fn_step1 fn : <% outputs.transform_fn_step1.data % > args : name : <% inputs.body.name % > args - Every handler fn has its own argument structure, which is kept in the args key.","content_length":482,"content_tokens":148,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"For example, id : httpbin_step1 fn : com.gs.http args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post headers : <% inputs.headers % > on_error - What to do if this task fails? on_error : #You can find sample usage of this in the examples below. Just search on_error in this page. continue : false # Whether the next task should be executed, in case this task fails. by default continue is true. response : <%Coffee/JS expression% > | String # If specified, the output of `response` is returned as the output of this task. If not specified, the error output is the default output of the failed task.","content_length":699,"content_tokens":187,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"tasks : # If specified, the tasks are executed in series/sequence. The output of the last task in these tasks is the default output of the failed task. - id : transform_error fn : com.gs.transform args : <% outputs.httpbin_step1 % > - id : publish_error fn : com.gs.kafka args : datasource : kafka1 data : value : <% outputs.transform_error.message % > config : topic : publish - producer1 The only exception to this is control functions like series, parallel, switch, which don't take the args , for the sake of more readability. retry - Retry logic helps to handle transient failures, internal server errors, and network errors with support for constant, exponential and random types. Currently applied only for com.gs.http workflow.","content_length":735,"content_tokens":177,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"retry : max_attempts : 5 type : constant interval : PT15m retry : max_attempts : 5 type : exponential interval : PT15s retry : max_attempts : 5 type : random min_interval : PT5s max_interval : PT10s Example of multiple task with arguments ​ summary : Workflow with switch - case and transform task id : example_switch_functionality_id description : | Run two tasks in series. Both take different arguments. First one is switch case task. Second is transform task which consumes the output of step1 and shapes the final output of this workflow. tasks : # tasks to be run in sequence (default is sequence) - id : step1_switch ## id of this switch task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below.","content_length":759,"content_tokens":189,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"fn : com.gs.switch # Switch workflow takes `value` and `cases` as arguments. The cases object specifies another task for every case. value : <%inputs.body.condition% > # Evaluation of dynamic values happens via <% %> cases : FIRST : id : 1st fn : com.gs.return args : \"'case - 1'\" SECOND : id : 2nd fn : com.gs.return args : \"'case - 2'\" THIRD : id : 3rd fn : com.gs.return args : \"'case - 3'\" defaults : id : default fn : com.gs.return args : <%inputs.body.default_return_val% > #coffee/js script for dyanmic evaluation. Wrapped in <% %> Same as that used elsewhere in workflows for dynamic calculations and variable substitutions. For ex.","content_length":640,"content_tokens":184,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"as used in com.gs.transform and com.gs.return - id : step2 fn : com.gs.transform args : | #coffee for dyanmic evaluation. Wrapped in <% %> <coffee% { code : 200 , data : outputs [ '1st' ] } % > 7.3 Location and fully qualified name (id) of workflows and functions ​ All the workflows and functions are to be kept in the src/functions folder. Their directory tree path, followed by the file name becomes the workflow's fully qualified name or id, by which it can be referenced in the events or within other workflows. The JS function shown below will be available in workflows under the F.Q.N. com.biz.custom_function  Similarly, com.biz.create_hdfc_account , com.biz.create_parallel etc. are accessible as handlers from within other workflow tasks or events.","content_length":758,"content_tokens":199,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"7.4 Referencing a workflow within an event or another workflow ​ A workflow task references and invokes other workflows written in either YAML or JS/TS, via the fn key. In future, other languages will also be supported. An event definition references the handler yaml workflows by their fully qualified name, via the same fn key. 7.5 Use of Coffee/JS for scripting ​ The framework provides coffee/js for Transformations in com.gs.transform and com.gs.return Dynamic evaluation or workflow or task variables, event variables, datasource variables. You will find its code in <% %> within various examples in this page below. Define language at global level ​ Default language for transformations (coffee/js) is configured in static configuration Define language at workflow level ​ Global configuration for language is overridden by defining specific language inside <coffee/js% %>","content_length":879,"content_tokens":187,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"For example, - id: httpbinCof_step2 fn: com.gs.transform args: | <coffee% if outputs.httpbinCof_step1.data.json.code == 200 then { code: 200, success: true, data: outputs.httpbinCof_step1.data.json, headers: outputs.httpbinCof_step1.data.headers } else { code: 500, success: false, message: 'error in httpbinCof_step1' } %> - id: step1 # the response of this will be accessible within the parent step key, under the step1 sub key description: upload documents fn: com.gs.http args: datasource: httpbin params: data: | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> Built-in Javascript modules ​ You can use build-in javascript modules in inline scripting.","content_length":726,"content_tokens":229,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Only synchronous methods of build-in modules are allowed in inline scripting. For example, summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : # fs is used directly in scripting in Body - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject 7.6 Inbuilt functions ​ The framework provides the following inbuilt functions 7.6.1 com.gs.http ​ Send HTTP events to other APIs in Axios compatible format.","content_length":563,"content_tokens":153,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Example 1 summary : agreement esign id : agreement_esign tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : agreement esign fn : com.gs.http params : # query params to be sent in the request id : 123 args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : true - id : step2 fn : com.gs.transform args : | <%if outputs.step1.data.success then outputs.step1.data else { code: outputs.step1.code, success : false, data: { error_data: outputs.step1.data['error'], uuid: outputs.step1.data.uuid, status_code_error: outputs.step1.data.status_code_error, event: outputs.step1.data.event } }%> Example 2 summary : upload documents id : upload_documents tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload documents fn : com.gs.http args : datasource : httpbin params : data : | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : false response : <%'Some error happened in saving' + inputs.body.entity_type% > - id : step2 fn : com.gs.transform args : <% delete outputs.step1.headers; outputs.step1 % > 7.6.2 com.gs.kafka ​ Publish events on Kafka.","content_length":1614,"content_tokens":486,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Publishing incoming event data to a Kafka topic id : push_to_kafka tasks : - id : step1 summary : Publish an event with input event's data , adding to_process = true fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : kyc_initiate_recieved group_id : kyc_domain data : # Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. value : <% inputs % > # Your message content. Evaluation of dynamic values happens via <% %> The type of scripting is coffee. key : # Optional - Used for partitioning. partition : # Optional - Which partition to send the message to. timestamp : # Optional - The timestamp of when the message was created.","content_length":734,"content_tokens":188,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"headers : # Optional - Metadata to associate with your message. Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. 7.6.3 com.gs.datastore ​ The datastore function allows CRUD access to any supported datastore in a format extending Prisma API  summary : Create and read data tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Create entity from REST input data (POST request) fn : com.gs.datastore args : datasource : mongo # Which ds to use.","content_length":564,"content_tokens":142,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"data : <% inputs.body + { extra_field : its_value } % > config : method : <% inputs.params.entity_type % > .create - id : step2 # the response of this will be accessible within the parent step key, under the step1 sub key description : test again fn : com.gs.datastore args : datasource : mongo # Adding this knows which ds/model we are talking about here. config : # Similar approach as Axios method : <% inputs.params.entity_type % > .findMany 7.6.4 com.gs.elasticgraph ​ The elasticgraph function allows CRUD access to elasticsearch datastore","content_length":545,"content_tokens":147,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : eg tasks : - id : create_entity1 description : create_entity1 fn : com.gs.elasticgraph args : datasource : elasticgraph1 data : index : <% inputs.params.entity_type + 's' % > type : '_doc' body : <% inputs.body % > config : method : index on_error : continue : false 7.6.5 com.gs.transform ​ This function allows to transform data from one format to another using coffee/js scripting.","content_length":394,"content_tokens":109,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : code : 200 data : <% outputs.step1_switch.data % > 7.6.6 com.gs.series ​ control flow function Executes the tasks in series. By default every top level workflow executes its task in series. But when invoking subworkflows if you need, you can explicitly use series workflow. Its syntax is same as parallel.","content_length":575,"content_tokens":160,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.series tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.7 com.gs.parallel ​ control flow function Executes the child tasks in parallel.","content_length":401,"content_tokens":128,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Syntax is same as com.gs.series summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : 3rd fn : com.gs.return args : | 'parallel task3' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.8 com.gs.switch ​ control flow function The classic switch-case flow execution The args of switch-flow are value and cases  value takes a coffee/js expression to be evaluated during runtime. Every case has a task associated with it.","content_length":644,"content_tokens":191,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"The task can invoke another function or a workflow. summary : create loan application for lender tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.switch value : <%inputs.headers [ 'lender' ] % > cases : httpbin : - id : 1st fn : com.biz.loan_application.httpbin_create_loan_application args : <%inputs% > 7.6.9 com.gs.each_sequential ​ control flow function The classic for-each flow execution The args is list of values in value field along with associated tasks. For each value in value tasks are executed sequentially. The final output each_sequential is the array of status of the last executed task of each iteration.","content_length":741,"content_tokens":182,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > on_error handling You can add on_error at task level as well as at each_sequential loop level. See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the loop else it continues the next tasks. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed.","content_length":929,"content_tokens":238,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > 7.6.10 com.gs.each_parallel ​ The args is list of values in value field along with associated tasks.","content_length":712,"content_tokens":205,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"For each value in value tasks are executed in parallel. The final output each_parallel is the array of status of the last executed task of each iteration. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > on_error handling You can add on_error at task level as well as at each_parallel loop level.","content_length":654,"content_tokens":177,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the execution for the next tasks in tasks for current task_value in value list. For example, in the below workflow, if each_task1 step of task_value 1 gets failed then each_task2 will not get executed on continue false. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed.","content_length":597,"content_tokens":135,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String - id : each_task2 fn : com.gs.transform args : <% 'each_task2 ' + task_value % > on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > 7.6.11 com.gs.return ​ return statement The classic return statement It returns from the current function to the function caller.","content_length":814,"content_tokens":235,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"The function stops executing when the return statement is called. summary : Multiplexing create loan for hdfc api calls id : helloworld tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.return args : | <coffee% 'Hello ' + inputs.query.name %> 7.6.12 com.gs.log ​ It logs the intermediate inputs/outputs during the workflow execution in pino logging format. The args are level and data  level takes any value from the Pino log levels and data takes a coffee/js expression to be evaluated during runtime or anything (like string, number, etc.) which you want to get logged during the workflow execution. summary : Summing x + y description : Here we sum two hardcoded x and y values.","content_length":799,"content_tokens":188,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.jfs.sum args : x : 1 y : 2 - id : sum_step2 description : log the output in logs fn : com.gs.log args : level : info # log levels: info, debug, error, warn, fatal, silent, trace data : <% outputs.sum_step1 % > - id : sum_step3 description : return the response fn : com.gs.transform args : <% outputs.sum_step1 % > 7.6.13 com.gs.dynamic_fn ​ It executes the workflow whose name is dynamically returned as the output of its task list. The tasks of this function should return a string output which will be the name of the workflow to be executed.","content_length":665,"content_tokens":181,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Event DSL '/sum.http.get' : fn : com.jfs.sum_dynamic summary : A workflow to sum x and y description : This workflow sums two integers params : - name : x in : query required : true allow_empty_value : false schema : type : string - name : y in : query required : true allow_empty_value : false schema : type : string com.jfs.sum_dynamic.yaml summary : Dynamic function to call com.jfs.sum_workflow.yaml description : This function dynamically is taking workflow name and executing it at the runtime. tasks : - id : sum_dynamic_step1 description : add two numbers fn : com.gs.dynamic_fn tasks : # the tasks should return a string value which will the name of the workflow to be executed.","content_length":687,"content_tokens":177,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"# For example, in below task list, final workflow name will be `com.jfs.sum_workflow` - id : get_wf_name_step1 fn : com.gs.transform args : com.jfs.sum_workflow - id : get_wf_name_step2 # this task is returning a workflow name dynamically fn : com.gs.transform args : <% outputs.get_wf_name_step1.data % > com.jfs.sum_workflow.yaml summary : Summing x + y description : Here we sum two hardcoded x and y values.","content_length":411,"content_tokens":134,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.gs.return args : | <% +inputs.query.x + +inputs.query.y %> 7.6.14 com.gs.aws ​ Interacts with AWS to use its various services and methods. params is the list of params to the AWS service methods. We are using AWS v3 style services. Please refer AWS S3 for AWS S3 methods.","content_length":391,"content_tokens":110,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject 7.6.15 com.gs.redis ​ Developer can read / write to redis datasource using standard redis client functions.","content_length":379,"content_tokens":117,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : demonstration of redis functions id : accessing_redis tasks : - id : store_value_to_key description : Writing user info in redis with key user fn : com.gs.redis args : config : method : set data : key : user value : Adam - id : retrieve_user_set_in_previous_task description : Retriving user from redis fn : com.gs.redis args : config : method : get data : key : user 7.6.16 com.gs.if, com.gs.elif, com.gs.else ​ control flow function The classic if-else flow execution The args are condition and tasks  condition takes a coffee/js expression to be evaluated during runtime. The tasks can invoke another function or a workflow.","content_length":637,"content_tokens":167,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"summary : Returning hello world tasks : - id : if fn : com.gs.if condition : <% inputs.query.status == 'Hello' % > tasks : - id : step1 description : Return hello world fn : com.gs.return args : 'Hello!' - id : elif1 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hell' % > tasks : - id : step2 description : Return hello world fn : com.gs.return args : 'Hell!' - id : elif2 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hel' % > tasks : - id : step3 description : Return hello world fn : com.gs.return args : 'Hel!' - id : else description : Return hello world fn : com.gs.else tasks : - id : step4 description : Return hello world fn : com.gs.return args : 'Hi!' 7.7 Developer written functions ​ Developer can write functions in JS/TS and kept in src/functions folder at a path, which becomes its fully qualified name.","content_length":906,"content_tokens":247,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Other languages support is planned. Once it is written, the function can be invoked from within any workflow or sub-workflow, with its fully qualified name and argument structure. summary : Custom workflow invocation id : custom_function tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : custom_fn fn : com.biz.custom_function # Can be JS/TS workflow in src/com/xyz directory with filename being custom.{js|ts} args : arg1 : 'hello world' arg2 : 'hello again' 7.8 Headers defined at workflow level ​ Headers defined at workflow level are applicable for a single workflow only. You can find the example usage here 7.9 File Upload feature ​ The framework provides file upload feature to upload files. Here is the sample event and workflow spec to upload any file.","content_length":840,"content_tokens":188,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"Event Spec /document.http.post : fn : com.biz.documents.upload_file id : '/sendDocuments' summary : upload document description : upload document on httpbin data : schema : body : required : false content : multipart/form-data : schema : type : object properties : fileName : type : string format : binary 7.9.1 Workflow spec to upload files with same file key ​ summary : upload file id : upload_file tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload docfileuments fn : com.gs.http args : datasource : httpbin params : file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M Note If file_key is same for all the files then you can use above workflow DSL.","content_length":831,"content_tokens":210,"embedding":[]},{"doc_title":"Workflows | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/microservices/workflows","doc_date":"2023-05-09T19:48:27.791Z","content":"In case you have different file_keys for multiple files then you can directly use <% inputs.file_obj %> as given in the below section 6.9.2 7.9.2 Workflow spec to upload multiple files with different file keys ​ summary : upload multiple documents tasks : - id : upload_multiple_files_step1 description : upload multiple documents fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : <% inputs.file_obj % > config : url : /anything method : post 7.9.3 Workflow spec to upload file directly from URL ​ summary : upload document from url tasks : - id : upload_url_step1 description : upload document from url fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : sample : url : https : //s3.ap - south - 1.amazonaws.com/sample.pdf method : get config : url : /anything method : post headers : Content-Type : 'multipart/form-data'","content_length":878,"content_tokens":233,"embedding":[]}]},{"title":"Notification API | Godspeed Docs","url":"https://docs.godspeed.systems/docs/notification-api","date":"2023-05-09T19:48:27.979Z","content":"On this page Notification API This will cover Email, SMS, whatsapp. It is open to extend any other type of notification and channel. The response will be in JSON format by default. Templates are supported in email and SMS. These templates must be created on providers. Developer must be passing the templateID and placeholder values in sequence. Proposed first integration providers : ​ email providers : Sendgrid, others to be filled after client's discussion SMS providers : to be filled after client's discussion whatsapp providers : to be filled after client's discussion Notification service can be run as independent microservices and as a module within other microservices. ​ 1. SendEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: /api/notification/v1/publish/sendEmail Parameters for the request Json ​ recipientsTo STRING message STRING from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be validated as valid email format. ​ 2. sendBulkEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkEmail Parameters for the request Json ​ recipientsToList [STRING, STRING...] message STRING from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER 3. sendBulkTemplateEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkTemplateEmail Parameters for the request Json ​ recipientsToList [STRING, STRING...] templateId INTEGER templateParam [{“placeholder1” : “value1”}, {“placeholder2” : “value2”} ] from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER 4. SendSMS ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendSMS Parameters for the request Json ​ recipientsTo INTEGER message STRING from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers. Message conent and length should meet the criteria of SMS. 5. SendBulkSMS ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkSMS Parameters for the request Json ​ recipientsToList [INTEGER, INTEGER] message STRING from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers. Message content & length should meet the criteria of SMS. 6. SendWhatsAppText ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendWhatsappText Parameters for the request Json ​ recipientsTo INTEGER message STRING message_type \"TEXT\" Channel “whatsapp” from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers. Message should meet the criteria prescribed by Whatsapp. 7. Glossary ​ Conventions ​ Status - HTTP status code of response. All response is in JSON format. All request parameters are mandatory unless explicitly marked as [optional] Status Codes ​ All status codes are standard HTTP status codes. The below ones are used in this API. ​ 2XX - Success of some kind 4XX - Error occurred in client’s part 5XX - Error occurred in server’s part following are the status codes ​ Status Code Description 200 OK 201 Created 202 Accepted (Request accepted, and queued for execution) 400 Bad request 401 Authentication failure 403 Forbidden 404 Resource not found 405 Method Not Allowed 412 Precondition Failed 413 Request Entity Too Large 500 Internal Server Error 501 Not Implemented 503 Service Unavailable 504 Invalid data","tokens":909,"length":3775,"chunks":[{"doc_title":"Notification API | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/notification-api","doc_date":"2023-05-09T19:48:27.979Z","content":"On this page Notification API This will cover Email, SMS, whatsapp. It is open to extend any other type of notification and channel. The response will be in JSON format by default. Templates are supported in email and SMS. These templates must be created on providers. Developer must be passing the templateID and placeholder values in sequence. Proposed first integration providers : ​ email providers : Sendgrid, others to be filled after client's discussion SMS providers : to be filled after client's discussion whatsapp providers : to be filled after client's discussion Notification service can be run as independent microservices and as a module within other microservices. ​ 1.","content_length":685,"content_tokens":131,"embedding":[]},{"doc_title":"Notification API | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/notification-api","doc_date":"2023-05-09T19:48:27.979Z","content":"SendEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: /api/notification/v1/publish/sendEmail Parameters for the request Json ​ recipientsTo STRING message STRING from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be validated as valid email format. ​ 2. sendBulkEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkEmail Parameters for the request Json ​ recipientsToList [STRING, STRING...] message STRING from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER 3.","content_length":673,"content_tokens":174,"embedding":[]},{"doc_title":"Notification API | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/notification-api","doc_date":"2023-05-09T19:48:27.979Z","content":"sendBulkTemplateEmail ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkTemplateEmail Parameters for the request Json ​ recipientsToList [STRING, STRING...] templateId INTEGER templateParam [{“placeholder1” : “value1”}, {“placeholder2” : “value2”} ] from STRING Subject STRING Parameters for the response JSON ​ Status_Code: INTEGER 4.","content_length":410,"content_tokens":126,"embedding":[]},{"doc_title":"Notification API | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/notification-api","doc_date":"2023-05-09T19:48:27.979Z","content":"SendSMS ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendSMS Parameters for the request Json ​ recipientsTo INTEGER message STRING from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers. Message conent and length should meet the criteria of SMS. 5. SendBulkSMS ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendBulkSMS Parameters for the request Json ​ recipientsToList [INTEGER, INTEGER] message STRING from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers.","content_length":742,"content_tokens":197,"embedding":[]},{"doc_title":"Notification API | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/notification-api","doc_date":"2023-05-09T19:48:27.979Z","content":"Message content & length should meet the criteria of SMS. 6. SendWhatsAppText ​ Request URL ​ Content Type: application/json ; charset= utf- 8 Method: POST URL: URL: /api/notification/v1/publish/sendWhatsappText Parameters for the request Json ​ recipientsTo INTEGER message STRING message_type \"TEXT\" Channel “whatsapp” from INTEGER Parameters for the response JSON ​ Status_Code: INTEGER Here recipientsTo and from must be valid mobile numbers. Message should meet the criteria prescribed by Whatsapp. 7. Glossary ​ Conventions ​ Status - HTTP status code of response. All response is in JSON format. All request parameters are mandatory unless explicitly marked as [optional] Status Codes ​ All status codes are standard HTTP status codes. The below ones are used in this API. ​ 2XX - Success of some kind 4XX - Error occurred in client’s part 5XX - Error occurred in server’s part following are the status codes ​ Status Code Description 200 OK 201 Created 202 Accepted (Request accepted, and queued for execution) 400 Bad request 401 Authentication failure 403 Forbidden 404 Resource not found 405 Method Not Allowed 412 Precondition Failed 413 Request Entity Too Large 500 Internal Server Error 501 Not Implemented 503 Service Unavailable 504 Invalid data.","content_length":1261,"content_tokens":282,"embedding":[]}]},{"title":"Auto Export | Godspeed Docs","url":"https://docs.godspeed.systems/docs/out-of-box/auto-export","date":"2023-05-09T19:48:28.105Z","content":"On this page Auto Export Why modular design? ​ To Be Done What happens on importing a module? ​ To Be Done How are modules imported in a microservice? ​ `name namespace git repoUrl version includedFunctions users registration auths JWT enabled: false validations - <fnName> - <fn2Name> preHooks - <fnName> onError - <fnName> postHooks - <fnName> crud create otel: false","tokens":96,"length":369,"chunks":[{"doc_title":"Auto Export | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/out-of-box/auto-export","doc_date":"2023-05-09T19:48:28.105Z","content":"On this page Auto Export Why modular design? ​ To Be Done What happens on importing a module? ​ To Be Done How are modules imported in a microservice? ​ `name namespace git repoUrl version includedFunctions users registration auths JWT enabled: false validations - <fnName> - <fn2Name> preHooks - <fnName> onError - <fnName> postHooks - <fnName> crud create otel: false","content_length":369,"content_tokens":96,"embedding":[]}]},{"title":"Auto Instrumentation | Godspeed Docs","url":"https://docs.godspeed.systems/docs/out-of-box/auto-instrumentation","date":"2023-05-09T19:48:28.239Z","content":"To Be Done","tokens":3,"length":10,"chunks":[{"doc_title":"Auto Instrumentation | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/out-of-box/auto-instrumentation","doc_date":"2023-05-09T19:48:28.239Z","content":"To Be Done","content_length":10,"content_tokens":3,"embedding":[]}]},{"title":"Dual Write | Godspeed Docs","url":"https://docs.godspeed.systems/docs/out-of-box/dual-write","date":"2023-05-09T19:48:28.349Z","content":"To Be Done","tokens":3,"length":10,"chunks":[{"doc_title":"Dual Write | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/out-of-box/dual-write","doc_date":"2023-05-09T19:48:28.349Z","content":"To Be Done","content_length":10,"content_tokens":3,"embedding":[]}]},{"title":"About Godspeed | Godspeed Docs","url":"https://docs.godspeed.systems/docs/preface","date":"2023-05-09T19:48:28.488Z","content":"1. Preface On this page GodSpeed – A Microservice framework This document is intended for stakeholders, tech leaders, architects & developers. It will provide high level goals, tenets, design principles, components & features of the platform for the intended audience. 1.1 Introduction ​ Godspeed is aimed at empowering teams to develop, maintain and observe microservices based backends, with high velocity, scalability, quality and performance. We want development (and hence also QA) teams to bypass all the repeatable and reusable work involved in building modern distributed backends with domain driven design, multi-tenancy, microservices and serverless functions. We want the developers to be able to speedily develop microservices in days, instead of months. For the same, we are trying to provide everything that a team needs to create and operate modern microservices. It will be configuration/templating driven, plug & play, extensible by nature and cloud independent. There will be no vendor lock-in, either with Godspeed or any vendor used. It will give developers choice and control over the kind of tools, DBs and cloud providers they wish to use, while following standards and unified interfaces. This framework is being systematically developed by Mindgrep over the last years, across various projects by extracting abstractions and reusable components. It is actively being customized/expanded/improved with new adaptations. 1.2 Goals ​ THE GOALS OF THE FRAMEWORK ARE AIMED TO MAKE BUSINESS AGILE BY EMPOWERING THE PRODUCT & DEVELOPMENT TEAMS TO DELIVER EXCELLENT SOLUTIONS VERY FAST. Developer friendly ​ Godspeed provides low code implementation, YAML based DSL, prebuilt feature set and easy project setup, making like of developers easy. Thus empowering them to focus and accomplish their core work with the least amount of effort, time & cost. Enhancing developer productivity ​ The framework provides fundamental functionalities of “a modern microservice” out of the box so that developer only needs to focus on business logic (80% reduction in work). Smaller, micro teams and lesser learning curve ​ Module owners can start shipping microservices within a week's ramp-up time. If at all, only a couple of members in the ogranization need to know the nitty gritty. Rest can just train to use the framework, and deliver with their help,or ours. Security ​ The framework can read the environmental variables from a secure source like K8s Vault. For data in transit and data at rest, we use encryption mechanisms. Also, the framework supports JWT Authentication. Further, all hits to other APIs are secured via security schemas specified in their Open API Specification (OAS 3). Fine grained authorization at API and datasources level is in the roadmap. Read more Easy and fast migrations ​ Migrate existing data models to Godspeed via database introspection. Autogenerate CRUD APIs based on the data models. Migrate existing API based on its introspection, to create Godspeed compliant events - planned. Now, all that remains for developers, is simply to migrate the business logic. 1.3 Features ​ 1.4 Tenets ​ Don't repeat yourself ​ Developer does not need to do anything at the levels lower than the schema (events, datasources) and business logic. All that, including project setup with required docker containers, is handled by the framework. The developers need not to repeat any work from api to api or project to project. Easy to extend & customize ​ Pluggable interfaces allow new integrations without changing code. For example, replacing datastores, APM/BPM tools, analytics engines, cache, email provider, file storage, CRM etc. should ideally require no change in the application code. Standards driven ​ Use standards in designing the system. For example, events using CouldEvents. Observability using OpenTelemetry. 1.5 Design principals ​ Three fundamental abstractions ​ The three fundamental abstractions in the Godspeed are events (sync/async), workflows (business logic) and datasources (APIs/datastores). Read more Unified Observability For APM and BPM ​ We will follow OpenTelemetry (OTEL) SDKs to collect and observe telemetry data, including application performance monitoring. This will be integrable with a plethora of open source or commercial tools of choice that integrate with the standard OTEL protocol. Read more 1.6 Framework architecture ​ The three main dimensions of Godspeed framework: events, workflows and datasources. 1.7 Scenarios and use cases ​ Use cases include any kind of microservice, CRUD microservice, wrapper service, search and suggest service, backend for frontend service, orchestration service, domain gateway service, etc.","tokens":982,"length":4699,"chunks":[{"doc_title":"About Godspeed | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/preface","doc_date":"2023-05-09T19:48:28.488Z","content":"1. Preface On this page GodSpeed – A Microservice framework This document is intended for stakeholders, tech leaders, architects & developers. It will provide high level goals, tenets, design principles, components & features of the platform for the intended audience. 1.1 Introduction ​ Godspeed is aimed at empowering teams to develop, maintain and observe microservices based backends, with high velocity, scalability, quality and performance. We want development (and hence also QA) teams to bypass all the repeatable and reusable work involved in building modern distributed backends with domain driven design, multi-tenancy, microservices and serverless functions. We want the developers to be able to speedily develop microservices in days, instead of months. For the same, we are trying to provide everything that a team needs to create and operate modern microservices. It will be configuration/templating driven, plug & play, extensible by nature and cloud independent.","content_length":979,"content_tokens":194,"embedding":[]},{"doc_title":"About Godspeed | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/preface","doc_date":"2023-05-09T19:48:28.488Z","content":"There will be no vendor lock-in, either with Godspeed or any vendor used. It will give developers choice and control over the kind of tools, DBs and cloud providers they wish to use, while following standards and unified interfaces. This framework is being systematically developed by Mindgrep over the last years, across various projects by extracting abstractions and reusable components. It is actively being customized/expanded/improved with new adaptations. 1.2 Goals ​ THE GOALS OF THE FRAMEWORK ARE AIMED TO MAKE BUSINESS AGILE BY EMPOWERING THE PRODUCT & DEVELOPMENT TEAMS TO DELIVER EXCELLENT SOLUTIONS VERY FAST. Developer friendly ​ Godspeed provides low code implementation, YAML based DSL, prebuilt feature set and easy project setup, making like of developers easy. Thus empowering them to focus and accomplish their core work with the least amount of effort, time & cost.","content_length":886,"content_tokens":192,"embedding":[]},{"doc_title":"About Godspeed | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/preface","doc_date":"2023-05-09T19:48:28.488Z","content":"Enhancing developer productivity ​ The framework provides fundamental functionalities of “a modern microservice” out of the box so that developer only needs to focus on business logic (80% reduction in work) Smaller, micro teams and lesser learning curve ​ Module owners can start shipping microservices within a week's ramp-up time. If at all, only a couple of members in the ogranization need to know the nitty gritty. Rest can just train to use the framework, and deliver with their help,or ours. Security ​ The framework can read the environmental variables from a secure source like K8s Vault. For data in transit and data at rest, we use encryption mechanisms. Also, the framework supports JWT Authentication. Further, all hits to other APIs are secured via security schemas specified in their Open API Specification (OAS 3) Fine grained authorization at API and datasources level is in the roadmap.","content_length":905,"content_tokens":189,"embedding":[]},{"doc_title":"About Godspeed | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/preface","doc_date":"2023-05-09T19:48:28.488Z","content":"Read more Easy and fast migrations ​ Migrate existing data models to Godspeed via database introspection. Autogenerate CRUD APIs based on the data models. Migrate existing API based on its introspection, to create Godspeed compliant events - planned. Now, all that remains for developers, is simply to migrate the business logic. 1.3 Features ​ 1.4 Tenets ​ Don't repeat yourself ​ Developer does not need to do anything at the levels lower than the schema (events, datasources) and business logic. All that, including project setup with required docker containers, is handled by the framework. The developers need not to repeat any work from api to api or project to project. Easy to extend & customize ​ Pluggable interfaces allow new integrations without changing code. For example, replacing datastores, APM/BPM tools, analytics engines, cache, email provider, file storage, CRM etc.","content_length":887,"content_tokens":191,"embedding":[]},{"doc_title":"About Godspeed | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/preface","doc_date":"2023-05-09T19:48:28.488Z","content":"should ideally require no change in the application code. Standards driven ​ Use standards in designing the system. For example, events using CouldEvents. Observability using OpenTelemetry. 1.5 Design principals ​ Three fundamental abstractions ​ The three fundamental abstractions in the Godspeed are events (sync/async), workflows (business logic) and datasources (APIs/datastores) Read more Unified Observability For APM and BPM ​ We will follow OpenTelemetry (OTEL) SDKs to collect and observe telemetry data, including application performance monitoring. This will be integrable with a plethora of open source or commercial tools of choice that integrate with the standard OTEL protocol. Read more 1.6 Framework architecture ​ The three main dimensions of Godspeed framework: events, workflows and datasources. 1.7 Scenarios and use cases ​ Use cases include any kind of microservice, CRUD microservice, wrapper service, search and suggest service, backend for frontend service, orchestration service, domain gateway service, etc.","content_length":1034,"content_tokens":216,"embedding":[]}]},{"title":"Roadmap | Godspeed Docs","url":"https://docs.godspeed.systems/docs/roadmap","date":"2023-05-09T19:48:28.643Z","content":"15. Roadmap Roadmap","tokens":6,"length":19,"chunks":[{"doc_title":"Roadmap | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/roadmap","doc_date":"2023-05-09T19:48:28.643Z","content":"15. Roadmap Roadmap","content_length":19,"content_tokens":6,"embedding":[]}]},{"title":"Config Loading | Godspeed Docs","url":"https://docs.godspeed.systems/docs/scaffolding/config-loading","date":"2023-05-09T19:48:28.763Z","content":"On this page Config Loading Introduction ​ In Godspeed landscape, a configuration can be expressed in two ways. Way 1: For simple and small configuration ​ When the config is simple and small, it is perhaps better to put all of it in a single yaml/json/toml file sample_project_module.yaml user name: 'Ayush' address: city: 'Dharamsala' locality: pincode: 176052 landmark: 'Hill ventures adventure park' Way 2: For large and complex configuration ​ But when a configuration is growing large and has many nested components, it is perhaps better to break them in separate folder/file structure, for better readability and maintenance. Within any folder, If there is an index.yaml/toml/json, its keys will be loaded at the root path ending at that folder's name. Any other files' data is loaded under the key of that filename Nested folders' data is loaded recursively using the same approach, under the key of the nested folders The config loader in GS will load collated JSON from nested configurations stored in folder structure and give the same output as if it was stored within a single file. sample_project_module ./sample_project_module index.yaml name: 'Ayush' //Contents of index.yaml file address index.yaml city: 'Dharamsala' //Content of address/index.yaml file locality.yaml pincode: 176052 landmark: 'Hill ventures adventure park' Conclusion ​ Both these settings will ead to the same output as shown in Way 1.","tokens":333,"length":1422,"chunks":[{"doc_title":"Config Loading | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/config-loading","doc_date":"2023-05-09T19:48:28.763Z","content":"On this page Config Loading Introduction ​ In Godspeed landscape, a configuration can be expressed in two ways. Way 1: For simple and small configuration ​ When the config is simple and small, it is perhaps better to put all of it in a single yaml/json/toml file sample_project_module.yaml user name: 'Ayush' address: city: 'Dharamsala' locality: pincode: 176052 landmark: 'Hill ventures adventure park' Way 2: For large and complex configuration ​ But when a configuration is growing large and has many nested components, it is perhaps better to break them in separate folder/file structure, for better readability and maintenance. Within any folder, If there is an index.yaml/toml/json, its keys will be loaded at the root path ending at that folder's name.","content_length":759,"content_tokens":177,"embedding":[]},{"doc_title":"Config Loading | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/config-loading","doc_date":"2023-05-09T19:48:28.763Z","content":"Any other files' data is loaded under the key of that filename Nested folders' data is loaded recursively using the same approach, under the key of the nested folders The config loader in GS will load collated JSON from nested configurations stored in folder structure and give the same output as if it was stored within a single file. sample_project_module ./sample_project_module index.yaml name: 'Ayush' //Contents of index.yaml file address index.yaml city: 'Dharamsala' //Content of address/index.yaml file locality.yaml pincode: 176052 landmark: 'Hill ventures adventure park' Conclusion ​ Both these settings will ead to the same output as shown in Way 1.","content_length":662,"content_tokens":156,"embedding":[]}]},{"title":"Project structure | Godspeed Docs","url":"https://docs.godspeed.systems/docs/scaffolding/intro","date":"2023-05-09T19:48:28.895Z","content":"On this page Project structure Introduction ​ There are two kinds of projects : Microservice Serverless Any project can Have its own code (./src) Include other libraries/modules (package.json) Add middleware to functions (imported or in /src) Export functions (own or imported) via HTTP, message bus or socket A CLI is planned to create the scaffolding structure. For now template git repository will be made available. Project Scaffolding Structure ​ Configuration of any kind can be written in a single yaml/toml/json file, or can be broken down into nested folders. Any single file when getting too big, can be broken down into folders. Read more here Middleware: Functions authored in /src or imported from other modules, when loaded at service start time, can be wrapped as GSInstruction, with zero or more pre and post hooks including validations and auth, based on middleware settings of the project and overriden settings per function ./ // Project root directory src/ // It includes your authored functions which you wish to expose via the microservice interfaces as API. The FQN of any exported from any function, is the folder path to that function relative to /src test/ // Test cases for the project ui/ // Any UI related code including static files static //html, images, css src //React, react-native, Ionic, config/ // All the configuration for this project, including that of imported modules and also own exported functions. src/ // Any config required by the code in src folder imported_modules auth/ // Auth related config telemetry/ // Telemetry related config data/ // Data related config, including model, databases used, batch settings, internationalization/localization etc. middleware/ // Inserting pre and post function hooks to functions ; common/ // Applicable to all functions, be it imported functions or functions defined in src function_overrides/ // Middleware related config exported_functions/ // Configuration for function/modules to be exported over REST, message bus or socket (whether from /src or imported modules) test/ // Any config required by the tests microservice/ // When exposed as microservice this is required. It contains any microservice level settings. For example, the microservice name, domain name, open API channels (like message bus, REST). domainName: 'lending' microserviceName: 'credit_card' enabled_channels: ['REST', 'messageBus', 'socket'] //By default all exported functions will be exported via all enabled channels serverless/ // When exposed as serverless function this is required. It contains any FAAS level settings. domainName: 'lending' FAASName: 'some_ETL' trigger: 'messageBus' | 'gitOps' ... //For full list of supported triggers, see the ArgoEvents for supported sources package.json // All package info including the imported modules and dependencies ReadMe.md TODO: Add details for microservice/serverless config for different environments like dev/staging/production. Defining API schema and exporting ​ In Godspeed land, the API schema is collection of defined and exported functions with middleware hooks like param validation & authorization. Refer the core runtime for the same. In Godspeed land, you need to write only the business logic & configurations, and not write any code for setting up the server, defining routes, listening to different sync/async channels, sending responses etc. This way the business logic of a function is decoupled from the way this function is exported and consumed, saving development and testing work, and also saving code repetition. All you need to export a function is to define its config in /config/exported_functions. ./config exported_functions/ com abc functionA // The Fullly qualified name (FQN) of the exported function to external consumer will be `${domainName}.${microserviceName}.com.abc.functionA` // On event interfaces, the microservie or serverless will be registered to listen on the FQN of this function. For REST, the FQN will itself become the URL for that endpoint. enabled_channels: // If channels are not specified for this function, it is exportd via all the channels exposed by this service. In case of HTTP, default export will be POST. REST: methodType: 'GET' | 'POST'... // The params of GET request and payload of POST request become the arguments of the underlying function, to be called with its middleware messageBus: true | false // Default value: true. By default every function is exported on all the exported channels of this microservice (see microservice config detailed in above section) socket: true | false // If a channel is not set at the microservice config level, yet a function can be exported on that channel by this local override Common middleware ​ Like discussed already, any function exported an be given middleware hooks to be run before and after the function execution. These are useful for param validation, authorization and any other use cases as need be. The developer gets default middleware functions defined in /config/middleware/common . He can further tweak the middleware for any function used in the project. Local changes will override the global settings. These settings will be same in case of FAAS or microservice project. ./config middleware/ common/ // all functions will have common middleware defined here preAuths com.mg.gs.telemetry.createSpan dot.separated.fqn.fn2 auths //GSAssert f.q.n2.cachedAclsBasedAuth f.q.n2.ownershipBasedAuth validations //GSAssert f.q.n3.applyValidationA f.q.n4.applyValidationB onError finally com.mg.gs.telemetry.closeSpan com.mg.gs.telemetry.EFKLog com.mg.gs.telemetry.trace com.mg.gs.telemetry.sendLatencyMetric function_overrides //function specific overrides values here com godspeed lending createLoanAccount middleware preAuths push // | prepend | set //One can add middleware before or after the common middleware. Or replace (set) the common middleware with override. - f.q.n.fn1 - dot.separated.fqn.afterAll prepend - f.q.n.beforeAll auths //GSAssert set // | prepend | set - f.q.n2.cachedAclsBasedAuth - f.q.n2.ownershipBasedAuth validations prepend - f.q.n3.applyValidationA - f.q.n4.applyValidationB finally","tokens":1424,"length":6147,"chunks":[{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"On this page Project structure Introduction ​ There are two kinds of projects : Microservice Serverless Any project can Have its own code (./src) Include other libraries/modules (package.json) Add middleware to functions (imported or in /src) Export functions (own or imported) via HTTP, message bus or socket A CLI is planned to create the scaffolding structure. For now template git repository will be made available. Project Scaffolding Structure ​ Configuration of any kind can be written in a single yaml/toml/json file, or can be broken down into nested folders. Any single file when getting too big, can be broken down into folders.","content_length":639,"content_tokens":136,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"Read more here Middleware: Functions authored in /src or imported from other modules, when loaded at service start time, can be wrapped as GSInstruction, with zero or more pre and post hooks including validations and auth, based on middleware settings of the project and overriden settings per function ./ // Project root directory src/ // It includes your authored functions which you wish to expose via the microservice interfaces as API. The FQN of any exported from any function, is the folder path to that function relative to /src test/ // Test cases for the project ui/ // Any UI related code including static files static //html, images, css src //React, react-native, Ionic, config/ // All the configuration for this project, including that of imported modules and also own exported functions.","content_length":802,"content_tokens":169,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"src/ // Any config required by the code in src folder imported_modules auth/ // Auth related config telemetry/ // Telemetry related config data/ // Data related config, including model, databases used, batch settings, internationalization/localization etc. middleware/ // Inserting pre and post function hooks to functions ; common/ // Applicable to all functions, be it imported functions or functions defined in src function_overrides/ // Middleware related config exported_functions/ // Configuration for function/modules to be exported over REST, message bus or socket (whether from /src or imported modules) test/ // Any config required by the tests microservice/ // When exposed as microservice this is required. It contains any microservice level settings. For example, the microservice name, domain name, open API channels (like message bus, REST)","content_length":855,"content_tokens":176,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"domainName: 'lending' microserviceName: 'credit_card' enabled_channels: ['REST', 'messageBus', 'socket'] //By default all exported functions will be exported via all enabled channels serverless/ // When exposed as serverless function this is required. It contains any FAAS level settings. domainName: 'lending' FAASName: 'some_ETL' trigger: 'messageBus' | 'gitOps' .. //For full list of supported triggers, see the ArgoEvents for supported sources package.json // All package info including the imported modules and dependencies ReadMe.md TODO: Add details for microservice/serverless config for different environments like dev/staging/production. Defining API schema and exporting ​ In Godspeed land, the API schema is collection of defined and exported functions with middleware hooks like param validation & authorization. Refer the core runtime for the same.","content_length":862,"content_tokens":192,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"In Godspeed land, you need to write only the business logic & configurations, and not write any code for setting up the server, defining routes, listening to different sync/async channels, sending responses etc. This way the business logic of a function is decoupled from the way this function is exported and consumed, saving development and testing work, and also saving code repetition. All you need to export a function is to define its config in /config/exported_functions. ./config exported_functions/ com abc functionA // The Fullly qualified name (FQN) of the exported function to external consumer will be `${domainName}.${microserviceName}.com.abc.functionA` // On event interfaces, the microservie or serverless will be registered to listen on the FQN of this function. For REST, the FQN will itself become the URL for that endpoint.","content_length":844,"content_tokens":190,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"enabled_channels: // If channels are not specified for this function, it is exportd via all the channels exposed by this service. In case of HTTP, default export will be POST. REST: methodType: 'GET' | 'POST'.. // The params of GET request and payload of POST request become the arguments of the underlying function, to be called with its middleware messageBus: true | false // Default value: true. By default every function is exported on all the exported channels of this microservice (see microservice config detailed in above section) socket: true | false // If a channel is not set at the microservice config level, yet a function can be exported on that channel by this local override Common middleware ​ Like discussed already, any function exported an be given middleware hooks to be run before and after the function execution. These are useful for param validation, authorization and any other use cases as need be. The developer gets default middleware functions defined in /config/middleware/common  He can further tweak the middleware for any function used in the project. Local changes will override the global settings. These settings will be same in case of FAAS or microservice project.","content_length":1202,"content_tokens":246,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"./config middleware/ common/ // all functions will have common middleware defined here preAuths com.mg.gs.telemetry.createSpan dot.separated.fqn.fn2 auths //GSAssert f.q.n2.cachedAclsBasedAuth f.q.n2.ownershipBasedAuth validations //GSAssert f.q.n3.applyValidationA f.q.n4.applyValidationB onError finally com.mg.gs.telemetry.closeSpan com.mg.gs.telemetry.EFKLog com.mg.gs.telemetry.trace com.mg.gs.telemetry.sendLatencyMetric function_overrides //function specific overrides values here com godspeed lending createLoanAccount middleware preAuths push // | prepend | set //One can add middleware before or after the common middleware.","content_length":634,"content_tokens":201,"embedding":[]},{"doc_title":"Project structure | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/scaffolding/intro","doc_date":"2023-05-09T19:48:28.895Z","content":"Or replace (set) the common middleware with override. - f.q.n.fn1 - dot.separated.fqn.afterAll prepend - f.q.n.beforeAll auths //GSAssert set // | prepend | set - f.q.n2.cachedAclsBasedAuth - f.q.n2.ownershipBasedAuth validations prepend - f.q.n3.applyValidationA - f.q.n4.applyValidationB finally.","content_length":298,"content_tokens":115,"embedding":[]}]},{"title":"Auth Spec | Godspeed Docs","url":"https://docs.godspeed.systems/docs/security/Auth/intro","date":"2023-05-09T19:48:29.063Z","content":"Auth Spec Coming soon!!","tokens":5,"length":23,"chunks":[{"doc_title":"Auth Spec | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/security/Auth/intro","doc_date":"2023-05-09T19:48:29.063Z","content":"Auth Spec Coming soon!!","content_length":23,"content_tokens":5,"embedding":[]}]},{"title":"Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/security/intro","date":"2023-05-09T19:48:29.191Z","content":"On this page Introduction The introduction of code level security is as follows: Security ​ 1. Code Level Security ​ Methodologies for code scanning, vulnerability prevention, and security are listed below: GitHub secret scanning to avoid committing private keys and secrets into the codebase across all git branches. Use of CodeQL to scan and analyze code for security vulnerabilities and code related problems. This will be integrated in the CI using GitHub Actions. Dependabot alerts will be configured on the github repos for the early detection of vulnerabilities in the third-party libraries and packages. Integrate Snyk container security to the Kubernetes cluster to identify and fix the vulnerabilities across all image layers. This will be hooked up in the CI process. Web Application security testing will be done using Zed Attack Proxy (ZAP) to identify risks of malicious attacks. 2. Network ​ The entire platform will be hosted inside a restricted private network similar to Amazon Virtual Private Cloud (VPC). All communication between the internet and this network will be encrypted with HTTPS. 3. Data at rest & transit ​ All personally identifiable and sensitive information will be encrypted when stored in the database. This will be achieved using client side field encryption within the microservices/CRUD APIs. The encryption of the fields will be configurable at the time of defining the schema. For example, a MongoDB client will encrypt the fields using Authenticated encryption with associated data (AEAD) with HMAC-SHA-512 MAC before sending the data to the MongoDB server. This is supported out of the box using mongodb-client-encryption library. During transit, the communication between microservices will follow a mutual TLS approach to ensure that the parties at each end of the network are who they claim to be. Additionally the data in transit will be encrypted as long as it was configured in the schema. 4. Logging ​ No Personally identifiable information (PII) to be present in logs. This will have to be ensured by the developers through model configuration. Once configured, the specific information will be redacted from the logs by the framework. Additionally, the use of pre-commit git hooks such as Husky will prevent committing console logs using ESlint rules. 5. Cache ​ Cache to support encryption at rest and in-transit. Caching services such as Redis Enterprise have built in encryption support data in transit and at rest. Additionally, Amazon ElastiCache for Redis, an in-memory distributed caching mechanism, provides in-transit and at rest encryption to protect the data. 6. Documents ​ Documents to be stored as Blobs and will be encrypted at rest. Storage services such as Amazon S3 allow configuring the default encryption on a bucket. Doing so will encrypt all the newly added objects using server side encryption. 7. Vaults ​ Secret Management -- Using Hashicorp Vault or AWS Secret Manager or Azure Key Vault. Hashicorp Vault is cloud agnostic","tokens":594,"length":3001,"chunks":[{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/security/intro","doc_date":"2023-05-09T19:48:29.191Z","content":"On this page Introduction The introduction of code level security is as follows: Security ​ 1. Code Level Security ​ Methodologies for code scanning, vulnerability prevention, and security are listed below: GitHub secret scanning to avoid committing private keys and secrets into the codebase across all git branches. Use of CodeQL to scan and analyze code for security vulnerabilities and code related problems. This will be integrated in the CI using GitHub Actions. Dependabot alerts will be configured on the github repos for the early detection of vulnerabilities in the third-party libraries and packages. Integrate Snyk container security to the Kubernetes cluster to identify and fix the vulnerabilities across all image layers. This will be hooked up in the CI process. Web Application security testing will be done using Zed Attack Proxy (ZAP) to identify risks of malicious attacks. 2. Network ​ The entire platform will be hosted inside a restricted private network similar to Amazon Virtual Private Cloud (VPC)","content_length":1023,"content_tokens":193,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/security/intro","doc_date":"2023-05-09T19:48:29.191Z","content":"All communication between the internet and this network will be encrypted with HTTPS. 3. Data at rest & transit ​ All personally identifiable and sensitive information will be encrypted when stored in the database. This will be achieved using client side field encryption within the microservices/CRUD APIs. The encryption of the fields will be configurable at the time of defining the schema. For example, a MongoDB client will encrypt the fields using Authenticated encryption with associated data (AEAD) with HMAC-SHA-512 MAC before sending the data to the MongoDB server. This is supported out of the box using mongodb-client-encryption library. During transit, the communication between microservices will follow a mutual TLS approach to ensure that the parties at each end of the network are who they claim to be. Additionally the data in transit will be encrypted as long as it was configured in the schema. 4. Logging ​ No Personally identifiable information (PII) to be present in logs.","content_length":995,"content_tokens":200,"embedding":[]},{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/security/intro","doc_date":"2023-05-09T19:48:29.191Z","content":"This will have to be ensured by the developers through model configuration. Once configured, the specific information will be redacted from the logs by the framework. Additionally, the use of pre-commit git hooks such as Husky will prevent committing console logs using ESlint rules. 5. Cache ​ Cache to support encryption at rest and in-transit. Caching services such as Redis Enterprise have built in encryption support data in transit and at rest. Additionally, Amazon ElastiCache for Redis, an in-memory distributed caching mechanism, provides in-transit and at rest encryption to protect the data. 6. Documents ​ Documents to be stored as Blobs and will be encrypted at rest. Storage services such as Amazon S3 allow configuring the default encryption on a bucket. Doing so will encrypt all the newly added objects using server side encryption. 7. Vaults ​ Secret Management -- Using Hashicorp Vault or AWS Secret Manager or Azure Key Vault. Hashicorp Vault is cloud agnostic.","content_length":980,"content_tokens":202,"embedding":[]}]},{"title":"Introduction | Godspeed Docs","url":"https://docs.godspeed.systems/docs/serverless%20workflows/intro","date":"2023-05-09T19:48:29.328Z","content":"On this page Essential 3: (Serverless) Workflow engine In Godspeed land, one can trigger any kind of serverless workflows (akin to Lambda functions) from a diversity of sources. This is programming language, framework and cloud agnostic. Technologies used ArgoEvents ArgoWorkflow Salient Feature ​ MULTIPLE TRIGGER SOURCES. MULTIPLE TRIGGERED DESTINATIONS Argo Events is a CloudEvents compliant, event-driven workflow automation framework for Kubernetes. It integrates with more than 20 trigger sources and can trigger multiple kinds of workflows. Manages everything from simple, linear, real-time to complex, multi-source events. It can trigger K8s objects, Argo Workflows, OpenFAAS functions, AWS Lamdba and other Serverless workloads, etc. on events from a variety of sources like git, webhooks, S3, schedules, messaging queues, gcp pubsub, sns, sqs, etc.","tokens":211,"length":858,"chunks":[{"doc_title":"Introduction | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/serverless%20workflows/intro","doc_date":"2023-05-09T19:48:29.328Z","content":"On this page Essential 3: (Serverless) Workflow engine In Godspeed land, one can trigger any kind of serverless workflows (akin to Lambda functions) from a diversity of sources. This is programming language, framework and cloud agnostic. Technologies used ArgoEvents ArgoWorkflow Salient Feature ​ MULTIPLE TRIGGER SOURCES. MULTIPLE TRIGGERED DESTINATIONS Argo Events is a CloudEvents compliant, event-driven workflow automation framework for Kubernetes. It integrates with more than 20 trigger sources and can trigger multiple kinds of workflows. Manages everything from simple, linear, real-time to complex, multi-source events. It can trigger K8s objects, Argo Workflows, OpenFAAS functions, AWS Lamdba and other Serverless workloads, etc. on events from a variety of sources like git, webhooks, S3, schedules, messaging queues, gcp pubsub, sns, sqs, etc.","content_length":857,"content_tokens":211,"embedding":[]}]},{"title":"Technologies used (Default) | Godspeed Docs","url":"https://docs.godspeed.systems/docs/serverless%20workflows/technology-used/intro","date":"2023-05-09T19:48:29.460Z","content":"Technologies used (Default) ArgoEvents ArgoWorkflow","tokens":13,"length":51,"chunks":[{"doc_title":"Technologies used (Default) | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/serverless%20workflows/technology-used/intro","doc_date":"2023-05-09T19:48:29.460Z","content":"Technologies used (Default) ArgoEvents ArgoWorkflow","content_length":51,"content_tokens":13,"embedding":[]}]},{"title":"Godspeed Integration with SpringBoot | Godspeed Docs","url":"https://docs.godspeed.systems/docs/springboot-integration/intro","date":"2023-05-09T19:48:29.577Z","content":"On this page Godspeed Integration with SpringBoot Godspeed will provide an Java SDK to integrate with SpringBoot. Through this JDK, client will have option to use the common microservices provided by Godspeed. Developer can write their core logic in language of their choice and framework. (Currently planned Java and Springboot) Options for the Springboot world ​ Godspeed can be useful for teams working in any framework or language. They can integrate to Godspeed modules through SDKs in those languages. SDK features ​ Telemetry Out of box telemetry data collection for distrubuted logging, tracing, monitoring Integrating legacy systems can easily be integrated with log4j etc. Custom telemetry for BPM(Business Process Monitoring) Pluggable telemtery sinks/backends(OTEL compliant) Godspeed will provide out of box preconfigured open source telemetry backeneds as an option Integration with Godspeed common services: - DB CRUD - Search, suggest & scoring - Data federation (Backend For Frontend) - Notification - Document Domain gatway ​ The Godspeed domain gateway ) provides Authorization Orchestration Distributed transaction","tokens":229,"length":1134,"chunks":[{"doc_title":"Godspeed Integration with SpringBoot | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/springboot-integration/intro","doc_date":"2023-05-09T19:48:29.577Z","content":"On this page Godspeed Integration with SpringBoot Godspeed will provide an Java SDK to integrate with SpringBoot. Through this JDK, client will have option to use the common microservices provided by Godspeed. Developer can write their core logic in language of their choice and framework. (Currently planned Java and Springboot) Options for the Springboot world ​ Godspeed can be useful for teams working in any framework or language. They can integrate to Godspeed modules through SDKs in those languages. SDK features ​ Telemetry Out of box telemetry data collection for distrubuted logging, tracing, monitoring Integrating legacy systems can easily be integrated with log4j etc. Custom telemetry for BPM(Business Process Monitoring) Pluggable telemtery sinks/backends(OTEL compliant) Godspeed will provide out of box preconfigured open source telemetry backeneds as an option Integration with Godspeed common services: - DB CRUD - Search, suggest & scoring - Data federation (Backend For Frontend) - Notification - Document Domain gatway ​ The Godspeed domain gateway ) provides Authorization Orchestration Distributed transaction.","content_length":1134,"content_tokens":230,"embedding":[]}]},{"title":"Table of Contents | Godspeed Docs","url":"https://docs.godspeed.systems/docs/table-of-contents","date":"2023-05-09T19:48:29.725Z","content":"Table of Contents Table of Contents 1. Preface 1.1 Introduction 1.2 Goals 1.3 Features 1.4 Tenets 1.5 Design principals 1.6 Framework architecture 1.7 Scenarios and use cases 2. Introduction 2.1 Developer's work 3. Setup 3.1 Getting started 3.1.1 Glossary 3.1.2 Pre-requisites 3.1.3 Steps to get started 3.1.4 Time to start the development 3.2 Project structure 3.2.1 Scaffolding & Project structure 3.3 Configuration 3.3.1 Introduction 3.3.2 Environment variables 3.3.3 Static variables 3.4 Tests 3.5 Auto watch and build 4. CLI 4.1 Functionality 4.2 Installation 4.3 Options 4.4 Commands: Outside the dev container 4.5 Commands: Inside the dev container 5. Swagger Specs 5.1 CLI command to generate documentation 5.2 Custom Server URL 6. Events 6.1 Event types 6.2 Event schema & examples for supported sources 6.2.1 JSON schema validation 6.2.2 HTTP event 6.2.3 Kafka event 7. Workflows 7.1 The structure of workflows 7.2 The tasks within workflows 7.3 Location and fully qualified name (id) of workflows and functions 7.4 Referencing a workflow within an event or another workflow 7.5 Use of Coffee/JS for scripting 7.6 Inbuilt functions 7.6.1 com.gs.http 7.6.2 com.gs.kafka 7.6.3 com.gs.datastore 7.6.4 com.gs.elasticgraph 7.6.5 com.gs.transform 7.6.6 com.gs.series 7.6.7 com.gs.parallel 7.6.8 com.gs.switch 7.6.9 com.gs.each_sequential 7.6.10 com.gs.each_parallel 7.6.11 com.gs.return 7.6.12 com.gs.log 7.6.13 com.gs.dynamic_fn 7.6.14 com.gs.aws 7.6.15 com.gs.redis 7.6.16 com.gs.if, com.gs.elif, com.gs.else 7.7 Developer written functions 7.8 Headers defined at workflow level 7.9 File Upload feature 7.9.1 Workflow spec to upload files with same file key 7.9.2 Workflow spec to upload multiple files with different file keys 7.9.3 Workflow spec to upload file directly from URL 8. Datasources 8.1 Introduction 8.1.1 Datasource types 8.2 API datasource 8.2.1 API datasource schema defined externally 8.2.2 API datasource schema defined within the yaml file 8.2.3 Headers defined at datasource level 8.2.4 Headers defined at task level 8.2.5 Example usage 8.3 Datastore as datasource 8.3.1 Schema specification 8.3.2 CLI Commands 8.3.3 Prisma Datastore Setup 8.3.4 Auto generating CRUD APIs from data store models 8.3.5 Sample datastore CRUD task 8.4 Kafka as datasource 8.4.1 Example spec 8.5 Elasticgraph as datasource 8.5.1 Folder Structure 8.5.2 Datasource DSL 8.5.3 Configuration files for elasticgraph 8.5.4 Elasticgraph Setup 8.5.5 Auto generating CRUD APIs for elasticgraph 8.6 Extensible datasources 8.6.1 Datasource definition 8.6.2 Example spec for the event 8.6.3 Example spec for the workflow 8.7 AWS as datasource 8.7.1 Example spec 8.7.2 com.gs.aws workflow 8.8 Redis as datasource 8.8.1 Example spec 9. Caching 9.1 Specifications 9.1.1 Datasource spec for redis 9.1.2 Configuration 9.1.3 Workflow spec 10. Mappings 10.1 Project structure 10.2 Sample mappings 10.3 Use mappings constants in other mapping files 11. Plugins 11.1 Project structure 11.2 Sample plugins 11.3 Sample workflow using plugins 12. Authentication & Authorization 12.1 Authentication 12.1.1 JWT Configuration 12.1.2 Event spec 12.1.3 Generate JWT 12.1.4 Datasource authentication 12.2 Authorization 12.2.1 Workflow DSL 12.2.2 Sample DB query call authorization 13. Telemetry 13.1 Introduction 13.1.1 Architecture 13.2 Goals 13.3 Configuration 13.3.1 OTEL exporter endpoint 13.3.2 OTEL service name 13.3.3 Logging 13.3.3.1 Log level 13.3.3.2 Log fields masking 13.3.3.3 Log format 13.3.3.4 Add custom identifiers in logs 13.4 Custom metrics, traces and logs (BPM) 13.4.1 DSL spec for custom metrics 13.4.2 DSL spec for custom trace 13.4.3 DSL spec for custom logs 13.5 Observability Stack 13.6 Recommended model for telemetry signals 14. Custom Middleware 14.1 How to add custom middleware in Godspeed 15. Roadmap 16. FAQ 16.1 What is the learning curve of the microservice framework? 16.2 What is the development process and quality metrics? 16.3 How can we adopt new versions of used technology easily and fast? For example, the new Postgres release. 16.4 How easy is it to add new technology in place of an existing one, or add something absolutely new and unique (not existing in the framework) ? 16.5 Which databases are currently supported? What is the roadmap for future support? 16.6 Does the API handle DB transactions? 16.7 How can apps be decoupled or loosely coupled with DBs? 16.8 When using Godspeed service alongside SpringBoot, what will be the impact on performance with another hop, versus direct connection with DB from Spring Boot? 16.9 What is the strategic advantage of making DB queries through Godspeed? 16.10 How to achieve multi-tenancy in DBs, for a single application? 16.11 How can we start adopting the Godspeed framework? 16.12 How to move out of the Godspeed framework? Can we have a two door exit? I.e. Can we move out of technology and data both? 16.13 How will we prevent unified CRUD API from limiting or choking us? 16.14 What kind of API standards does the framework support? 16.15 Why Rest first approach ? Why not Graphql first approach? 16.16 How are we doing testing given there is quite a bit of custom DSL in the framework. How do we ensure the correctness? 16.17 How will the upgrades and migrations be done to the framework? 16.18 How CRUD APIs will support the paid as well as the non paid features of databases such as MongoDB. For example: MongoDB free vs paid versions will support different features. 16.19 How to ship new models easily?","tokens":1547,"length":5481,"chunks":[{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Table of Contents Table of Contents 1. Preface 1.1 Introduction 1.2 Goals 1.3 Features 1.4 Tenets 1.5 Design principals 1.6 Framework architecture 1.7 Scenarios and use cases 2. Introduction 2.1 Developer's work 3. Setup 3.1 Getting started 3.1.1 Glossary 3.1.2 Pre-requisites 3.1.3 Steps to get started 3.1.4 Time to start the development 3.2 Project structure 3.2.1 Scaffolding & Project structure 3.3 Configuration 3.3.1 Introduction 3.3.2 Environment variables 3.3.3 Static variables 3.4 Tests 3.5 Auto watch and build 4. CLI 4.1 Functionality 4.2 Installation 4.3 Options 4.4 Commands: Outside the dev container 4.5 Commands: Inside the dev container 5. Swagger Specs 5.1 CLI command to generate documentation 5.2 Custom Server URL 6. Events 6.1 Event types 6.2 Event schema & examples for supported sources 6.2.1 JSON schema validation 6.2.2 HTTP event 6.2.3 Kafka event 7.","content_length":878,"content_tokens":244,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Workflows 7.1 The structure of workflows 7.2 The tasks within workflows 7.3 Location and fully qualified name (id) of workflows and functions 7.4 Referencing a workflow within an event or another workflow 7.5 Use of Coffee/JS for scripting 7.6 Inbuilt functions 7.6.1 com.gs.http 7.6.2 com.gs.kafka 7.6.3 com.gs.datastore 7.6.4 com.gs.elasticgraph 7.6.5 com.gs.transform 7.6.6 com.gs.series 7.6.7 com.gs.parallel 7.6.8 com.gs.switch 7.6.9 com.gs.each_sequential 7.6.10 com.gs.each_parallel 7.6.11 com.gs.return 7.6.12 com.gs.log 7.6.13 com.gs.dynamic_fn 7.6.14 com.gs.aws 7.6.15 com.gs.redis 7.6.16 com.gs.if, com.gs.elif, com.gs.else 7.7 Developer written functions 7.8 Headers defined at workflow level 7.9 File Upload feature 7.9.1 Workflow spec to upload files with same file key 7.9.2 Workflow spec to upload multiple files with different file keys 7.9.3 Workflow spec to upload file directly from URL 8.","content_length":909,"content_tokens":322,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Datasources 8.1 Introduction 8.1.1 Datasource types 8.2 API datasource 8.2.1 API datasource schema defined externally 8.2.2 API datasource schema defined within the yaml file 8.2.3 Headers defined at datasource level 8.2.4 Headers defined at task level 8.2.5 Example usage 8.3 Datastore as datasource 8.3.1 Schema specification 8.3.2 CLI Commands 8.3.3 Prisma Datastore Setup 8.3.4 Auto generating CRUD APIs from data store models 8.3.5 Sample datastore CRUD task 8.4 Kafka as datasource 8.4.1 Example spec 8.5 Elasticgraph as datasource 8.5.1 Folder Structure 8.5.2 Datasource DSL 8.5.3 Configuration files for elasticgraph 8.5.4 Elasticgraph Setup 8.5.5 Auto generating CRUD APIs for elasticgraph 8.6 Extensible datasources 8.6.1 Datasource definition 8.6.2 Example spec for the event 8.6.3 Example spec for the workflow 8.7 AWS as datasource 8.7.1 Example spec 8.7.2 com.gs.aws workflow 8.8 Redis as datasource 8.8.1 Example spec 9.","content_length":935,"content_tokens":286,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Caching 9.1 Specifications 9.1.1 Datasource spec for redis 9.1.2 Configuration 9.1.3 Workflow spec 10. Mappings 10.1 Project structure 10.2 Sample mappings 10.3 Use mappings constants in other mapping files 11. Plugins 11.1 Project structure 11.2 Sample plugins 11.3 Sample workflow using plugins 12. Authentication & Authorization 12.1 Authentication 12.1.1 JWT Configuration 12.1.2 Event spec 12.1.3 Generate JWT 12.1.4 Datasource authentication 12.2 Authorization 12.2.1 Workflow DSL 12.2.2 Sample DB query call authorization 13.","content_length":532,"content_tokens":145,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Telemetry 13.1 Introduction 13.1.1 Architecture 13.2 Goals 13.3 Configuration 13.3.1 OTEL exporter endpoint 13.3.2 OTEL service name 13.3.3 Logging 13.3.3.1 Log level 13.3.3.2 Log fields masking 13.3.3.3 Log format 13.3.3.4 Add custom identifiers in logs 13.4 Custom metrics, traces and logs (BPM) 13.4.1 DSL spec for custom metrics 13.4.2 DSL spec for custom trace 13.4.3 DSL spec for custom logs 13.5 Observability Stack 13.6 Recommended model for telemetry signals 14. Custom Middleware 14.1 How to add custom middleware in Godspeed 15. Roadmap 16. FAQ 16.1 What is the learning curve of the microservice framework? 16.2 What is the development process and quality metrics? 16.3 How can we adopt new versions of used technology easily and fast? For example, the new Postgres release.","content_length":785,"content_tokens":220,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"16.4 How easy is it to add new technology in place of an existing one, or add something absolutely new and unique (not existing in the framework) ? 16.5 Which databases are currently supported? What is the roadmap for future support? 16.6 Does the API handle DB transactions? 16.7 How can apps be decoupled or loosely coupled with DBs? 16.8 When using Godspeed service alongside SpringBoot, what will be the impact on performance with another hop, versus direct connection with DB from Spring Boot? 16.9 What is the strategic advantage of making DB queries through Godspeed? 16.10 How to achieve multi-tenancy in DBs, for a single application? 16.11 How can we start adopting the Godspeed framework? 16.12 How to move out of the Godspeed framework? Can we have a two door exit? I.e.","content_length":782,"content_tokens":182,"embedding":[]},{"doc_title":"Table of Contents | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/table-of-contents","doc_date":"2023-05-09T19:48:29.725Z","content":"Can we move out of technology and data both? 16.13 How will we prevent unified CRUD API from limiting or choking us? 16.14 What kind of API standards does the framework support? 16.15 Why Rest first approach ? Why not Graphql first approach? 16.16 How are we doing testing given there is quite a bit of custom DSL in the framework. How do we ensure the correctness? 16.17 How will the upgrades and migrations be done to the framework? 16.18 How CRUD APIs will support the paid as well as the non paid features of databases such as MongoDB. For example: MongoDB free vs paid versions will support different features. 16.19 How to ship new models easily?","content_length":652,"content_tokens":148,"embedding":[]}]},{"title":"Tech Stack | Godspeed Docs","url":"https://docs.godspeed.systems/docs/tech-stack","date":"2023-05-09T19:48:29.907Z","content":"Godspeed RECOMMENDED STACK, STANDARDS & PROCESSES Preferred Alternatives Soon to be available on official documentation site. Containerisation Docker Developer setup Docker with Godspeed repository (Docker images and templates) Cloud provisioning Gitops, Kubernetes, Crossplane Software provisioning and configuring Git ops, Argo events, ArgoWorkflows, Argo CD, Argo Rollout IAM ORY Kratos/Keycloak Gateway authorisation ORY OATHKEEPER Ingress gateway NGINX Proxy/Istio/Kong/Ambassdor/Trafeik Service mesh Linkerd Microservice framework SpringBoot + Open Telemetry SDK + Java SDK Godspeed: SDK with TS, JS, Webassembly (Java, Python, Go, .Net, C#, JS, TS..) Serverless/ETL framework \"Argo events, Argo Workflow, Godspeed SDK with webassembly. \" Distributed transactions, microservice orchestration Godspeed function-DAG interface with Temporal plugin. SpringBoot with Temporal integration . Document service Document service. Godspeed document service or MinIO MinIO has community and commercial editions for hosting a S3 comptabile document service. Notification service Godspeed based service with universal API . Adapters with providers. Eventual consistency (polygot persistence) Godspeed plus Debezium plus Kafka Databases Planned support for Mongodb, Elasticsearch Later support for Postegres, Mysql Testing Godspeed itself Chai/Mocha Test cases for business logic (in any language/framework) Can be written in any langauge/framework and integrated in CI/CD MessageBus/Queue Kafka, ActiveMQ Telemetry - origination SDKs in each of Springboot and Godspeed service, compliant with OTEL Observability DataDog Observability.logging Elasticsearch, FluentBit, Kibana CloudWatch Observability.monitoring Prometheus/Grafana CloudWatch Observability.tracing Jaeger X-RAY Observability.alerting Grafana Version control Git & GitHub Standards- telemetry OpenTelementry Standards- events CloudEvents Search, Suggest, Analytics Elasticsearch Data-events Debezium + Godspeed (fast to create and execute ETLs) CI/CD Argo Events,ARGO Workflow, ArgoCD Security- Key secret management Hashicorp Vault and Sealed Secrets (Kubernetes) Security- Network VPC Security- Code Github, Synk Encryption","tokens":518,"length":2181,"chunks":[{"doc_title":"Tech Stack | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/tech-stack","doc_date":"2023-05-09T19:48:29.907Z","content":"Godspeed RECOMMENDED STACK, STANDARDS & PROCESSES Preferred Alternatives Soon to be available on official documentation site. Containerisation Docker Developer setup Docker with Godspeed repository (Docker images and templates) Cloud provisioning Gitops, Kubernetes, Crossplane Software provisioning and configuring Git ops, Argo events, ArgoWorkflows, Argo CD, Argo Rollout IAM ORY Kratos/Keycloak Gateway authorisation ORY OATHKEEPER Ingress gateway NGINX Proxy/Istio/Kong/Ambassdor/Trafeik Service mesh Linkerd Microservice framework SpringBoot + Open Telemetry SDK + Java SDK Godspeed: SDK with TS, JS, Webassembly (Java, Python, Go, .Net, C#, JS, TS..) Serverless/ETL framework \"Argo events, Argo Workflow, Godspeed SDK with webassembly. \" Distributed transactions, microservice orchestration Godspeed function-DAG interface with Temporal plugin. SpringBoot with Temporal integration  Document service Document service. Godspeed document service or MinIO MinIO has community and commercial editions for hosting a S3 comptabile document service. Notification service Godspeed based service with universal API  Adapters with providers.","content_length":1137,"content_tokens":269,"embedding":[]},{"doc_title":"Tech Stack | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/tech-stack","doc_date":"2023-05-09T19:48:29.907Z","content":"Eventual consistency (polygot persistence) Godspeed plus Debezium plus Kafka Databases Planned support for Mongodb, Elasticsearch Later support for Postegres, Mysql Testing Godspeed itself Chai/Mocha Test cases for business logic (in any language/framework) Can be written in any langauge/framework and integrated in CI/CD MessageBus/Queue Kafka, ActiveMQ Telemetry - origination SDKs in each of Springboot and Godspeed service, compliant with OTEL Observability DataDog Observability.logging Elasticsearch, FluentBit, Kibana CloudWatch Observability.monitoring Prometheus/Grafana CloudWatch Observability.tracing Jaeger X-RAY Observability.alerting Grafana Version control Git & GitHub Standards- telemetry OpenTelementry Standards- events CloudEvents Search, Suggest, Analytics Elasticsearch Data-events Debezium + Godspeed (fast to create and execute ETLs) CI/CD Argo Events,ARGO Workflow, ArgoCD Security- Key secret management Hashicorp Vault and Sealed Secrets (Kubernetes) Security- Network VPC Security- Code Github, Synk Encryption.","content_length":1041,"content_tokens":250,"embedding":[]}]},{"title":"Observability | Godspeed Docs","url":"https://docs.godspeed.systems/docs/telemetry/intro","date":"2023-05-09T19:48:30.058Z","content":"13. Telemetry On this page Observability 13.1 Introduction ​ For observability, the framework supports Application Performance Monitoring(APM) abd Business Performance Monitoring(BPM) out of the box. This includes distributed trace context propagation across sync and async channels, logging and basic metrics. For the same, we are leveraging the OpenTelemetry standard and its supporting tech ecosystem. Not even a single request must go untracked! 13.1.1 Architecture ​ Both Traces and Metrics are sent to OTEL Collector directly. Tempo is used as tracing backend for traces and Prometheus is used for metrics with Mimir as its backend. For Logs , a fluent bit daemonset is running on node, which collects logs from various applications on the node. Loki is used as logs aggregation solution. 13.2 Goals ​ Auto application performance monitoring ​ No code APM across microservices, integrable with standard APM tools and logging backends, without any dev effort. Backend agnostic ​ Numerous open source and commercial softwares for Observability support OpenTelemetry out of the box, allowing one to switch between them if needed. Complete debuggability ​ Collect, correlate and debug signals across logs (events), traces and metrics, based on the request id and the attributes defined for the organization. For example, app version, function, DB query, K8s pod, domain, microservice etc. 13.3 Configuration ​ 13.3.1 OTEL exporter endpoint ​ Specify the IP address of your OTEL collector as env variable. Refer OTEL Exporter for more information. $ export OTEL_EXPORTER_OTLP_ENDPOINT=<IP of OTEL collector>:4317 For example, export OTEL_EXPORTER_OTLP_ENDPOINT=http://172.17.0.1:4317 13.3.2 OTEL service name ​ Specify the service name by which you want to setup observability and set it as env variable. $ export OTEL_SERVICE_NAME=sample_proj1 Let's assume you have setup SigNoz as the exporter then you will see something like this: In case you have any questions, please reach out to us on our Discord channel . 13.3.3 Logging ​ 13.3.3.1 Log level ​ The minimum level set to log above this level. Please refer Pino log levels for more information. Set log_level in Static variables 13.3.3.2 Log fields masking ​ If you want to hide sensitive information in logs then define the fields which need to be hidden in redact feature in Static variables . Please refer Pino redaction paths for more information. 13.3.3.3 Log format ​ By default, the logs are dumped in OTEL Logging format when you deploy your service anywhere (UAT, Prod, K8s, etc.) except inside the vscode remote containers/dev containers. {\"Body\":\"adding body schema for /upload_doc.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_multiple_docs.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_s3.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /another_workflow post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /create/:entity_type post\",\"Timestamp\":\"1676531763728000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} . . . . . . . . . . . {\"Body\":\"args.retry {\\\"max_attempts\\\":3,\\\"type\\\":\\\"constant\\\",\\\"interval\\\":5000}\",\"Timestamp\":\"1676531764656000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Result of _executeFn test_step1 {\\\"success\\\":true,\\\"code\\\":200,\\\"data\\\":{\\\"args\\\":{},\\\"data\\\":\\\"{\\\\\\\"data\\\\\\\":{\\\\\\\"lan\\\\\\\":\\\\\\\"12345\\\\\\\"}}\\\",\\\"files\\\":{},\\\"form\\\":{},\\\"headers\\\":{\\\"Accept\\\":\\\"application/json, text/plain, */*\\\",\\\"Content-Length\\\":\\\"24\\\",\\\"Content-Type\\\":\\\"application/json\\\",\\\"Host\\\":\\\"httpbin.org\\\",\\\"Traceparent\\\":\\\"00-a58ef2d7ff7725c39f1e058bf22fe724-2f13e28430d61bdb-01\\\",\\\"User-Agent\\\":\\\"axios/0.25.0\\\",\\\"X-Amzn-Trace-Id\\\":\\\"Root=1-63edd835-22cff8e60555fa522c8544cf\\\"},\\\"json\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"method\\\":\\\"POST\\\",\\\"origin\\\":\\\"180.188.224.177\\\",\\\"url\\\":\\\"https://httpbin.org/anything\\\"},\\\"message\\\":\\\"OK\\\",\\\"headers\\\":{\\\"date\\\":\\\"Thu, 16 Feb 2023 07:16:05 GMT\\\",\\\"content-type\\\":\\\"application/json\\\",\\\"content-length\\\":\\\"598\\\",\\\"connection\\\":\\\"close\\\",\\\"server\\\":\\\"gunicorn/19.9.0\\\",\\\"access-control-allow-origin\\\":\\\"*\\\",\\\"access-control-allow-credentials\\\":\\\"true\\\"}}\",\"Timestamp\":\"1676531765810000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Validate Response JSON Schema Success\",\"Timestamp\":\"1676531765811000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"}} Dev Format The dev format is basically a transformation of OTEL log format to increase readability for developers. Please note that the default logging format inside vscode dev container on your local machine is dev format as given below: datetime [SeverityText] TraceId SpanId {Attributes} Body Sample Logs: 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_doc.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_multiple_docs.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_s3.http.post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /another_workflow post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /create/:entity_type post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /document post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /fn_script post . . . . . . . . . . 16/02/23, 12:44:43 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} args.retry {\"max_attempts\":3,\"type\":\"constant\",\"interval\":5000} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} Result of _executeFn test_step1 {\"success\":true,\"code\":200,\"data\":{\"args\":{},\"data\":\"{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}}\",\"files\":{},\"form\":{},\"headers\":{\"Accept\":\"application/json, text/plain, */*\",\"Content-Length\":\"24\",\"Content-Type\":\"application/json\",\"Host\":\"httpbin.org\",\"Traceparent\":\"00-f9f61d4940e3a8e5be8bc80faf6e36a2-f6c0a5ce67f5b07c-01\",\"User-Agent\":\"axios/0.25.0\",\"X-Amzn-Trace-Id\":\"Root=1-63edd7e4-0b8b6ba319833492520e6b0c\"},\"json\":{\"data\":{\"lan\":\"12345\"}},\"method\":\"POST\",\"origin\":\"180.188.224.177\",\"url\":\"https://httpbin.org/anything\"},\"message\":\"OK\",\"headers\":{\"date\":\"Thu, 16 Feb 2023 07:14:44 GMT\",\"content-type\":\"application/json\",\"content-length\":\"598\",\"connection\":\"close\",\"server\":\"gunicorn/19.9.0\",\"access-control-allow-origin\":\"*\",\"access-control-allow-credentials\":\"true\"}} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"} Validate Response JSON Schema Success note If you want to change the OTEL format to dev format , then set the environment variable NODE_ENV to dev in your environment as given below. The default value of NODE_ENV is production . export NODE_ENV=dev 13.3.3.4 Add custom identifiers in logs ​ You can add any custom identifier in the logging whenever any event is triggered on your service. The value for the custom identifier will be picked up from event body, params, query, or headers. To enable this feature ,you need to specify two things: log_attributes variable as environment variable / static variable which contains custom identifiers. For example, this is the sample static configuration: log_attributes: mobileNumber: \"query?.mobileNumber\" id: \"params?.id\" lan: \"body?.data?.lan\" location of the identifier in the request payload. As specified in the above example, if mobileNumber is present in query params then specify query?.mobileNumber . if id is present in path params then specify params?.id . if lan is present in data field inside body then specify body?.data?.lan . note Please make sure to add ? in case any field is optional like body?.data?.lan so that it works well with undefined values. This will add lan in the logs if it is present else it will not get added. Sample Logs Dev format 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} Processing event /test/:id.http.post 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event inputs {\"baseUrl\":\"\",\"body\":{\"data\":{\"lan\":\"12345\"}},\"fresh\":false,\"hostname\":\"localhost\",\"ip\":\"::ffff:172.22.0.1\",\"ips\":[],\"method\":\"POST\",\"originalUrl\":\"/test/12?mobileNumber=9878987898\",\"params\":{\"id\":\"12\"},\"path\":\"/test/12\",\"protocol\":\"http\",\"query\":{\"mobileNumber\":\"9878987898\"},\"route\":{\"path\":\"/test/:id\",\"stack\":[{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"},{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"}],\"methods\":{\"post\":true}},\"secure\":false,\"stale\":true,\"subdomains\":[],\"xhr\":false,\"headers\":{\"content-type\":\"application/json\",\"user-agent\":\"PostmanRuntime/7.29.2\",\"accept\":\"*/*\",\"postman-token\":\"835edd29-7c36-4e11-9b79-c661bbd911b0\",\"host\":\"localhost:4000\",\"accept-encoding\":\"gzip, deflate, br\",\"connection\":\"keep-alive\",\"content-length\":\"46\"},\"files\":[]} 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event body and eventSpec exist OTEL format {\"Body\":\"Processing event /test/:id.http.post\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event inputs {\\\"baseUrl\\\":\\\"\\\",\\\"body\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"fresh\\\":false,\\\"hostname\\\":\\\"localhost\\\",\\\"ip\\\":\\\"::ffff:172.22.0.1\\\",\\\"ips\\\":[],\\\"method\\\":\\\"POST\\\",\\\"originalUrl\\\":\\\"/test/12?mobileNumber=9878987898\\\",\\\"params\\\":{\\\"id\\\":\\\"12\\\"},\\\"path\\\":\\\"/test/12\\\",\\\"protocol\\\":\\\"http\\\",\\\"query\\\":{\\\"mobileNumber\\\":\\\"9878987898\\\"},\\\"route\\\":{\\\"path\\\":\\\"/test/:id\\\",\\\"stack\\\":[{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"},{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"}],\\\"methods\\\":{\\\"post\\\":true}},\\\"secure\\\":false,\\\"stale\\\":true,\\\"subdomains\\\":[],\\\"xhr\\\":false,\\\"headers\\\":{\\\"content-type\\\":\\\"application/json\\\",\\\"user-agent\\\":\\\"PostmanRuntime/7.29.2\\\",\\\"accept\\\":\\\"*/*\\\",\\\"postman-token\\\":\\\"9e57df7d-0a75-48b6-bc52-921bd5c045b7\\\",\\\"host\\\":\\\"localhost:4000\\\",\\\"accept-encoding\\\":\\\"gzip, deflate, br\\\",\\\"connection\\\":\\\"keep-alive\\\",\\\"content-length\\\":\\\"46\\\"},\\\"files\\\":[]}\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event body and eventSpec exist\",\"Timestamp\":\"1676960742404000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} 13.4 Custom metrics, traces and logs (BPM) ​ Custom metrics, traces and logs can be added in the workflow DSL at each task level then these will be available out of the box along with APM. 13.4.1 DSL spec for custom metrics ​ # refer https://github.com/siimon/prom-client metrics: - name: metric_name type: counter|gauge|histogram|summary labels: label1: val1 label2: val2 # followng functions depending on the metric type and all of them could be scripts, can use inputs/outputs inc: 10 dec: 10 set: 100 observe: 2000 timer: true|false(boolean) starts at the beginning of workflow/task and ends at the end of workflow/task Example spec ​ In the following example, we are using two custom metrics: httpbin_calls_total: counter type metric, counter is incremented by 1. httpbin_calls_duration: histogram type metric, timer is set to true to record duration. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http metrics: - name: httpbin_calls_total help: 'httpbin_calls_total counter of httpbin requests labeled with: method, status_code' type: counter labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> inc: 1 - name: httpbin_calls_duration help: 'httpbin_calls_duration duration histogram of httpbin responses labeled with: method, status_code' type: histogram labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> timer: true args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post 13.4.2 DSL spec for custom trace ​ trace: name: span_name attributes: attribute1: value1 attribute2: value2 Example spec ​ In the following example, we are creating a new span named httpbin_trace with span attributes request and param . This span gets created when the task starts and ended when the task completes its execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http trace: name: httpbin_trace attributes: request: <%inputs.body%> param: <%inputs.query%> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post 13.4.3 DSL spec for custom logs ​ logs: before: level: fatal|error|warn|info|debug|trace # refer pino for levels message: 'Sample log before' params: param1: val1 param2: val2 attributes: request: query: <%inputs.query%> after: level: info message: 'Sample log after' params: attributes: The logs are dumped in OTEL format. Please refer to OTEL Logging Data model for understanding of fields dumped in the logs. message and params are part of Body field and attributes are part of Attributes field in the log. Example spec ​ In the following example, we are two additional logs before and after the task execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http logs: before: level: error message: 'Hello' params: - key1: v1 key2: v2 - v1 attributes: request: <%inputs.query%> after: level: error message: 'World' params: key1: v1 key2: v2 attributes: customer_name: <% outputs.httpbin_step1.data.json.customer_name %> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post Sample Logs {\"Body\":\"Hello [{\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"},\\\"v1\\\"]\",\"Timestamp\":\"1676011973016000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"request\":{\"status\":\"Hello\"},\"task_id\":\"if\",\"workflow_name\":\"if_else\"}} . . . . . . . . . . . {\"Body\":\"World {\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"}\",\"Timestamp\":\"1676011973019000000\",\"SeverityNumber\":17,\"SeverityText\":\"ERROR\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"customer_name\":\"Hell!\",\"task_id\":\"if\",\"workflow_name\":\"if_else\"}} 13.5 Observability Stack ​ The complete observability stack with K8s helm-charts will be made available soon. 13.6 Recommended model for telemetry signals ​ Please find the draft documentation here . This is compiled in one place from various references across the OpenTelemetry documentation. This may require works by the DevOps team as well e.g. K8s related attributes.","tokens":5735,"length":18402,"chunks":[{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"13. Telemetry On this page Observability 13.1 Introduction ​ For observability, the framework supports Application Performance Monitoring(APM) abd Business Performance Monitoring(BPM) out of the box. This includes distributed trace context propagation across sync and async channels, logging and basic metrics. For the same, we are leveraging the OpenTelemetry standard and its supporting tech ecosystem. Not even a single request must go untracked! 13.1.1 Architecture ​ Both Traces and Metrics are sent to OTEL Collector directly. Tempo is used as tracing backend for traces and Prometheus is used for metrics with Mimir as its backend. For Logs , a fluent bit daemonset is running on node, which collects logs from various applications on the node. Loki is used as logs aggregation solution.","content_length":794,"content_tokens":165,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"13.2 Goals ​ Auto application performance monitoring ​ No code APM across microservices, integrable with standard APM tools and logging backends, without any dev effort. Backend agnostic ​ Numerous open source and commercial softwares for Observability support OpenTelemetry out of the box, allowing one to switch between them if needed. Complete debuggability ​ Collect, correlate and debug signals across logs (events), traces and metrics, based on the request id and the attributes defined for the organization. For example, app version, function, DB query, K8s pod, domain, microservice etc. 13.3 Configuration ​ 13.3.1 OTEL exporter endpoint ​ Specify the IP address of your OTEL collector as env variable. Refer OTEL Exporter for more information.","content_length":753,"content_tokens":164,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"$ export OTEL_EXPORTER_OTLP_ENDPOINT=<IP of OTEL collector>:4317 For example, export OTEL_EXPORTER_OTLP_ENDPOINT=http://172.17.0.1:4317 13.3.2 OTEL service name ​ Specify the service name by which you want to setup observability and set it as env variable. $ export OTEL_SERVICE_NAME=sample_proj1 Let's assume you have setup SigNoz as the exporter then you will see something like this: In case you have any questions, please reach out to us on our Discord channel  13.3.3 Logging ​ 13.3.3.1 Log level ​ The minimum level set to log above this level. Please refer Pino log levels for more information.","content_length":601,"content_tokens":178,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"Set log_level in Static variables 13.3.3.2 Log fields masking ​ If you want to hide sensitive information in logs then define the fields which need to be hidden in redact feature in Static variables  Please refer Pino redaction paths for more information. 13.3.3.3 Log format ​ By default, the logs are dumped in OTEL Logging format when you deploy your service anywhere (UAT, Prod, K8s, etc.) except inside the vscode remote containers/dev containers.","content_length":452,"content_tokens":110,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"{\"Body\":\"adding body schema for /upload_doc.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_multiple_docs.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_s3.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /another_workflow post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /create/:entity_type post\",\"Timestamp\":\"1676531763728000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} ","content_length":1262,"content_tokens":376,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"{\"Body\":\"args.retry {\\\"max_attempts\\\":3,\\\"type\\\":\\\"constant\\\",\\\"interval\\\":5000}\",\"Timestamp\":\"1676531764656000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Result of _executeFn test_step1 {\\\"success\\\":true,\\\"code\\\":200,\\\"data\\\":{\\\"args\\\":{},\\\"data\\\":\\\"{\\\\\\\"data\\\\\\\":{\\\\\\\"lan\\\\\\\":\\\\\\\"12345\\\\\\\"}}\\\",\\\"files\\\":{},\\\"form\\\":{},\\\"headers\\\":{\\\"Accept\\\":\\\"application/json, text/plain, */*\\\",\\\"Content-Length\\\":\\\"24\\\",\\\"Content-Type\\\":\\\"application/json\\\",\\\"Host\\\":\\\"httpbin.org\\\",\\\"Traceparent\\\":\\\"00-a58ef2d7ff7725c39f1e058bf22fe724-2f13e28430d61bdb-01\\\",\\\"User-Agent\\\":\\\"axios/0.25.0\\\",\\\"X-Amzn-Trace-Id\\\":\\\"Root=1-63edd835-22cff8e60555fa522c8544cf\\\"},\\\"json\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"method\\\":\\\"POST\\\",\\\"origin\\\":\\\"180.188.224.177\\\",\\\"url\\\":\\\"https://httpbin.org/anything\\\"},\\\"message\\\":\\\"OK\\\",\\\"headers\\\":{\\\"date\\\":\\\"Thu, 16 Feb 2023 07:16:05 GMT\\\",\\\"content-type\\\":\\\"application/json\\\",\\\"content-length\\\":\\\"598\\\",\\\"connection\\\":\\\"close\\\",\\\"server\\\":\\\"gunicorn/19.9.0\\\",\\\"access-control-allow-origin\\\":\\\"*\\\",\\\"access-control-allow-credentials\\\":\\\"true\\\"}}\",\"Timestamp\":\"1676531765810000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Validate Response JSON Schema Success\",\"Timestamp\":\"1676531765811000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"}} Dev Format The dev format is basically a transformation of OTEL log format to increase readability for developers.","content_length":2254,"content_tokens":814,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"Please note that the default logging format inside vscode dev container on your local machine is dev format as given below: datetime [SeverityText] TraceId SpanId {Attributes} Body Sample Logs: 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_doc.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_multiple_docs.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_s3.http.post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /another_workflow post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /create/:entity_type post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /document post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /fn_script post ","content_length":741,"content_tokens":229,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"16/02/23, 12:44:43 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} args.retry {\"max_attempts\":3,\"type\":\"constant\",\"interval\":5000} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} Result of _executeFn test_step1 {\"success\":true,\"code\":200,\"data\":{\"args\":{},\"data\":\"{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}}\",\"files\":{},\"form\":{},\"headers\":{\"Accept\":\"application/json, text/plain, */*\",\"Content-Length\":\"24\",\"Content-Type\":\"application/json\",\"Host\":\"httpbin.org\",\"Traceparent\":\"00-f9f61d4940e3a8e5be8bc80faf6e36a2-f6c0a5ce67f5b07c-01\",\"User-Agent\":\"axios/0.25.0\",\"X-Amzn-Trace-Id\":\"Root=1-63edd7e4-0b8b6ba319833492520e6b0c\"},\"json\":{\"data\":{\"lan\":\"12345\"}},\"method\":\"POST\",\"origin\":\"180.188.224.177\",\"url\":\"https://httpbin.org/anything\"},\"message\":\"OK\",\"headers\":{\"date\":\"Thu, 16 Feb 2023 07:14:44 GMT\",\"content-type\":\"application/json\",\"content-length\":\"598\",\"connection\":\"close\",\"server\":\"gunicorn/19.9.0\",\"access-control-allow-origin\":\"*\",\"access-control-allow-credentials\":\"true\"}} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"} Validate Response JSON Schema Success note If you want to change the OTEL format to dev format , then set the environment variable NODE_ENV to dev in your environment as given below.","content_length":1525,"content_tokens":598,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"The default value of NODE_ENV is production  export NODE_ENV=dev 13.3.3.4 Add custom identifiers in logs ​ You can add any custom identifier in the logging whenever any event is triggered on your service. The value for the custom identifier will be picked up from event body, params, query, or headers. To enable this feature ,you need to specify two things: log_attributes variable as environment variable / static variable which contains custom identifiers. For example, this is the sample static configuration: log_attributes: mobileNumber: \"query?.mobileNumber\" id: \"params?.id\" lan: \"body?.data?.lan\" location of the identifier in the request payload. As specified in the above example, if mobileNumber is present in query params then specify query?.mobileNumber  if id is present in path params then specify params?.id if lan is present in data field inside body then specify body?.data?.lan  note Please make sure to add ? in case any field is optional like body?.data?.lan so that it works well with undefined values. This will add lan in the logs if it is present else it will not get added.","content_length":1099,"content_tokens":256,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"Sample Logs Dev format 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} Processing event /test/:id.http.post 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event inputs {\"baseUrl\":\"\",\"body\":{\"data\":{\"lan\":\"12345\"}},\"fresh\":false,\"hostname\":\"localhost\",\"ip\":\"::ffff:172.22.0.1\",\"ips\":[],\"method\":\"POST\",\"originalUrl\":\"/test/12?mobileNumber=9878987898\",\"params\":{\"id\":\"12\"},\"path\":\"/test/12\",\"protocol\":\"http\",\"query\":{\"mobileNumber\":\"9878987898\"},\"route\":{\"path\":\"/test/:id\",\"stack\":[{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"},{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"}],\"methods\":{\"post\":true}},\"secure\":false,\"stale\":true,\"subdomains\":[],\"xhr\":false,\"headers\":{\"content-type\":\"application/json\",\"user-agent\":\"PostmanRuntime/7.29.2\",\"accept\":\"*/*\",\"postman-token\":\"835edd29-7c36-4e11-9b79-c661bbd911b0\",\"host\":\"localhost:4000\",\"accept-encoding\":\"gzip, deflate, br\",\"connection\":\"keep-alive\",\"content-length\":\"46\"},\"files\":[]} 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event body and eventSpec exist OTEL format {\"Body\":\"Processing event /test/:id.http.post\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event inputs {\\\"baseUrl\\\":\\\"\\\",\\\"body\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"fresh\\\":false,\\\"hostname\\\":\\\"localhost\\\",\\\"ip\\\":\\\"::ffff:172.22.0.1\\\",\\\"ips\\\":[],\\\"method\\\":\\\"POST\\\",\\\"originalUrl\\\":\\\"/test/12?mobileNumber=9878987898\\\",\\\"params\\\":{\\\"id\\\":\\\"12\\\"},\\\"path\\\":\\\"/test/12\\\",\\\"protocol\\\":\\\"http\\\",\\\"query\\\":{\\\"mobileNumber\\\":\\\"9878987898\\\"},\\\"route\\\":{\\\"path\\\":\\\"/test/:id\\\",\\\"stack\\\":[{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"},{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"}],\\\"methods\\\":{\\\"post\\\":true}},\\\"secure\\\":false,\\\"stale\\\":true,\\\"subdomains\\\":[],\\\"xhr\\\":false,\\\"headers\\\":{\\\"content-type\\\":\\\"application/json\\\",\\\"user-agent\\\":\\\"PostmanRuntime/7.29.2\\\",\\\"accept\\\":\\\"*/*\\\",\\\"postman-token\\\":\\\"9e57df7d-0a75-48b6-bc52-921bd5c045b7\\\",\\\"host\\\":\\\"localhost:4000\\\",\\\"accept-encoding\\\":\\\"gzip, deflate, br\\\",\\\"connection\\\":\\\"keep-alive\\\",\\\"content-length\\\":\\\"46\\\"},\\\"files\\\":[]}\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event body and eventSpec exist\",\"Timestamp\":\"1676960742404000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} 13.4 Custom metrics, traces and logs (BPM) ​ Custom metrics, traces and logs can be added in the workflow DSL at each task level then these will be available out of the box along with APM.","content_length":4052,"content_tokens":1478,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"13.4.1 DSL spec for custom metrics ​ # refer https://github.com/siimon/prom-client metrics: - name: metric_name type: counter|gauge|histogram|summary labels: label1: val1 label2: val2 # followng functions depending on the metric type and all of them could be scripts, can use inputs/outputs inc: 10 dec: 10 set: 100 observe: 2000 timer: true|false(boolean) starts at the beginning of workflow/task and ends at the end of workflow/task Example spec ​ In the following example, we are using two custom metrics: httpbin_calls_total: counter type metric, counter is incremented by 1. httpbin_calls_duration: histogram type metric, timer is set to true to record duration. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data.","content_length":894,"content_tokens":228,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"It will send back same as response fn: com.gs.http metrics: - name: httpbin_calls_total help: 'httpbin_calls_total counter of httpbin requests labeled with: method, status_code' type: counter labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> inc: 1 - name: httpbin_calls_duration help: 'httpbin_calls_duration duration histogram of httpbin responses labeled with: method, status_code' type: histogram labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> timer: true args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post 13.4.2 DSL spec for custom trace ​ trace: name: span_name attributes: attribute1: value1 attribute2: value2 Example spec ​ In the following example, we are creating a new span named httpbin_trace with span attributes request and param This span gets created when the task starts and ended when the task completes its execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data.","content_length":1171,"content_tokens":304,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"It will send back same as response fn: com.gs.http trace: name: httpbin_trace attributes: request: <%inputs.body%> param: <%inputs.query%> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post 13.4.3 DSL spec for custom logs ​ logs: before: level: fatal|error|warn|info|debug|trace # refer pino for levels message: 'Sample log before' params: param1: val1 param2: val2 attributes: request: query: <%inputs.query%> after: level: info message: 'Sample log after' params: attributes: The logs are dumped in OTEL format. Please refer to OTEL Logging Data model for understanding of fields dumped in the logs. message and params are part of Body field and attributes are part of Attributes field in the log. Example spec ​ In the following example, we are two additional logs before and after the task execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data.","content_length":1094,"content_tokens":280,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"It will send back same as response fn: com.gs.http logs: before: level: error message: 'Hello' params: - key1: v1 key2: v2 - v1 attributes: request: <%inputs.query%> after: level: error message: 'World' params: key1: v1 key2: v2 attributes: customer_name: <% outputs.httpbin_step1.data.json.customer_name %> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post Sample Logs {\"Body\":\"Hello [{\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"},\\\"v1\\\"]\",\"Timestamp\":\"1676011973016000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"request\":{\"status\":\"Hello\"},\"task_id\":\"if\",\"workflow_name\":\"if_else\"}}","content_length":846,"content_tokens":292,"embedding":[]},{"doc_title":"Observability | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/telemetry/intro","doc_date":"2023-05-09T19:48:30.058Z","content":"{\"Body\":\"World {\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"}\",\"Timestamp\":\"1676011973019000000\",\"SeverityNumber\":17,\"SeverityText\":\"ERROR\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"customer_name\":\"Hell!\",\"task_id\":\"if\",\"workflow_name\":\"if_else\"}} 13.5 Observability Stack ​ The complete observability stack with K8s helm-charts will be made available soon. 13.6 Recommended model for telemetry signals ​ Please find the draft documentation here  This is compiled in one place from various references across the OpenTelemetry documentation. This may require works by the DevOps team as well e.g. K8s related attributes.","content_length":771,"content_tokens":229,"embedding":[]}]},{"title":"Events | Godspeed Docs","url":"https://docs.godspeed.systems/docs/writing-business-logic/events","date":"2023-05-09T19:48:30.263Z","content":"On this page Introduction Events are used to expose the functions of this microservice to the external world. Whether via HTTP, message bus, gRPC or socket. HTTP events ​ Two events needed to complete an HTTP call. HTTP request event ​ Whenever an event has event.full.name._http.{method_name} as part of its name, then the framework will start listening on the URL ${app_base_url}/event/full/name against the method ${method_name} which can be PUT, GET, POST etc. The body, params, headers and query of the HTTP request are serialized by the framework into the event object and passed to the __handler (function handler for this event) which consumes this request and returns a reponse. Implicitly the framework will emit a response for every event which can again be an event or yaml execution. Convention is event_name.response, but if we want to emit any other event, we can invoke any other DAG, it could send event to message_bus or GRPC also. Sample input definition ​ do_KYC.__http.post: # exposed by convention as REST URL: app_base_url/do_KYC on method POST __handler: __src.com.abc.do_KYC __data: # {body, params, query, headers} Bank API POST url is: /create_loan/${pan}/?user_id=${user_id} & body takes {user_name, address} __example: body: user_name: Ayush pan: AKJP**** address: India # In case of HTTP event, query, headers and params will be also present headers: query: params: __schema: #Validation will happen after recieving and before sending out event. In case of HTTP channel, payload will have metadata.http.{headers, params, query} # ${config.src.com.pinelabs.li.schemas.create_loan_api} #using template written elsewhere. For ex. compulsory pan, user_name, address body: type: object properties: user_name: string pan: string address: string required: [user_name, pan, address] headers: query: params: #__response: someother.do_KYC.http.post.response # Can be implicit or explicit (in case of custom event names) Sample response event definition ​ The framework, upon recieving the __response , emits another event whose name is event.full.name._http.{method_name}.response , by convention. This event is in turn caught by the framework internally itself, and passed on to the http_event_handler which then sends over the __response data to the HTTP caller. Every event has a schema which determines the shape of its data. In case of HTTP event, it will have body, params, query and headers in its data. Further, note that the event emmitted by the framework will have CloudEvents specific format. This includes the events which returns the HTTP response. This means that the HTTP response will also have CloudEvents format. do_KYC.__http.post.response: #this URI can be customized. By default eventName.response is the actual response event. # __handler: __http.response_handler # provided by the framework. No need to specify here __data: # Bank API POST url is: /create_loan/${pan}/?user_id=${user_id} & body takes {user_name, address} __example: body: user_id: 1 headers: __schema: #Validation will happen after recieving and before sending out event. In case of HTTP channel, payload will have metadata.http.{headers, params, query} # Schema can be in any shape, which is supported by the event handler attached to this event 200: *200-json-schema 400: *400-json-schema 500: *500-json-schema","tokens":840,"length":3323,"chunks":[{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/events","doc_date":"2023-05-09T19:48:30.263Z","content":"On this page Introduction Events are used to expose the functions of this microservice to the external world. Whether via HTTP, message bus, gRPC or socket. HTTP events ​ Two events needed to complete an HTTP call. HTTP request event ​ Whenever an event has event.full.name._http.{method_name} as part of its name, then the framework will start listening on the URL ${app_base_url}/event/full/name against the method ${method_name} which can be PUT, GET, POST etc. The body, params, headers and query of the HTTP request are serialized by the framework into the event object and passed to the __handler (function handler for this event) which consumes this request and returns a reponse. Implicitly the framework will emit a response for every event which can again be an event or yaml execution.","content_length":796,"content_tokens":181,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/events","doc_date":"2023-05-09T19:48:30.263Z","content":"Convention is event_name.response, but if we want to emit any other event, we can invoke any other DAG, it could send event to message_bus or GRPC also. Sample input definition ​ do_KYC.__http.post: # exposed by convention as REST URL: app_base_url/do_KYC on method POST __handler: __src.com.abc.do_KYC __data: # {body, params, query, headers} Bank API POST url is: /create_loan/${pan}/?user_id=${user_id} & body takes {user_name, address} __example: body: user_name: Ayush pan: AKJP**** address: India # In case of HTTP event, query, headers and params will be also present headers: query: params: __schema: #Validation will happen after recieving and before sending out event.","content_length":678,"content_tokens":196,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/events","doc_date":"2023-05-09T19:48:30.263Z","content":"In case of HTTP channel, payload will have metadata.http.{headers, params, query} # ${config.src.com.pinelabs.li.schemas.create_loan_api} #using template written elsewhere. For ex. compulsory pan, user_name, address body: type: object properties: user_name: string pan: string address: string required: [user_name, pan, address] headers: query: params: #__response: someother.do_KYC.http.post.response # Can be implicit or explicit (in case of custom event names) Sample response event definition ​ The framework, upon recieving the __response , emits another event whose name is event.full.name._http.{method_name}.response , by convention.","content_length":641,"content_tokens":167,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/events","doc_date":"2023-05-09T19:48:30.263Z","content":"This event is in turn caught by the framework internally itself, and passed on to the http_event_handler which then sends over the __response data to the HTTP caller. Every event has a schema which determines the shape of its data. In case of HTTP event, it will have body, params, query and headers in its data. Further, note that the event emmitted by the framework will have CloudEvents specific format. This includes the events which returns the HTTP response. This means that the HTTP response will also have CloudEvents format. do_KYC.__http.post.response: #this URI can be customized. By default eventName.response is the actual response event. # __handler: __http.response_handler # provided by the framework.","content_length":717,"content_tokens":156,"embedding":[]},{"doc_title":"Events | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/events","doc_date":"2023-05-09T19:48:30.263Z","content":"No need to specify here __data: # Bank API POST url is: /create_loan/${pan}/?user_id=${user_id} & body takes {user_name, address} __example: body: user_id: 1 headers: __schema: #Validation will happen after recieving and before sending out event. In case of HTTP channel, payload will have metadata.http.{headers, params, query} # Schema can be in any shape, which is supported by the event handler attached to this event 200: *200-json-schema 400: *400-json-schema 500: *500-json-schema.","content_length":488,"content_tokens":142,"embedding":[]}]},{"title":"Godspeed DSL | Godspeed Docs","url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","date":"2023-05-09T19:48:30.445Z","content":"On this page Introduction The DSL provided by Godspeed is an extension of the YAML spec. Note: Also every keyword will start with double underscores. Name ​ A name of the function. It will has response data as the value to its key. __name: step1 Summary ​ A basic sumary of the function. __summary: A sample summary of the function Description ​ A basic description of the function. __summary: A sample description of the function Args ​ The YAML function argument that is needed in the function dag, we specify here. The arg consists of 2 parts An example of the how the argument looks for the function using __example __example: sample_arg_1: sample_value_1 sample_arg_2: sample_value_2 Three types of functions ​ The DSL allows one to define functions in YAML format. A function definition can invoke A single function using __ref . This means, internally we would call this function. __ref: __src.com.abc.anuj A series of functions using __sequence . This means, that we want to run multiple function in our DAG in sequential manner. __sequence: - __ref: __modules.imported_module_1.some_function - __ref: __modules.imported_module_2.some_function_2 A parallel exeuction of list of functions using __parallel . This means, that we want to run multiple function in our DAG in parallel manner. __parallel: - __ref: __modules.imported_module_1.some_function - __ref: __modules.imported_module_2.some_function_2 The functions wrapped around and invoked can be either JS, or TS or YAML functions. Also we would only choose of the options between __ref , __sequential , _parallel . The name contains the response data in nested manner. Say we want data from step1 of DAG, the way to access that would be __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} Hooks ​ Hooks are like decorators in Python world or annotations in Java world, where you define logic that must execute before or after a function call. The framework has some common hooks for all functions in the project, like Telemetry hooks. But the developer has the flexibility to override or change commmon hooks through function overriding defined below. Here are the different kind of hook functions possible __hooks: __pre_validations: # This ref is executed for pre_validation __validations: # This ref is executed for validation __preauths: # This ref is executed for pre_auths __auths: # This ref is executed for auths __pre_ref: # This ref is executed before the function call itself __post_ref: # This ref is executed after the function call itself __on_error: # This ref is executed if the function raises an error __finally: # This ref is executed when the execution of the function call is done An example on hooks for __on_error __hooks: __on_error: - __ref: __log __args: data: key1:value1 key2:value2 Example using single function ​ ``` com: xyz: someFn: __name: step1 __summary: Summary of this function __description: long description __args: __example: __schema: __ref: com.abc.anuj # JS, TS, yaml __src.com.a.b.c, __modules __args: arg1: 5 arg2: Hello World __hooks: __pre_validations: __validations: __preauths: __auths: __pre_ref: __post_ref: __on_error: __finally: ``` Sequence or parallel execution of list of functions ​ ``` __sequence: - __ref: src.com.abc.a_function - __parallel: - __ref: src.com.abc.b_function - __ref: src.com.abc.c_function ``` Sequence or parallel business logic over an array of items or an object ​ __sequence: __args: items: ${__request.body.items} as: item_name __sequence: - __ref: __if_else # or __sequence or __parallel __args: when: ${__vars.item_name}: in: ${__config.shop.inventory.unavailable_items} then: __ref: __continue # also there is __break - __ref: com.ecommerce.add_to_invoice # or __sequence or __parallel __args: text: ${__vars.item_name} Overriding a function ​ Written within the project's src folder (DSL, JS or TS functions) Exported by the modules included in the project (via package.json) Context variables ​ Purpose __super (for overriding) ​ __vars ​ All variables created using __assign are available under __vars __config ​ All variables that are part of the project config are present in the __config variable __src ​ The __src variables contains every function under source folder(js, ts, yaml). We can access all these functions using the above variable __env ​ The __env variables contains all the environment variables __response ​ The __response has object for the incoming data. How to define variables ​ __ref: __assign __name: nested_step __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} How to use variables ​ __summary: Create loan in LOS __ref: __http_post __args: url: ${__vars.step1.code}/${__vars.user_id}/?user_name=${__vars.user_name} body: # body is {user_name: string, address: string} user_name: ${__event.data.body.user_name} address: ${__request.body.address} loan_id: ${__response.step1.data.loan_id} query: user_id: ${__vars.user_id} , config, src, modules, env, event.data, response (starting from the first parent span), __args (of the running GS instruction) Examples ​ com: pinelabs: create_account_hdfc: __summary: Multiplexing create loan for hdfc api calls __args: __parallel: - __name: step1 # the response of this will be accessible within the parent step key, under the step1 sub key __description: create account in the bank __ref: __http_post __args: url: ${__config.banks.urls.hdfc.create_user}/${__args.pan} body: # body is {user_name: string, address: string} user_name: ${__request.body.user_name} address: ${__request.body.address} query: user_name: ${__request.query.user_name} headers: xyz: 2134234 __hooks: __on_error: - __ref: __log __args: level: data: key1: val1 key2: val2 - __description: create account in our LOS __name: step2 __sequence: - __ref: __assign __name: nested_step __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} - __summary: Create loan in LOS __ref: __http_post __args: url: ${__vars.step1.code}/${__vars.user_id}/?user_name=${__vars.user_name} body: # body is {user_name: string, address: string} user_name: ${__event.data.body.user_name} address: ${__request.body.address} loan_id: ${__response.step1.data.loan_id} query: user_id: ${__vars.user_id} headers: access_token: ${__env.hdfc_token} __hooks: __on_error: - ${__super.__on_error} __finally: - ${__super.preauths} __on_err: __args: acceptable: true retry: count: 3 interval: 100 #milliseconds strategy: incremental_backoff onError: __ref: *alias_deadqueue_ref saga_compensation: __ref: __notification__email __args: #args for compensation __on_err: __args: ignore: true - __description: send event to message bus of successful loan creation __name: item_name __ref: __send_message # For example send the request to MB to notify any consumers that this API has been called on this microservice __args: topic: com.abc.li.create_loan.success body: data: ${__request} headers: custom_header_1: custom_value - __description: Save in the DB __ref: __data.insert __args: _type: user _id: ${user_id} body: user_name: ${__vars.user_name} hooks: validation: auths: - role_permission __on_error: __return:","tokens":2098,"length":7347,"chunks":[{"doc_title":"Godspeed DSL | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","doc_date":"2023-05-09T19:48:30.445Z","content":"On this page Introduction The DSL provided by Godspeed is an extension of the YAML spec. Note: Also every keyword will start with double underscores. Name ​ A name of the function. It will has response data as the value to its key. __name: step1 Summary ​ A basic sumary of the function. __summary: A sample summary of the function Description ​ A basic description of the function. __summary: A sample description of the function Args ​ The YAML function argument that is needed in the function dag, we specify here. The arg consists of 2 parts An example of the how the argument looks for the function using __example __example: sample_arg_1: sample_value_1 sample_arg_2: sample_value_2 Three types of functions ​ The DSL allows one to define functions in YAML format. A function definition can invoke A single function using __ref  This means, internally we would call this function.","content_length":886,"content_tokens":199,"embedding":[]},{"doc_title":"Godspeed DSL | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","doc_date":"2023-05-09T19:48:30.445Z","content":"__ref: __src.com.abc.anuj A series of functions using __sequence  This means, that we want to run multiple function in our DAG in sequential manner. __sequence: - __ref: __modules.imported_module_1.some_function - __ref: __modules.imported_module_2.some_function_2 A parallel exeuction of list of functions using __parallel  This means, that we want to run multiple function in our DAG in parallel manner. __parallel: - __ref: __modules.imported_module_1.some_function - __ref: __modules.imported_module_2.some_function_2 The functions wrapped around and invoked can be either JS, or TS or YAML functions. Also we would only choose of the options between __ref , __sequential , _parallel  The name contains the response data in nested manner.","content_length":742,"content_tokens":198,"embedding":[]},{"doc_title":"Godspeed DSL | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","doc_date":"2023-05-09T19:48:30.445Z","content":"Say we want data from step1 of DAG, the way to access that would be __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} Hooks ​ Hooks are like decorators in Python world or annotations in Java world, where you define logic that must execute before or after a function call. The framework has some common hooks for all functions in the project, like Telemetry hooks. But the developer has the flexibility to override or change commmon hooks through function overriding defined below.","content_length":576,"content_tokens":142,"embedding":[]},{"doc_title":"Godspeed DSL | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","doc_date":"2023-05-09T19:48:30.445Z","content":"Here are the different kind of hook functions possible __hooks: __pre_validations: # This ref is executed for pre_validation __validations: # This ref is executed for validation __preauths: # This ref is executed for pre_auths __auths: # This ref is executed for auths __pre_ref: # This ref is executed before the function call itself __post_ref: # This ref is executed after the function call itself __on_error: # This ref is executed if the function raises an error __finally: # This ref is executed when the execution of the function call is done An example on hooks for __on_error __hooks: __on_error: - __ref: __log __args: data: key1:value1 key2:value2 Example using single function ​ ``` com: xyz: someFn: __name: step1 __summary: Summary of this function __description: long description __args: __example: __schema: __ref: com.abc.anuj # JS, TS, yaml __src.com.a.b.c, __modules __args: arg1: 5 arg2: Hello World __hooks: __pre_validations: __validations: __preauths: __auths: __pre_ref: __post_ref: __on_error: __finally: ``` Sequence or parallel execution of list of functions ​ ``` __sequence: - __ref: src.com.abc.a_function - __parallel: - __ref: src.com.abc.b_function - __ref: src.com.abc.c_function ``` Sequence or parallel business logic over an array of items or an object ​ __sequence: __args: items: ${__request.body.items} as: item_name __sequence: - __ref: __if_else # or __sequence or __parallel __args: when: ${__vars.item_name}: in: ${__config.shop.inventory.unavailable_items} then: __ref: __continue # also there is __break - __ref: com.ecommerce.add_to_invoice # or __sequence or __parallel __args: text: ${__vars.item_name} Overriding a function ​ Written within the project's src folder (DSL, JS or TS functions) Exported by the modules included in the project (via package.json) Context variables ​ Purpose __super (for overriding) ​ __vars ​ All variables created using __assign are available under __vars __config ​ All variables that are part of the project config are present in the __config variable __src ​ The __src variables contains every function under source folder(js, ts, yaml) We can access all these functions using the above variable __env ​ The __env variables contains all the environment variables __response ​ The __response has object for the incoming data.","content_length":2307,"content_tokens":632,"embedding":[]},{"doc_title":"Godspeed DSL | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions","doc_date":"2023-05-09T19:48:30.445Z","content":"How to define variables ​ __ref: __assign __name: nested_step __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} How to use variables ​ __summary: Create loan in LOS __ref: __http_post __args: url: ${__vars.step1.code}/${__vars.user_id}/?user_name=${__vars.user_name} body: # body is {user_name: string, address: string} user_name: ${__event.data.body.user_name} address: ${__request.body.address} loan_id: ${__response.step1.data.loan_id} query: user_id: ${__vars.user_id} , config, src, modules, env, event.data, response (starting from the first parent span), __args (of the running GS instruction) Examples ​ com: pinelabs: create_account_hdfc: __summary: Multiplexing create loan for hdfc api calls __args: __parallel: - __name: step1 # the response of this will be accessible within the parent step key, under the step1 sub key __description: create account in the bank __ref: __http_post __args: url: ${__config.banks.urls.hdfc.create_user}/${__args.pan} body: # body is {user_name: string, address: string} user_name: ${__request.body.user_name} address: ${__request.body.address} query: user_name: ${__request.query.user_name} headers: xyz: 2134234 __hooks: __on_error: - __ref: __log __args: level: data: key1: val1 key2: val2 - __description: create account in our LOS __name: step2 __sequence: - __ref: __assign __name: nested_step __args: create_user_url: ${__config.los.urls.create_user} user_id: ${__response.data.step1.data.user_id} user_name: ${__request.params.user_name} - __summary: Create loan in LOS __ref: __http_post __args: url: ${__vars.step1.code}/${__vars.user_id}/?user_name=${__vars.user_name} body: # body is {user_name: string, address: string} user_name: ${__event.data.body.user_name} address: ${__request.body.address} loan_id: ${__response.step1.data.loan_id} query: user_id: ${__vars.user_id} headers: access_token: ${__env.hdfc_token} __hooks: __on_error: - ${__super.__on_error} __finally: - ${__super.preauths} __on_err: __args: acceptable: true retry: count: 3 interval: 100 #milliseconds strategy: incremental_backoff onError: __ref: *alias_deadqueue_ref saga_compensation: __ref: __notification__email __args: #args for compensation __on_err: __args: ignore: true - __description: send event to message bus of successful loan creation __name: item_name __ref: __send_message # For example send the request to MB to notify any consumers that this API has been called on this microservice __args: topic: com.abc.li.create_loan.success body: data: ${__request} headers: custom_header_1: custom_value - __description: Save in the DB __ref: __data.insert __args: _type: user _id: ${user_id} body: user_name: ${__vars.user_name} hooks: validation: auths: - role_permission __on_error: __return:","content_length":2826,"content_tokens":927,"embedding":[]}]},{"title":"Core Interfaces | Godspeed Docs","url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","date":"2023-05-09T19:48:30.594Z","content":"On this page Core Interfaces In this, we cover some of the base interfaces of Godspeed runtime module. These will be widely used across entire Godspeed SDK. These interface definitions will be put under Apache 2 license so that our clients have no lockin with Godspeed, while enjoying the best of it! Type GSInstruction ​ A GSInstruction is a wrapper around actual business logic aka function or action or even another GSInstruction. This is a core building block of the Godspeed SDK modules, function DAG composition, and the proposed way of doing things from a flexibility, seperation of concern and dynamism point of view. GSInstruction allows to wrap a set of checks and custom behavior before and after the actual action (an atomic action unit of the business logic). The actual function of a GSInstruction can be written in any language. Except for JS and TS, all other language functions are compiled to Webassembly and run on this framework. GSInstruction also has the finally clause to wrap up before returning from this instruction. Function DAG is also an Instruction comprising of set of Instructions recursively. This layered design has following benefits Separation of concern Business logic is decoupled from authorizations, validations, auto instrumentation, auto exposing REST/Event interface etc. This can be plugged into any kind of microservice framework, including Godspeed. Flexibility It gives developer flexibility to implement different business flows around a common action. This can be across products, A/B tests or tenants. This is achieved by reusing core actions while decorating them with different pre and post hooks. For example Geo fencing API monetization Rate limiting Tenant specific payload validation Dynamism: The pre and post hooks of an Instruction can be dynamically updated (added or removed) by the program, without restarting the service . Custom observability: Add business metrices or any other signals. GSInstruction Constructor ​ name: String, preAuthHooks?: [GSInstruction] // Auto telemetry, custom business logic, pre-loading. auths?: [GSAssert] //RBAC/ABAC/JWT onAuthError?:[GSAction] // What to do on error in auths validations?: [GSAssert | GSL] //GSL when parsed, its command should be implemented via a GSAssert interface onValidationsError?:[GSAction] // What to do on error in validations preFunctionHooks: [GSAction] onPreFunctionHooksError?:[GSAction] // What to do on error in PreFunctionHooks _function: [GSAction] onFunctionError?:[GSAction] //What to do on error in _functions postFunctionHooks?: [GSAction] //To act upon error, or success cases. finally?: [GSAction] GSInstruction.execute() ​ // Arguments ctx: GSContext, params: JSON GSReturn ​ res: GSResponse //On successful execution error: GSError //On Error events: [GSEvent] //Any events that happened when the instruction was running, in order of occurence. Usefor for Business Process Monitoring. Type: GSError ​ code: //An error code extending the standards message: String stack: [String]//Stack trace errors: [GSError] Type: GSResponse ​ code: //A GS error code extending the standards message: String data: Any object or data type Type GSContext ​ The GSContext includes all the context specific information like tracing information, actor, environment, headers, payload, shared state (if this ctx is shared with other instruction threads, this part can be shared with them), immutable state (personal copy, personal view, for concurrency) A context has the following properties data: Any object or data type //All memoized data with shared references. References can have promises or actual data. This is not concurrency safe. immutable: Any object or data type //For storing and accessing references which are unique to this execution context (for concurrency safety, in the functional programming way). actor: String //JWT and other user specific info headers: Object //In case there were any headers with the payload in messagebus or in HTTP payload: Object //The arguments to include in this function otel: Any object or data type //OTEL compliant trace/span information Type: GSAssert extends GSInstruction ​ This is typically a condition check that matches a condition against given data or data promise. ctx: GSContext data: JSON | GSDataFetch // Internally GSAssert invokes GSDataFetch.getData() and applies the pass/fail condition to it. condition: [GSCondition] // to run on the data, evaluating to true or false. Returns ​ On successful assertion ​ A GSResponse instance with following data pass: true | false On error ​ It return a GSError instance Type GSDataFetch extends GSInstruction ​ GSDataFetch.invoke() data or promise of it. GSDataFetch can be a DB/cache fetch instruction a file fetch instruction HTTP fetch instruction (Own or third party API) Data federation instruction Any kind of a function which returns some data Type GSCondition ​ A GSCondition interface is a function which evaluates to true or false, when supplied with context and data This condition can be GSInstruction A TS/JS function JSON schema instruction","tokens":1127,"length":5076,"chunks":[{"doc_title":"Core Interfaces | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","doc_date":"2023-05-09T19:48:30.594Z","content":"On this page Core Interfaces In this, we cover some of the base interfaces of Godspeed runtime module. These will be widely used across entire Godspeed SDK. These interface definitions will be put under Apache 2 license so that our clients have no lockin with Godspeed, while enjoying the best of it! Type GSInstruction ​ A GSInstruction is a wrapper around actual business logic aka function or action or even another GSInstruction. This is a core building block of the Godspeed SDK modules, function DAG composition, and the proposed way of doing things from a flexibility, seperation of concern and dynamism point of view. GSInstruction allows to wrap a set of checks and custom behavior before and after the actual action (an atomic action unit of the business logic) The actual function of a GSInstruction can be written in any language. Except for JS and TS, all other language functions are compiled to Webassembly and run on this framework.","content_length":948,"content_tokens":195,"embedding":[]},{"doc_title":"Core Interfaces | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","doc_date":"2023-05-09T19:48:30.594Z","content":"GSInstruction also has the finally clause to wrap up before returning from this instruction. Function DAG is also an Instruction comprising of set of Instructions recursively. This layered design has following benefits Separation of concern Business logic is decoupled from authorizations, validations, auto instrumentation, auto exposing REST/Event interface etc. This can be plugged into any kind of microservice framework, including Godspeed. Flexibility It gives developer flexibility to implement different business flows around a common action. This can be across products, A/B tests or tenants. This is achieved by reusing core actions while decorating them with different pre and post hooks. For example Geo fencing API monetization Rate limiting Tenant specific payload validation Dynamism: The pre and post hooks of an Instruction can be dynamically updated (added or removed) by the program, without restarting the service  Custom observability: Add business metrices or any other signals.","content_length":1000,"content_tokens":189,"embedding":[]},{"doc_title":"Core Interfaces | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","doc_date":"2023-05-09T19:48:30.594Z","content":"GSInstruction Constructor ​ name: String, preAuthHooks?: [GSInstruction] // Auto telemetry, custom business logic, pre-loading. auths?: [GSAssert] //RBAC/ABAC/JWT onAuthError?:[GSAction] // What to do on error in auths validations?: [GSAssert | GSL] //GSL when parsed, its command should be implemented via a GSAssert interface onValidationsError?:[GSAction] // What to do on error in validations preFunctionHooks: [GSAction] onPreFunctionHooksError?:[GSAction] // What to do on error in PreFunctionHooks _function: [GSAction] onFunctionError?:[GSAction] //What to do on error in _functions postFunctionHooks?: [GSAction] //To act upon error, or success cases. finally?: [GSAction] GSInstruction.execute() ​ // Arguments ctx: GSContext, params: JSON GSReturn ​ res: GSResponse //On successful execution error: GSError //On Error events: [GSEvent] //Any events that happened when the instruction was running, in order of occurence. Usefor for Business Process Monitoring.","content_length":969,"content_tokens":276,"embedding":[]},{"doc_title":"Core Interfaces | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","doc_date":"2023-05-09T19:48:30.594Z","content":"Type: GSError ​ code: //An error code extending the standards message: String stack: [String]//Stack trace errors: [GSError] Type: GSResponse ​ code: //A GS error code extending the standards message: String data: Any object or data type Type GSContext ​ The GSContext includes all the context specific information like tracing information, actor, environment, headers, payload, shared state (if this ctx is shared with other instruction threads, this part can be shared with them), immutable state (personal copy, personal view, for concurrency) A context has the following properties data: Any object or data type //All memoized data with shared references. References can have promises or actual data. This is not concurrency safe. immutable: Any object or data type //For storing and accessing references which are unique to this execution context (for concurrency safety, in the functional programming way)","content_length":911,"content_tokens":189,"embedding":[]},{"doc_title":"Core Interfaces | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/functions_old","doc_date":"2023-05-09T19:48:30.594Z","content":"actor: String //JWT and other user specific info headers: Object //In case there were any headers with the payload in messagebus or in HTTP payload: Object //The arguments to include in this function otel: Any object or data type //OTEL compliant trace/span information Type: GSAssert extends GSInstruction ​ This is typically a condition check that matches a condition against given data or data promise. ctx: GSContext data: JSON | GSDataFetch // Internally GSAssert invokes GSDataFetch.getData() and applies the pass/fail condition to it. condition: [GSCondition] // to run on the data, evaluating to true or false. Returns ​ On successful assertion ​ A GSResponse instance with following data pass: true | false On error ​ It return a GSError instance Type GSDataFetch extends GSInstruction ​ GSDataFetch.invoke() data or promise of it. GSDataFetch can be a DB/cache fetch instruction a file fetch instruction HTTP fetch instruction (Own or third party API) Data federation instruction Any kind of a function which returns some data Type GSCondition ​ A GSCondition interface is a function which evaluates to true or false, when supplied with context and data This condition can be GSInstruction A TS/JS function JSON schema instruction.","content_length":1240,"content_tokens":280,"embedding":[]}]},{"title":"Business Logic | Godspeed Docs","url":"https://docs.godspeed.systems/docs/writing-business-logic/intro","date":"2023-05-09T19:48:30.752Z","content":"On this page Business Logic Introduction ​ You can express and run your business logic with Godspeed microservice or servlerless, in the two basic ways Within the Godspeed framework's runtime As YAML function DAG (Godspeed DSL) As JS/TS In a different runtime (Using any language or framework) Via HTTP, gRpc or event interface The two basic concepts to learn here are functions and events With Godspeed microservice framework ​ The Godspeed framework's runtime has capability to execute a function DAG written in the Godspeed DSL, or in JS/TS. Both kinds of business logic expressions are executed by Godspeed, as GSInstruction . The framework patches in and executes the business logic as GSInstructions which also provide middleware hooking mechanism. This decouples the function logic and middleware from the runtime environment. When patched into a microservice, the function gets automatically exposed through REST, event driven and socket interfaces, based on the api schema settings. There is a standard way to define and patch business logic and middleware into a Godspeed microservice or in serverless workflows, during both the process load time and run time. Steps ​ Use Godspeed CLI to generate the folder template (scaffolding) for a project. Import other modules in the project, as per the templating specifications. Write your business logic in YAML DSL or JS/TS, in the scaffolded module directory. Modify the microservice config including which namespace/functions of this project are to be exposed as REST/Event endpoints, with query validations. Start the Godspeed service specifying the path to the project folder.","tokens":337,"length":1635,"chunks":[{"doc_title":"Business Logic | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/intro","doc_date":"2023-05-09T19:48:30.752Z","content":"On this page Business Logic Introduction ​ You can express and run your business logic with Godspeed microservice or servlerless, in the two basic ways Within the Godspeed framework's runtime As YAML function DAG (Godspeed DSL) As JS/TS In a different runtime (Using any language or framework) Via HTTP, gRpc or event interface The two basic concepts to learn here are functions and events With Godspeed microservice framework ​ The Godspeed framework's runtime has capability to execute a function DAG written in the Godspeed DSL, or in JS/TS. Both kinds of business logic expressions are executed by Godspeed, as GSInstruction  The framework patches in and executes the business logic as GSInstructions which also provide middleware hooking mechanism. This decouples the function logic and middleware from the runtime environment. When patched into a microservice, the function gets automatically exposed through REST, event driven and socket interfaces, based on the api schema settings.","content_length":990,"content_tokens":200,"embedding":[]},{"doc_title":"Business Logic | Godspeed Docs","doc_url":"https://docs.godspeed.systems/docs/writing-business-logic/intro","doc_date":"2023-05-09T19:48:30.752Z","content":"There is a standard way to define and patch business logic and middleware into a Godspeed microservice or in serverless workflows, during both the process load time and run time. Steps ​ Use Godspeed CLI to generate the folder template (scaffolding) for a project. Import other modules in the project, as per the templating specifications. Write your business logic in YAML DSL or JS/TS, in the scaffolded module directory. Modify the microservice config including which namespace/functions of this project are to be exposed as REST/Event endpoints, with query validations. Start the Godspeed service specifying the path to the project folder.","content_length":643,"content_tokens":137,"embedding":[]}]},{"title":"Hello from Godspeed Docs | Godspeed Docsdocu_tree","url":"https://docs.godspeed.systems/","date":"2023-05-09T19:48:30.922Z","content":"","tokens":0,"length":0,"chunks":[{"doc_title":"Hello from Godspeed Docs | Godspeed Docsdocu_tree","doc_url":"https://docs.godspeed.systems/","doc_date":"2023-05-09T19:48:30.922Z","content":"","content_length":0,"content_tokens":0,"embedding":[]}]}]}
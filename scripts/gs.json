[
  {
    "url": "https://docs.godspeed.systems/docs/table-of-contents",
    "content": "Table of Contents. . Table of Contents 1. Preface 1.1 Introduction 1.2 Goals 1.3 Features 1.4 Tenets 1.5 Design principals 1.6 Framework architecture 1.7 Scenarios and use cases 2. Introduction 2.1 Developer's work 3. Setup 3.1 Getting started 3.1.1 Glossary 3.1.2 Pre-requisites 3.1.3 Steps to get started 3.1.4 Time to start the development 3.2 Project structure 3.2.1 Scaffolding & Project structure 3.3 Configuration 3.3.1 Introduction 3.3.2 Environment variables 3.3.3 Static variables 3.4 Tests 3.5 Auto watch and build 4. CLI 4.1 Functionality 4.2 Installation 4.3 Options 4.4 Commands: Outside the dev container 4.5 Commands: Inside the dev container 5. Swagger Specs 5.1 CLI command to generate documentation 5.2 Custom Server URL 6. Events 6.1 Event types 6.2 Event schema & examples for supported sources 6.2.1 JSON schema validation 6.2.2 HTTP event 6.2.3 Kafka event 7. Workflows 7.1 The structure of workflows 7.2 The tasks within workflows 7.3 Location and fully qualified name (id) of workflows and functions 7.4 Referencing a workflow within an event or another workflow 7.5 Use of Coffee/JS for scripting 7.6 Inbuilt functions 7.6.1 com.gs.http 7.6.2 com.gs.kafka 7.6.3 com.gs.datastore 7.6.4 com.gs.elasticgraph 7.6.5 com.gs.transform 7.6.6 com.gs.series 7.6.7 com.gs.parallel 7.6.8 com.gs.switch 7.6.9 com.gs.each_sequential 7.6.10 com.gs.each_parallel 7.6.11 com.gs.return 7.6.12 com.gs.log 7.6.13 com.gs.dynamic_fn 7.6.14 com.gs.aws 7.6.15 com.gs.redis 7.6.16 com.gs.if, com.gs.elif, com.gs.else 7.7 Developer written functions 7.8 Headers defined at workflow level 7.9 File Upload feature 7.9.1 Workflow spec to upload files with same file key 7.9.2 Workflow spec to upload multiple files with different file keys 7.9.3 Workflow spec to upload file directly from URL 8. Datasources 8.1 Introduction 8.1.1 Datasource types 8.2 API datasource 8.2.1 API datasource schema defined externally 8.2.2 API datasource schema defined within the yaml file 8.2.3 Headers defined at datasource level 8.2.4 Headers defined at task level 8.2.5 Example usage 8.3 Datastore as datasource 8.3.1 Schema specification 8.3.2 CLI Commands 8.3.3 Prisma Datastore Setup 8.3.4 Auto generating CRUD APIs from data store models 8.3.5 Sample datastore CRUD task 8.4 Kafka as datasource 8.4.1 Example spec 8.5 Elasticgraph as datasource 8.5.1 Folder Structure 8.5.2 Datasource DSL 8.5.3 Configuration files for elasticgraph 8.5.4 Elasticgraph Setup 8.5.5 Auto generating CRUD APIs for elasticgraph 8.6 Extensible datasources 8.6.1 Datasource definition 8.6.2 Example spec for the event 8.6.3 Example spec for the workflow 8.7 AWS as datasource 8.7.1 Example spec 8.7.2 com.gs.aws workflow 8.8 Redis as datasource 8.8.1 Example spec 9. Caching 9.1 Specifications 9.1.1 Datasource spec for redis 9.1.2 Configuration 9.1.3 Workflow spec 10. Mappings 10.1 Project structure 10.2 Sample mappings 10.3 Use mappings constants in other mapping files 11. Plugins 11.1 Project structure 11.2 Sample plugins 11.3 Sample workflow using plugins 12. Authentication & Authorization 12.1 Authentication 12.1.1 JWT Configuration 12.1.2 Event spec 12.1.3 Generate JWT 12.1.4 Datasource authentication 12.2 Authorization 12.2.1 Workflow DSL 12.2.2 Sample DB query call authorization 13. Telemetry 13.1 Introduction 13.1.1 Architecture 13.2 Goals 13.3 Configuration 13.3.1 OTEL exporter endpoint 13.3.2 OTEL service name 13.3.3 Logging 13.3.3.1 Log level 13.3.3.2 Log fields masking 13.3.3.3 Log format 13.3.3.4 Add custom identifiers in logs 13.4 Custom metrics, traces and logs (BPM) 13.4.1 DSL spec for custom metrics 13.4.2 DSL spec for custom trace 13.4.3 DSL spec for custom logs 13.5 Observability Stack 13.6 Recommended model for telemetry signals 14. Custom Middleware 14.1 How to add custom middleware in Godspeed 15. Roadmap 16. FAQ 16.1 What is the learning curve of the microservice framework? 16.2 What is the development process and quality metrics? 16.3 How can we adopt new versions of used technology easily and fast? For example, the new Postgres release. 16.4 How easy is it to add new technology in place of an existing one, or add something absolutely new and unique (not existing in the framework) ? 16.5 Which databases are currently supported? What is the roadmap for future support? 16.6 Does the API handle DB transactions? 16.7 How can apps be decoupled or loosely coupled with DBs? 16.8 When using Godspeed service alongside SpringBoot, what will be the impact on performance with another hop, versus direct connection with DB from Spring Boot? 16.9 What is the strategic advantage of making DB queries through Godspeed? 16.10 How to achieve multi-tenancy in DBs, for a single application? 16.11 How can we start adopting the Godspeed framework? 16.12 How to move out of the Godspeed framework? Can we have a two door exit? I.e. Can we move out of technology and data both? 16.13 How will we prevent unified CRUD API from limiting or choking us? 16.14 What kind of API standards does the framework support? 16.15 Why Rest first approach ? Why not Graphql first approach? 16.16 How are we doing testing given there is quite a bit of custom DSL in the framework. How do we ensure the correctness? 16.17 How will the upgrades and migrations be done to the framework? 16.18 How CRUD APIs will support the paid as well as the non paid features of databases such as MongoDB. For example: MongoDB free vs paid versions will support different features. 16.19 How to ship new models easily?",
    "title": "Table of Contents. . Table of Contents",
    "tokens": 2237,
    "length": 6141
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface",
    "content": "GodSpeed – A Microservice framework. . GodSpeed – A Microservice framework This document is intended for stakeholders, tech leaders, architects & developers. It will provide high level goals, tenets, design principles, components & features of the platform for the intended audience.",
    "title": "GodSpeed – A Microservice framework. . GodSpeed – A Microservice framework",
    "tokens": 37,
    "length": 208
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#11-introduction",
    "content": "GodSpeed – A Microservice framework. . 1.1 Introduction ​ Godspeed is aimed at empowering teams to develop, maintain and observe microservices based backends, with high velocity, scalability, quality and performance. We want development (and hence also QA) teams to bypass all the repeatable and reusable work involved in building modern distributed backends with domain driven design, multi-tenancy, microservices and serverless functions. We want the developers to be able to speedily develop microservices in days, instead of months.For the same, we are trying to provide everything that a team needs to create and operate modern microservices. It will be configuration/templating driven, plug & play, extensible by nature and cloud independent. There will be no vendor lock-in, either with Godspeed or any vendor used. It will give developers choice and control over the kind of tools, DBs and cloud providers they wish to use, while following standards and unified interfaces.This framework is being systematically developed by Mindgrep over the last years, across various projects by extracting abstractions and reusable components. It is actively being customized/expanded/improved with new adaptations.",
    "title": "GodSpeed – A Microservice framework. . 1.1 Introduction ​",
    "tokens": 229,
    "length": 1154
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#12-goals",
    "content": "GodSpeed – A Microservice framework. . 1.2 Goals ​ THE GOALS OF THE FRAMEWORK ARE AIMED TO MAKE BUSINESS AGILE BY EMPOWERING THE PRODUCT & DEVELOPMENT TEAMS TO DELIVER EXCELLENT SOLUTIONS VERY FAST.",
    "title": "GodSpeed – A Microservice framework. . 1.2 Goals ​",
    "tokens": 46,
    "length": 147
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#developer-friendly",
    "content": "GodSpeed – A Microservice framework. 1.2 Goals ​. Developer friendly ​ Godspeed provides low code implementation, YAML based DSL, prebuilt feature set and easy project setup, making like of developers easy. Thus empowering them to focus and accomplish their core work with the least amount of effort, time & cost.",
    "title": "GodSpeed – A Microservice framework. 1.2 Goals ​. Developer friendly ​",
    "tokens": 49,
    "length": 242
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#enhancing-developer-productivity",
    "content": "GodSpeed – A Microservice framework. 1.2 Goals ​. Enhancing developer productivity ​ The framework provides fundamental functionalities of “a modern microservice” out of the box so that developer only needs to focus on business logic (80% reduction in work).",
    "title": "GodSpeed – A Microservice framework. 1.2 Goals ​. Enhancing developer productivity ​",
    "tokens": 36,
    "length": 173
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#smaller-micro-teams-and-lesser-learning-curve",
    "content": "GodSpeed – A Microservice framework. 1.2 Goals ​. Smaller, micro teams and lesser learning curve ​ Module owners can start shipping microservices within a week's ramp-up time. If at all, only a couple of members in the ogranization need to know the nitty gritty. Rest can just train to use the framework, and deliver with their help,or ours.",
    "title": "GodSpeed – A Microservice framework. 1.2 Goals ​. Smaller, micro teams and lesser learning curve ​",
    "tokens": 56,
    "length": 242
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#security",
    "content": "GodSpeed – A Microservice framework. 1.2 Goals ​. Security ​ The framework can read the environmental variables from a secure source like K8s Vault. For data in transit and data at rest, we use encryption mechanisms. Also, the framework supports JWT Authentication. Further, all hits to other APIs are secured via security schemas specified in their Open API Specification (OAS 3). Fine grained authorization at API and datasources level is in the roadmap. Read more",
    "title": "GodSpeed – A Microservice framework. 1.2 Goals ​. Security ​",
    "tokens": 83,
    "length": 406
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#easy-and-fast-migrations",
    "content": "GodSpeed – A Microservice framework. 1.2 Goals ​. Easy and fast migrations ​ Migrate existing data models to Godspeed via database introspection. Autogenerate CRUD APIs based on the data models. Migrate existing API based on its introspection, to create Godspeed compliant events - planned. Now, all that remains for developers, is simply to migrate the business logic.",
    "title": "GodSpeed – A Microservice framework. 1.2 Goals ​. Easy and fast migrations ​",
    "tokens": 61,
    "length": 292
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#13-features",
    "content": "GodSpeed – A Microservice framework. . 1.3 Features ​",
    "title": "GodSpeed – A Microservice framework. . 1.3 Features ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#14-tenets",
    "content": "GodSpeed – A Microservice framework. . 1.4 Tenets ​",
    "title": "GodSpeed – A Microservice framework. . 1.4 Tenets ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#dont-repeat-yourself",
    "content": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Don't repeat yourself ​ Developer does not need to do anything at the levels lower than the schema (events, datasources) and business logic. All that, including project setup with required docker containers, is handled by the framework. The developers need not to repeat any work from api to api or project to project.",
    "title": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Don't repeat yourself ​",
    "tokens": 58,
    "length": 294
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#easy-to-extend--customize",
    "content": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Easy to extend & customize ​ Pluggable interfaces allow new integrations without changing code. For example, replacing datastores, APM/BPM tools, analytics engines, cache, email provider, file storage, CRM etc. should ideally require no change in the application code.",
    "title": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Easy to extend & customize ​",
    "tokens": 52,
    "length": 239
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#standards-driven",
    "content": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Standards driven ​ Use standards in designing the system. For example, events using CouldEvents. Observability using OpenTelemetry.",
    "title": "GodSpeed – A Microservice framework. 1.4 Tenets ​. Standards driven ​",
    "tokens": 22,
    "length": 112
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#15-design-principals",
    "content": "GodSpeed – A Microservice framework. . 1.5 Design principals ​",
    "title": "GodSpeed – A Microservice framework. . 1.5 Design principals ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#three-fundamental-abstractions",
    "content": "GodSpeed – A Microservice framework. 1.5 Design principals ​. Three fundamental abstractions ​ The three fundamental abstractions in the Godspeed are events (sync/async), workflows (business logic) and datasources (APIs/datastores). Read more",
    "title": "GodSpeed – A Microservice framework. 1.5 Design principals ​. Three fundamental abstractions ​",
    "tokens": 37,
    "length": 148
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#unified-observability-for-apm-and-bpm",
    "content": "GodSpeed – A Microservice framework. 1.5 Design principals ​. Unified Observability For APM and BPM ​ We will follow OpenTelemetry (OTEL) SDKs to collect and observe telemetry data, including application performance monitoring. This will be integrable with a plethora of open source or commercial tools of choice that integrate with the standard OTEL protocol. Read more",
    "title": "GodSpeed – A Microservice framework. 1.5 Design principals ​. Unified Observability For APM and BPM ​",
    "tokens": 56,
    "length": 271
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#16-framework-architecture",
    "content": "GodSpeed – A Microservice framework. . 1.6 Framework architecture ​ The three main dimensions of Godspeed framework: events, workflows and datasources.",
    "title": "GodSpeed – A Microservice framework. . 1.6 Framework architecture ​",
    "tokens": 17,
    "length": 83
  },
  {
    "url": "https://docs.godspeed.systems/docs/preface#17-scenarios-and-use-cases",
    "content": "GodSpeed – A Microservice framework. . 1.7 Scenarios and use cases ​ Use cases include any kind of microservice, CRUD microservice, wrapper service, search and suggest service, backend for frontend service, orchestration service, domain gateway service, etc.",
    "title": "GodSpeed – A Microservice framework. . 1.7 Scenarios and use cases ​",
    "tokens": 38,
    "length": 189
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/intro",
    "content": "Introduction. . Introduction Every microservice in the Godspeed framework has three fundamental abstractions, and the developer needs to work with just these three. Events : Events trigger workflows. Events are generated by event sources like REST endpoints, gRPC, message bus, webhooks, websockets, S3, and more. Workflows : Workflows are triggered by events. They not only perform business logic but also provide orchestration over datasources and microservices, and data/API federation. They will use datasources to store or retrieve data, join across various datasources, transform data, emit events and send responses. The framework provides a YAML dsl with some inbuilt workflows . If YAML does not suffice for any particular case, developers can currently put JS/TS workflows alongside YAML workflows and use them. Coming in future : Support for other languages. Datasources : Datasources are locations where data can be stored or read from. For example API datasource (another microservice or third party), datastores (RDBMS, document, key-value), file system, S3 storage, etc. A microservice can use multiple datasources. The framework provides abstractions for Authn/Authz making it easy for the developer to express the same in a low code manner. These abstractions allow the developer to focus purely on their business logic. 99.9% - 100% of typical functionality needed by the developer is covered by the framework's YAML-based DSL. Devs can forget about the low-level stuff they typically need to do - which accounts for 90% of the work in typical app dev scenario. The framework aims to handle all the low level functionality and saves developer's effort to do the same. For example creating controllers for endpoints, endpoint authentication/authorization, input validation, auto-telemetry with distributed context, setting up DB client and authorizing DB access, authentication of third party API, key management, creating Swagger docs or Postman collection, creating basic test suite based on documentation, etc.There is a standard project structure which will give the developer a kickstart to their project and also reference code/declarations, for the kind of stuff they can do using the framework.",
    "title": "Introduction. . Introduction",
    "tokens": 469,
    "length": 2202
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/intro#21-developers-work",
    "content": "Introduction. . 2.1 Developer's work ​ The developer will use the CLI provided by the framework to setup a new microservice project and start developing. (S)he will configure the events, datasources, and workflows for the required functionality, along with mappings, environment variables, and common configurations, like for telemetry. To configure the datasources, For datastores: they will either define the db schema or autogenerate it from the existing database using the CLI. For APIs: they will need to define the APIs OpenAPI schema or provide the url for the same.",
    "title": "Introduction. . 2.1 Developer's work ​",
    "tokens": 114,
    "length": 537
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/intro#salient-features",
    "content": "Introduction. 2.1 Developer's work ​. Salient Features ​ Note Some of the features mentioned here are in the product roadmap and planned for upcoming releases. Schema driven development The developer has to specify the API and data schema to start the development. YAML based DSL and configurations We have YAML based DSL which makes it much easier and succinct to express policies, business logic, and configurations. Code is shorter and easier to comprehend than programming, even for new learners. This DSL can be further customized by developers to add custom requirements. Multi datastore support The same model configuration & unified CRUD API (including full-text search and autosuggest) will provide interfaces with multiple kinds of datastores (SQL or NoSQL). The API is aimed to provide validation, relationship management, transactions, denormalization, and multilingual support. Each integration will support the possible functionality as per the nature of the store. Data validation The framework provides validation of third party API requests & responses, datastore queries, and its own API endpoints request and response. The developer only needs to specify the schema of third party API, own microservice API, and datastore model. Rest is taken care of by the framework. In case of more complex validation scenarios, where customer journeys may require conditional validation of incoming requests based on some attributes (in the database or the query {i.e. subject, object, environment, payload}), the developer can add such rules to the application logic as part of the workflows. Authentication The microservice framework authenticates every incoming request and extracts the user role and other info, for further processing, based on a valid JWT token. An IAM provider like ORY Kratos can be integrated into the platform for providing identity service. It will generate a JWT token which will include user id, information, and roles. This token is consumed by the microservices for user validation. Authorization ( Planned ) Each microservice will do the job of authorization for any request. Developers will write authorization rules for every microservice in simple configuration files. This will cover not only API endpoint access but also fine grained data access from datastores. This will integrate with third party Authz services in a pluggable way, with abstractions. Distributed transactions ( Planned ) Each domain’s orchestrator is able to use the Saga pattern to ensure distributed transactions across multiple microservices. Autogenerated documentation The framework provides autogenerated documentation using CLI. Autogenerated CRUD API ( Planned ) The framework provides autogenereated CRUD APIs from database model. Generated API's can be extended by the developers as per their needs. Autogenerated test suite The framework provides autogenerated test suite for APIs using CLI. Multiple languages support In case YAML is not enough for a corner case, developers can write custom business logic in any language. If written in JS/TS, they can place the code within the same microservice project. Other language support will also work in the same way, and is planned for the future. Observability The framework provides automatic observability support with correlation, for modern distributed systems, via the OpenTelemetry spec. For the same, it will work in conjunction with the microservice mesh used. The developer can extend that to include customized observability. This can integrate with any tools that support OpenTelemetry.LoggingThe inbuilt logging mechanism will log both sync request/response cycle or async events, for both success and failure scenarios. MonitoringThe framework allows the developer to monitor custom business metrics, along with application level metrics like latency, success, and failures. TracingEvery incoming sync & async request will carry trace information in its headers. The same is propagated further through the microservice framework when it makes a sync or async hit to another service.",
    "title": "Introduction. 2.1 Developer's work ​. Salient Features ​",
    "tokens": 795,
    "length": 4036
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/setup/getting-started",
    "content": "Getting started. . Getting started Hereby is a step by step guide on running your first project. The setup is independent of the OS you are running it on. info You can also refer to tutorial on Getting Started with Godspeed .",
    "title": "Getting started. . Getting started",
    "tokens": 50,
    "length": 199
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/setup/getting-started#311-glossary",
    "content": "Getting started. . 3.1.1 Glossary ​ gs_service : The framework code version. During this setup, you will be asked to select the version of gs_service. Remote containers/Dev containers : Refer VSCode Remote containers for more information.",
    "title": "Getting started. . 3.1.1 Glossary ​",
    "tokens": 48,
    "length": 207
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/setup/getting-started#312-pre-requisites",
    "content": "Getting started. . 3.1.2 Pre-requisites ​ Please ensure you have the following in your machine NVM, with Node LTS installed (Currently 16+) Visual Studio Code LTS, with the following plugins installed: Remote Containers Run on Save Refer Run On Save Godspeed Extension Pack Docker-desktop should be up and running. On Linux systems, please ensure that docker compose plugin is installed. You can verify it by executing docker compose version command. Refer Install Compose plugin for more information. Git Hardware recommendations RAM: 8GB Hard Disk: SSD tip Depending your setup, you may need to run the above command using administrator privileges On Windows machines, sometimes Docker-desktop doesn't start. Make sure you have WSL installed with Ubuntu 18.04, for Docker to work fine.",
    "title": "Getting started. . 3.1.2 Pre-requisites ​",
    "tokens": 190,
    "length": 786
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/setup/getting-started#313-steps-to-get-started",
    "content": "Getting started. . 3.1.3 Steps to get started ​ Step1: Install the Godspeed CLI ​ npm install -g @mindgrep/godspeed Step 2: Setting up a project on your local machine ​ note If you are creating a new project then follow section 2.1 OR If you are setting up a project from any existing git repository then follow section 2.2 2.1 Create a new project ​ godspeed create my_test_project During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements. By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. 2.2 Setting up a project from an existing GIT repository ​ Clone the git repository on your local machine. cd <your git repo> godspeed update During the setup, you will be asked which datastores you need. Also whether you need Kafka. Say yes or no, depending on your requirements. By default, latest version is selected for gs_service. You should select either latest or any highest semantic version available in the list. Step3: cd to your project ​ cd <your project directory> Step4: Start Visual Studio from the project directory ​ code . Step 5: Open in Dev container ​ Again click on the dev container tray icon. If this is your first time, click on Open folder in Dev Container . Else for every other time, click on Re-open in Dev Container Step 6: Building the project ​ godspeed build Step 7: Start the service for local development in watch mode ​ godspeed dev tip With the dev container running, we have auto watch and auto build enabled when you make changes to your project files. You don't need to run build manually everytime you make changes.",
    "title": "Getting started. . 3.1.3 Steps to get started ​",
    "tokens": 610,
    "length": 1915
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/setup/getting-started#314-time-to-start-the-development",
    "content": "Getting started. . 3.1.4 Time to start the development ​ If you have successfully reached here, then it is time to start the development of your project!",
    "title": "Getting started. . 3.1.4 Time to start the development ​",
    "tokens": 19,
    "length": 96
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli",
    "content": "Godspeed CLI. . Godspeed CLI The CLI is the primary way to interact with your Godspeed project from the command line. It provides a bunch of useful functionalities during the project development lifecycle.",
    "title": "Godspeed CLI. . Godspeed CLI",
    "tokens": 33,
    "length": 176
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#41-functionality",
    "content": "Godspeed CLI. . 4.1 Functionality ​",
    "title": "Godspeed CLI. . 4.1 Functionality ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#outside-the-dev-container",
    "content": "Godspeed CLI. 4.1 Functionality ​. Outside the dev container ​ Creating a new project environment with dev container setup, which includes the folder structure, all the databases, message bus, cache, etc. Open up an existing project in the dev container, add/update a container in the dev environment, based on updated settings. List the versions of gs_service. Change the version of gs_service.",
    "title": "Godspeed CLI. 4.1 Functionality ​. Outside the dev container ​",
    "tokens": 73,
    "length": 335
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#inside-the-dev-container",
    "content": "Godspeed CLI. 4.1 Functionality ​. Inside the dev container ​ All Prisma commands including DB push, pull or migration. OAS 3 documentation file generation. Test suite/Postman collection generation. Running test suite.",
    "title": "Godspeed CLI. 4.1 Functionality ​. Inside the dev container ​",
    "tokens": 34,
    "length": 159
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#42-installation",
    "content": "Godspeed CLI. . 4.2 Installation ​ npm install -g @mindgrep/godspeed Once Godspeed CLI is installed, the godspeed command can be called from command line. When called without arguments, it displays its help and command usage. $ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command",
    "title": "Godspeed CLI. . 4.2 Installation ​",
    "tokens": 618,
    "length": 1156
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#43-options",
    "content": "Godspeed CLI. . 4.3 Options ​",
    "title": "Godspeed CLI. . 4.3 Options ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#--version--v",
    "content": "Godspeed CLI. 4.3 Options ​. --version (-v) ​ The --version option outputs information about your current godspeed version. $ godspeed -v _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| 0.0.26",
    "title": "Godspeed CLI. 4.3 Options ​. --version (-v) ​",
    "tokens": 324,
    "length": 503
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#--help--h",
    "content": "Godspeed CLI. 4.3 Options ​. --help (-h) ​ The --help option displays help and command usage. $ godspeed _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed [options] [command] Options: -v, --version output the version number -h, --help display help for command Commands: create [options] <projectName> versions List all the available versions of gs_service prepare prepare the containers, before launch or after cleaning the containers version <version> help [command] display help for command",
    "title": "Godspeed CLI. 4.3 Options ​. --help (-h) ​",
    "tokens": 563,
    "length": 994
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#44-commands-outside-the-dev-container",
    "content": "Godspeed CLI. . 4.4 Commands: Outside the dev container ​",
    "title": "Godspeed CLI. . 4.4 Commands: Outside the dev container ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#create",
    "content": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. create ​ The create command creates project structure for any microservice. When called without arguments, it creates project structure with examples. $ godspeed create my_service _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| projectDir: /home/gurjot/cli-test/my_service projectTemplateDir undefined project created Do you need mongodb? [y/n] [default: n] n Do you need postgresdb? [y/n] [default: n] y Please enter name of the postgres database [default: test] Do you need kafka? [y/n] [default: n] n Do you need elastisearch? [y/n] [default: n] n Please enter host port on which you want to run your service [default: 3000] 3100 Fetching release version information. Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13 Enter your version [default: latest] 1.0.13 Selected version 1.0.13 . . . . . . . . Options ​ $ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <existing_project_directory> existing project template dir -h, --help display help for command",
    "title": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. create ​",
    "tokens": 1202,
    "length": 2200
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#update",
    "content": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. update ​ The update can be executed in the following cases: If you want to launch an existing project (i.e. copied from local/cloned from repo) instead of creating a new one, then execute godspeed update command before launching the project. If you want to reloads the containers with updated project settings. For example, if you have not selected any database during the project creation and you want to include any database in the project later on, then execute godspeed update with the required settings. If there is any change in gs_service image of standard tags (e.g. latest, stable) and you want to fetch the latest code for the same tag, then execute godspeed update command. It fetches the new docker image itself. Please note that the command should be executed from inside the project root directory. note Whenever you update your project using godspeed update and open the project in VScode dev container after update, then it is mandatory to do godspeed build inside dev container for the first time. $ godspeed update _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Do you need postgresdb? [y/n] [default: n] Do you need kafka? [y/n] [default: n] Do you need elastisearch? [y/n] [default: n] Please enter host port on which you want to run your service [default: 3000] Fetching release version information. Please select release version of gs_service from the available list: latest 1.0.0 1.0.1 1.0.2 1.0.3 1.0.4 dev stable Enter your version [default: latest] Selected version latest Removing dev_test_devcontainer_node_1 . . . . . . . . . . . Step 1/9 : FROM adminmindgrep/gs_service:latest latest: Pulling from adminmindgrep/gs_service 824b15f81d65: Already exists 325d38bcb229: Already exists d6d638bf61bf: Already exists 55daac95cedf: Already exists 4c701498752d: Already exists a48b0ae49665: Pulling fs layer 4c393fb6deac: Pulling fs layer 4f4fb700ef54: Pulling fs layer 8992963a9530: Pulling fs layer 4f4fb700ef54: Verifying Checksum 4f4fb700ef54: Download complete 4c393fb6deac: Verifying Checksum 4c393fb6deac: Download complete 8992963a9530: Verifying Checksum 8992963a9530: Download complete a48b0ae49665: Verifying Checksum a48b0ae49665: Download complete a48b0ae49665: Pull complete 4c393fb6deac: Pull complete 4f4fb700ef54: Pull complete 8992963a9530: Pull complete Digest: sha256:7195b3c921f1278153c911e6e77cbcfb385a84c435bfcb7b8272ffcf9a3278ee Status: Downloaded newer image for adminmindgrep/gs_service:latest ---> 988917710d1a Step 2/9 : ARG USERNAME=node ---> Running in c70404bb4f3e Removing intermediate container c70404bb4f3e ---> 47a7406b2473 Step 3/9 : ARG USER_UID=1000 ---> Running in 51e68336d8d8 Removing intermediate container 51e68336d8d8 ---> ce913f6898bb Step 4/9 : ARG USER_GID=$USER_UID ---> Running in 7cf1c1f2a3ec Removing intermediate container 7cf1c1f2a3ec ---> 91f045b32e0f Step 5/9 : USER root ---> Running in f338d755a032 Removing intermediate container f338d755a032 ---> fa9898eb4c23 Step 6/9 : RUN sudo groupmod --gid $USER_GID $USERNAME && usermod --uid $USER_UID --gid $USER_GID $USERNAME && chown -R $USER_UID:$USER_GID /workspace/development ---> Running in eba3659fb919 Removing intermediate container eba3659fb919 ---> 414f34560b0d Step 7/9 : USER node ---> Running in 23818c5f4882 Removing intermediate container 23818c5f4882 ---> 1bd65323ae91 Step 8/9 : RUN sudo npm i -g @mindgrep/godspeed ---> Running in a66cb062390d . . . . . . . . . . godspeed update dev_test is done.",
    "title": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. update ​",
    "tokens": 1773,
    "length": 4215
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#versions",
    "content": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. versions ​ The versions command lists all the versions available of gs_service. $ godspeed versions _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| latest 1.0.0 1.0.1 1.0.10 1.0.11 1.0.12 1.0.13 1.0.2 1.0.3 1.0.4 1.0.5 1.0.6 1.0.7 1.0.8 1.0.9 base dev v1.0.13",
    "title": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. versions ​",
    "tokens": 483,
    "length": 690
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#version",
    "content": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. version ​ The version command helps to change the version of gs_service for any microservice. Execute the command from inside the project root directory. $ godspeed version 1.0.13 _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Generating prisma modules Starting test1_devcontainer_postgres_1 . Starting test1_devcontainer_postgres_1 . done Creating test1_devcontainer_node_run . Creating test1_devcontainer_node_run . done Environment variables loaded from .env . . . . . . . . . .",
    "title": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. version ​",
    "tokens": 444,
    "length": 873
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#help",
    "content": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. help ​ The help command displays help and usage for any command. $ godspeed help create _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Usage: godspeed create [options] <projectName> Options: -n, --noexamples create blank project without examples -d, --directory <projectTemplateDir> local project template dir -h, --help display help for command",
    "title": "Godspeed CLI. 4.4 Commands: Outside the dev container ​. help ​",
    "tokens": 447,
    "length": 777
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#45-commands-inside-the-dev-container",
    "content": "Godspeed CLI. . 4.5 Commands: Inside the dev container ​",
    "title": "Godspeed CLI. . 4.5 Commands: Inside the dev container ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#prisma",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. prisma ​ You can run all the prisma commands in your project root directory inside the dev container. This command is useful for db migration and introspection. Read more here . $ godspeed prisma <prisma command with args>",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. prisma ​",
    "tokens": 51,
    "length": 219
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#build",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. build ​ You can build the complete project using this command. It is the first command which you need to run whenever you open your project in VScode Dev container. Refer Open in Dev container godspeed build",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. build ​",
    "tokens": 46,
    "length": 205
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#dev",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. dev ​ You can run your project using dev command. godspeed dev",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. dev ​",
    "tokens": 16,
    "length": 60
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#gen-api-docs",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-api-docs ​ You can get OAS 3 documentation generated automatically by executing this command in your project root directory inside the dev container. $ godspeed gen-api-docs _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-api-docs > node ./gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529346164] INFO (GS-logger/7684 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529346190] DEBUG (GS-logger/7684 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529346289] INFO (GS-logger/7684 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved!",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-api-docs ​",
    "tokens": 731,
    "length": 1581
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#gen-test-suite",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-test-suite ​ You can get test suite/postman collection generated automatically by executing this command in your project root directory inside the dev container. Now, you can import the collection in postman directly. godspeed gen-test-suite _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 gen-test-suite > npm run gen-api-docs && mkdir -p tests && openapi2postmanv2 -s docs/api-doc.yaml -o tests/test-suite.json -p -O folderStrategy=Tags,includeAuthInfoInExample=false > proj_upd@1.0.0 gen-api-docs > node ./gs_service/dist/api-specs/api-spec.js | pino-pretty [1657529443249] INFO (GS-logger/8145 on 4c20ee3c4c38): Loading events from /workspace/development/app/src/events [1657529443273] DEBUG (GS-logger/8145 on 4c20ee3c4c38): parsing files: /workspace/development/app/src/events/call_another_workflow.yaml,/workspace/development/app/src/events/create_user_then_show_all.yaml,/workspace/development/app/src/events/cross_db_join.yaml,/workspace/development/app/src/events/document.yaml,/workspace/development/app/src/events/helloworld.yaml,/workspace/development/app/src/events/httpbin_anything_coffee.yaml,/workspace/development/app/src/events/httpbin_anything.yaml,/workspace/development/app/src/events/run_tasks_in_parallel.yaml,/workspace/development/app/src/events/sum.yaml,/workspace/development/app/src/events/switch_case.yaml [1657529443374] INFO (GS-logger/8145 on 4c20ee3c4c38): /workspace/development/app/docs/api-doc.yaml file is saved! Input file: /workspace/development/app/docs/api-doc.yaml Writing to file: true /workspace/development/app/tests/test-suite.json { result: true, output: [ { type: 'collection', data: [Object] } ] } Conversion successful, collection written to file",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-test-suite ​",
    "tokens": 932,
    "length": 2131
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#gen-crud-api",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-crud-api ​ You can get CRUD API generated automatically for datastores and elasticgraph datasources by executing this command in your project root directory inside the dev container. $ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs (x) elasticgraph.yaml ( ) For all ( ) Cancel",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. gen-crud-api ​",
    "tokens": 432,
    "length": 794
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#test",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. test ​ You can run the test suite generated in above command from the following two ways: Postman: Import the collection in postman and run the test suite. CLI: You can use below command to run the test suite from CLI. Please make sure your service is up and running before running the test suite. godspeed test _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > proj_upd@1.0.0 test > newman run tests/test-suite.json newman Godspeed: Sample Microservice → Call another (sub) workflow from main workflow POST http://localhost:3000/another_workflow?bank_id=<string> [200 OK, 630B, 2.6s] . . . . . . . .",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. test ​",
    "tokens": 507,
    "length": 1010
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/introduction-cli#help-1",
    "content": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. help ​ The help command displays help and usage for any command. Click here to know more",
    "title": "Godspeed CLI. 4.5 Commands: Inside the dev container ​. help ​",
    "tokens": 17,
    "length": 82
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/swagger-specs",
    "content": "Introduction. . Introduction You can access autogenerated Swagger API specifications at <domain name>/api-docs url. For example, http://localhost:3000/api-docs Godspeed also provides a facility to auto-generate OAS 3 documentation using CLI.",
    "title": "Introduction. . Introduction",
    "tokens": 82,
    "length": 241
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/swagger-specs#51-cli-command-to-generate-documentation",
    "content": "Introduction. . 5.1 CLI command to generate documentation ​ You can generate OAS3 documentation using godspeed gen-api-docs CLI command.",
    "title": "Introduction. . 5.1 CLI command to generate documentation ​",
    "tokens": 22,
    "length": 80
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/swagger-specs#52-custom-server-url",
    "content": "Introduction. . 5.2 Custom Server URL ​ You can add custom server URL for API documentation in static configuration By adding the custom server url, your autogenerated documentation or swagger specs will have this url set in the Servers . server_url: https://api.example.com:8443/v1/api For example,",
    "title": "Introduction. . 5.2 Custom Server URL ​",
    "tokens": 89,
    "length": 286
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events",
    "content": "Events. . Events A microservice can be configured to consume events from variety of event sources , like HTTP, gRpc, GraphQl, S3 etc. The event schema, for each event source, closely follows the OpenAPI specification. It includes The name/topic/URL of the event The event source and other information for the source (for ex. group_id in case of Kafka events) The event handler workflow Validation (input and output) Examples of input and output The response of the event is flexible for the developer to change as per the requirement.",
    "title": "Events. . Events",
    "tokens": 117,
    "length": 522
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#61-event-types",
    "content": "Events. . 6.1 Event types ​ Currently supported http.{method_type} For example, post or get Kafka salesforce cron Planned Webhook S3 gRPC GraphQL Websocket",
    "title": "Events. . 6.1 Event types ​",
    "tokens": 42,
    "length": 137
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#62-event-schema--examples-for-supported-sources",
    "content": "Events. . 6.2 Event schema & examples for supported sources ​ All event declarations are stored in the src/events folder, in YAML files.",
    "title": "Events. . 6.2 Event schema & examples for supported sources ​",
    "tokens": 18,
    "length": 74
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#621-json-schema-validation",
    "content": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.1 JSON schema validation ​ The framework provides request and response schema validation out of the box.Request schema validation ​ Sample spec for request schema. body: content: application/json: schema: type: 'object' required: [] properties: dob: type: 'string' format : 'date' pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" If request schema validation fails, then status code 400 is returned.Response schema validation ​ Sample spec for response schema. responses: #Output data defined as per the OpenAPI spec 200: description: content: application/json: # For ex. application/json application/xml schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id] examples: # <string, ExampleObject> example1: summary: description: value: application_id: PRM20478956N external_value: If response schema validation fails, then status code 500 is returned.",
    "title": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.1 JSON schema validation ​",
    "tokens": 648,
    "length": 1315
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#622-http-event",
    "content": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.2 HTTP event ​ For an HTTP event, the headers, query, params and body data are captured in a standard format, and made available in the inputs object for use in the workflows . The inputs (event) object has following properties: - query: `<%inputs.query.var_name%>` # present in case of http events - params: `<%inputs.params.path_param%>` # present in case of http events - headers: `<%inputs.headers.some_header_key%>` # present in case of http events - body: `<%inputs.body.key%>` # Present for all events except for http events which don't have a body. For ex. http.get - files: `<%input.files%>` # Any files uploaded via HTTP event. Not present in other kind of events Example spec for HTTP event ​ /v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post : #Adding .http.post after #the endpoint exposes the endpoint as REST via the POST method (in this example) fn : com.biz.kyc.ckyc.ckyc_initiate #The event handler written in ckyc_initiate.yml, and # kept in src/workflows/com/biz/kyc/ckyc folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs/ folder (in this example) body : required : true content : application/json : schema : type : 'object' required : [ ] properties : dob : { type : 'string' , format : 'date' , pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta : type : 'object' params : - name : lender_loan_application_id in : params # same as open api spec: one of cookie, path, query, header required : true allow_empty_value : false schema : type : string responses : #Output data defined as per the OpenAPI spec 200 : description : required : # default value is false content : application/json : # For ex. application/json application/xml schema : type : object properties : application_id : type : string additionalProperties : false required : [ application_id ] examples : # <string, ExampleObject> example1 : summary : description : value : application_id : PRM20478956N external_value : encoding : 400 : description : required : # default value is false content : application/json : # For ex. application/json application/xml schema : type : object properties : lender_response_code : type : string examples : # <string, ExampleObject> example1 : summary : description : value : lender_response_code : E001 external_value : encoding : Example workflow consuming an HTTP event ​ summary : Simply returning query & body data of an http.post event id : some_unique_id tasks : - id : step1 fn : com.gs.return args : <%inputs.body% > # Evaluation of dynamic values happens via <% %>. The type of scripting can be coffee/js. # Here we are returning the body of the HTTP post event. Example workflow (on_validation_error handler) handling json schema validation error ​ summary : Handle json scehma validation error id : error_handler tasks : - id : erorr_step1 fn : com.gs.kafka args : datasource : kafka1 data : # publish the event and validation error to kafka on a topic value : event : <% inputs.event % > validation_error : <% inputs.validation_error % > config : topic : kafka_error_handle method : publish",
    "title": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.2 HTTP event ​",
    "tokens": 2613,
    "length": 4931
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#623-kafka-event",
    "content": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.3 Kafka event ​ A kafka event is specified as {topic_name}.{datasourceName}.{group_id} in the kafka event specification . The group_id represents identifier for all the consumers of the group. Only one consumer of the group will consume a message. This is useful for microservices, when a single services runs in multiple K8s pods. Each pod is part of the same group. This ensures the message is eventually consumed by any one of the pods.The message body of a kafka event is captured and represented as inputs.body for consumption in the handler workflow .Datasource for kafka ​ The datasources for kafka are defined in src/datasources . Refer Kafka as datasource for more information.Example spec for kafka event ​ kafka-consumer1.kafka1.kafka_proj : # This event will be triggered whenever # a new message arrives on the topic_name id : /kafkaWebhook fn : com.jfs.publish_kafka #The event handler written in publish_kafka.yml, and # kept in src/workflows/com/jfs folder (in this example) on_validation_error : com.jfs.handle_validation_error # The validation error handler if event's json schema validation gets failed and # kept in src/workflows/com/jfs folder (in this example) body : description : The body of the query content : application/json : # For ex. application/json application/xml schema : type : object properties : name : type : string required : [ name ] Example workflow consuming a kafka event ​ summary : Handle kafka event id : some_unique_id tasks : - id : step1 summary : Publish an event with this data fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : publish - producer1 data : value : <% inputs % > # Here we are publishing an event data to another topic Refer com.gs.kafka native function to publish an event on kafka.",
    "title": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.3 Kafka event ​",
    "tokens": 1028,
    "length": 2356
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#624-salesforce-event",
    "content": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.4 Salesforce event ​ A salesforce event is specified as {topic_name}.salesforce.{datasource_name} topic_name is salaesforce event topic datasource_name is name of the salesforce datasource filenamePrerequisite: For using salesforce , You need to enable redis datasource. You can enable redis while creating a new godspeed project or run godspeed update on an existing project. in config/default.yaml add a property as caching: redis . Where redis is datasource name. If your redis type datasource name is redis1.yaml , then caching: redis1 will be the correct configuration. Example of salesforce datasource, eg: src/datasources/salesforce.yaml type : salesforce connection : # Please Check https:#jsforce.github.io/document/ #1. Username and Password Login # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com' #2. Username and Password Login (OAuth2 Resource Owner Password Credential) oauth2 : # you can change loginUrl to connect to sandbox or prerelease env. # loginUrl : 'https:#test.salesforce.com', clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<callback URI is here>' #3. Session ID serverUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' sessionId : '<your Salesforce session ID is here>' #4. Access Token instanceUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' #5. Access Token with Refresh Token oauth2 : clientId : '<your Salesforce OAuth2 client ID is here>' clientSecret : '<your Salesforce OAuth2 client secret is here>' redirectUri : '<your Salesforce OAuth2 redirect URI is here>' instanceUrl : '<your Salesforce server URL (e.g. https:#na1.salesforce.com) is here>' accessToken : '<your Salesforrce OAuth2 access token is here>' refreshToken : '<your Salesforce OAuth2 refresh token is here>' username : <% config.salesforce_username % > password : <% config.salesforce_password % > Example of salesforce event: { topic_name } .salesforce. { datasourceName } id : /salesforcetopic fn : com.jfs.handle_title_events on_validation_error : com.jfs.handle_validation_error body : description : The body of the query content : application/json : schema : type : object properties : name : type : string required : [ name ]",
    "title": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.4 Salesforce event ​",
    "tokens": 1461,
    "length": 3175
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/events#625-cron-event",
    "content": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.5 CRON event ​ A CRON event will allow you to run events at scheduled time / interval. A CRON event is specified as {schedule_expression.cron.timezone} schedule_expression You can refer crontab to generate schedule. timezone Refer this wikipedia to get the timezone format. Here is an example of a CRON event which run every minute. every_minute_cron.yaml \"* * * * *.cron.Asia/Kolkata\" : fn : com.every_minute and corresponding function is com/every_minute.yaml summary : this workflow will be running every minute tasks : - id : print description : print every fn : com.gs.log args : level : info data : HELLO from CRON",
    "title": "Events. 6.2 Event schema & examples for supported sources ​. 6.2.5 CRON event ​",
    "tokens": 336,
    "length": 784
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows",
    "content": "Workflows. . Workflows Workflows is where the actual computation and flow orchestration happens. The framework supports a YAML based DSL to write workflows and tasks containing the business logic. These workflows can be attached to the events as their handlers, or called from within another workflow. The framework exposes CoffeeScript /JS based expressions for evaluation of dynamic variables or transformation of data from inputs of event, or outputs of previous tasks. Default language for transformations (coffee/js) can be configured in configuration",
    "title": "Workflows. . Workflows",
    "tokens": 106,
    "length": 542
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#71-the-structure-of-workflows",
    "content": "Workflows. . 7.1 The structure of workflows ​ A workflow has the following attributes summary - the title description - more details id - Recommended for better logging visibility on_error - Default error handling if any tasks fails. tasks - the tasks (workflows or sub-workflows) to be run in series (sequence, or one by one). The tasks invoke other workflows written in YAML or JS/TS. Other languages support is planned. summary : Hello world description : Hello world example which invokes the com.gs.return workflow id : hello_world # needed for better logging visibility on_error : continue : false response : success : false code : 500 data : \"Default error\" tasks : # tasks to be run in sequence (default is sequence) - id : step1 ## id of this task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below. fn : com.gs.return args : 'Hello World!' # com.gs.return takes its return value as `args`. Hence the args key.",
    "title": "Workflows. . 7.1 The structure of workflows ​",
    "tokens": 434,
    "length": 1149
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#72-the-tasks-within-workflows",
    "content": "Workflows. . 7.2 The tasks within workflows ​ A workflow has one or more tasks associated with it. A task has the following attributes id - Needed for better logging visibility. It is compulsory for a task. Importantly, this is also used to access the output of this task in subsequent tasks in the outputs.{task_id} path, as shown in example below . summary - the title description - more details fn - The handler to be run in this task. It can be one of the framework functions , control functions (like parallel, sequential, switch), developer written functions , or another workflow. You can also use scripting in dynamic evaluation of a function name as given in below example. Refer Coffee/JS scripting for more information. summary : Call an API and transform the tasks : - id : transform_fn_step1 description : find fn name fn : com.gs.transform args : | <js% if (inputs.body.fn == 'sum') { return 'com.jfs.sum_workflow' } else { return 'com.jfs.helloworld' } %> - id : call_fn_step2 description : call fn returned in transform_fn_step1 fn : <% outputs.transform_fn_step1.data % > args : name : <% inputs.body.name % > args - Every handler fn has its own argument structure, which is kept in the args key. For example, id : httpbin_step1 fn : com.gs.http args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post headers : <% inputs.headers % > on_error - What to do if this task fails? on_error : #You can find sample usage of this in the examples below. Just search on_error in this page. continue : false # Whether the next task should be executed, in case this task fails. by default continue is true. response : <%Coffee/JS expression% > | String # If specified, the output of `response` is returned as the output of this task. If not specified, the error output is the default output of the failed task. tasks : # If specified, the tasks are executed in series/sequence. The output of the last task in these tasks is the default output of the failed task. - id : transform_error fn : com.gs.transform args : <% outputs.httpbin_step1 % > - id : publish_error fn : com.gs.kafka args : datasource : kafka1 data : value : <% outputs.transform_error.message % > config : topic : publish - producer1 The only exception to this is control functions like series, parallel, switch, which don't take the args , for the sake of more readability. retry - Retry logic helps to handle transient failures, internal server errors, and network errors with support for constant, exponential and random types. Currently applied only for com.gs.http workflow. retry : max_attempts : 5 type : constant interval : PT15m retry : max_attempts : 5 type : exponential interval : PT15s retry : max_attempts : 5 type : random min_interval : PT5s max_interval : PT10s",
    "title": "Workflows. . 7.2 The tasks within workflows ​",
    "tokens": 1787,
    "length": 3872
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#the-output-of-task--external-function",
    "content": "Workflows. . The output of task & external function ​ The output of every task and function can be expected in the following format within other task success : true/false. Default value is true code : standard HTTP response codes[1xx, 2xx, 3xx, 4xx, 5xx] Default value is 200 message : any string explaining the response. Optional data : the actual data returned from the task/function. Optional Note If a task or external JS function returns a value which is not in this JSON structure then framework assumes the output is the data itself & wraps it in this JSON structure with default values. The output of any previously executed task is accesible in following manner outputs.step1.code Example of multiple task with arguments ​ summary : Workflow with switch - case and transform task id : example_switch_functionality_id description : | Run two tasks in series. Both take different arguments. First one is switch case task. Second is transform task which consumes the output of step1 and shapes the final output of this workflow. tasks : # tasks to be run in sequence (default is sequence) - id : step1_switch ## id of this switch task. Its output will be accessible # to subsequent tasks at `outputs.step1_switch` location. Like in step2 below. fn : com.gs.switch # Switch workflow takes `value` and `cases` as arguments. The cases object specifies another task for every case. value : <%inputs.body.condition% > # Evaluation of dynamic values happens via <% %> cases : FIRST : id : 1st fn : com.gs.return args : \"'case - 1'\" SECOND : id : 2nd fn : com.gs.return args : \"'case - 2'\" THIRD : id : 3rd fn : com.gs.return args : \"'case - 3'\" defaults : id : default fn : com.gs.return args : <%inputs.body.default_return_val% > #coffee/js script for dyanmic evaluation. Wrapped in <% %>. Same as that used elsewhere in workflows for dynamic calculations and variable substitutions. For ex. as used in com.gs.transform and com.gs.return - id : step2 fn : com.gs.transform args : | #coffee for dyanmic evaluation. Wrapped in <% %> <coffee% { code : 200 , data : outputs [ '1st' ] } % >",
    "title": "Workflows. . The output of task & external function ​",
    "tokens": 1102,
    "length": 2627
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#73-location-and-fully-qualified-name-id-of-workflows-and-functions",
    "content": "Workflows. . 7.3 Location and fully qualified name (id) of workflows and functions ​ All the workflows and functions are to be kept in the src/functions folder. Their directory tree path, followed by the file name becomes the workflow's fully qualified name or id, by which it can be referenced in the events or within other workflows. The JS function shown below will be available in workflows under the F.Q.N. com.biz.custom_function . Similarly, com.biz.create_hdfc_account , com.biz.create_parallel etc. are accessible as handlers from within other workflow tasks or events.",
    "title": "Workflows. . 7.3 Location and fully qualified name (id) of workflows and functions ​",
    "tokens": 124,
    "length": 501
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#74-referencing-a-workflow-within-an-event-or-another-workflow",
    "content": "Workflows. . 7.4 Referencing a workflow within an event or another workflow ​ A workflow task references and invokes other workflows written in either YAML or JS/TS, via the fn key. In future, other languages will also be supported. An event definition references the handler yaml workflows by their fully qualified name, via the same fn key.",
    "title": "Workflows. . 7.4 Referencing a workflow within an event or another workflow ​",
    "tokens": 65,
    "length": 270
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#75-use-of-coffeejs-for-scripting",
    "content": "Workflows. . 7.5 Use of Coffee/JS for scripting ​ The framework provides coffee/js for Transformations in com.gs.transform and com.gs.return Dynamic evaluation or workflow or task variables, event variables, datasource variables. You will find its code in <% %> within various examples in this page below.Define language at global level ​ Default language for transformations (coffee/js) is configured in static configuration Define language at workflow level ​ Global configuration for language is overridden by defining specific language inside <coffee/js% %>. For example, - id: httpbinCof_step2 fn: com.gs.transform args: | <coffee% if outputs.httpbinCof_step1.data.json.code == 200 then { code: 200, success: true, data: outputs.httpbinCof_step1.data.json, headers: outputs.httpbinCof_step1.data.headers } else { code: 500, success: false, message: 'error in httpbinCof_step1' } %> - id: step1 # the response of this will be accessible within the parent step key, under the step1 sub key description: upload documents fn: com.gs.http args: datasource: httpbin params: data: | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> Built-in Javascript modules ​ You can use build-in javascript modules in inline scripting. Only synchronous methods of build-in modules are allowed in inline scripting. For example, summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : # fs is used directly in scripting in Body - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject",
    "title": "Workflows. . 7.5 Use of Coffee/JS for scripting ​",
    "tokens": 1091,
    "length": 2289
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#76-inbuilt-functions",
    "content": "Workflows. . 7.6 Inbuilt functions ​ The framework provides the following inbuilt functions7.6.1 com.gs.http ​ Send HTTP events to other APIs in Axios compatible format. Example 1 summary : agreement esign id : agreement_esign tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : agreement esign fn : com.gs.http params : # query params to be sent in the request id : 123 args : datasource : httpbin config : url : /v1/loan - application/<% inputs.params.lender_loan_application_id % > /agreement/esign/initiate method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : true - id : step2 fn : com.gs.transform args : | <%if outputs.step1.data.success then outputs.step1.data else { code: outputs.step1.code, success : false, data: { error_data: outputs.step1.data['error'], uuid: outputs.step1.data.uuid, status_code_error: outputs.step1.data.status_code_error, event: outputs.step1.data.event } }%> Example 2 summary : upload documents id : upload_documents tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload documents fn : com.gs.http args : datasource : httpbin params : data : | <js% { [inputs.body.entity_type + 'id']: inputs.body.entity_id, _.omit(inputs.body, ['entity_type', 'entity_id'])} %> file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M on_error : continue : false response : <%'Some error happened in saving' + inputs.body.entity_type% > - id : step2 fn : com.gs.transform args : <% delete outputs.step1.headers; outputs.step1 % > 7.6.2 com.gs.kafka ​ Publish events on Kafka. summary : Publishing incoming event data to a Kafka topic id : push_to_kafka tasks : - id : step1 summary : Publish an event with input event's data , adding to_process = true fn : com.gs.kafka args : # similar to Axios format datasource : kafka1 config : method : publish topic : kyc_initiate_recieved group_id : kyc_domain data : # Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. value : <% inputs % > # Your message content. Evaluation of dynamic values happens via <% %>. The type of scripting is coffee. key : # Optional - Used for partitioning. partition : # Optional - Which partition to send the message to. timestamp : # Optional - The timestamp of when the message was created. headers : # Optional - Metadata to associate with your message. Refer https://kafka.js.org/docs/producing#message-structure for information on data attributes. 7.6.3 com.gs.datastore ​ The datastore function allows CRUD access to any supported datastore in a format extending Prisma API . summary : Create and read data tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Create entity from REST input data (POST request) fn : com.gs.datastore args : datasource : mongo # Which ds to use. data : <% inputs.body + { extra_field : its_value } % > config : method : <% inputs.params.entity_type % > .create - id : step2 # the response of this will be accessible within the parent step key, under the step1 sub key description : test again fn : com.gs.datastore args : datasource : mongo # Adding this knows which ds/model we are talking about here. config : # Similar approach as Axios method : <% inputs.params.entity_type % > .findMany 7.6.4 com.gs.elasticgraph ​ The elasticgraph function allows CRUD access to elasticsearch datastore . summary : eg tasks : - id : create_entity1 description : create_entity1 fn : com.gs.elasticgraph args : datasource : elasticgraph1 data : index : <% inputs.params.entity_type + 's' % > type : '_doc' body : <% inputs.body % > config : method : index on_error : continue : false 7.6.5 com.gs.transform ​ This function allows to transform data from one format to another using coffee/js scripting. summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : code : 200 data : <% outputs.step1_switch.data % > 7.6.6 com.gs.series ​ control flow function Executes the tasks in series. By default every top level workflow executes its task in series. But when invoking subworkflows if you need, you can explicitly use series workflow. Its syntax is same as parallel. summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.series tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.7 com.gs.parallel ​ control flow function Executes the child tasks in parallel. Syntax is same as com.gs.series summary : Parallel Multiplexing create loan for hdfc api calls tasks : - id : parallel fn : com.gs.parallel tasks : - id : 1st fn : com.gs.return args : | 'parallel task1' - id : 2nd fn : com.gs.return args : | 'parallel task2' - id : 3rd fn : com.gs.return args : | 'parallel task3' - id : step2 fn : com.gs.transform args : | <coffee% { code: 200, data: outputs['1st'] } %> 7.6.8 com.gs.switch ​ control flow function The classic switch-case flow execution The args of switch-flow are value and cases . value takes a coffee/js expression to be evaluated during runtime. Every case has a task associated with it. The task can invoke another function or a workflow. summary : create loan application for lender tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.switch value : <%inputs.headers [ 'lender' ] % > cases : httpbin : - id : 1st fn : com.biz.loan_application.httpbin_create_loan_application args : <%inputs% > 7.6.9 com.gs.each_sequential ​ control flow function The classic for-each flow execution The args is list of values in value field along with associated tasks. For each value in value tasks are executed sequentially. The final output each_sequential is the array of status of the last executed task of each iteration. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > on_error handling You can add on_error at task level as well as at each_sequential loop level.See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the loop else it continues the next tasks. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_sequential_step1 description : for each fn : com.gs.each_sequential value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_sequential_step2 description : return the response fn : com.gs.transform args : <% outputs.each_sequential_step1 % > 7.6.10 com.gs.each_parallel ​ The args is list of values in value field along with associated tasks. For each value in value tasks are executed in parallel. The final output each_parallel is the array of status of the last executed task of each iteration. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > on_error handling You can add on_error at task level as well as at each_parallel loop level.See the below example, If a task gets failed for any task_value then control goes to on_error defined at task level. On continue false, it breaks the execution for the next tasks in tasks for current task_value in value list. For example, in the below workflow, if each_task1 step of task_value 1 gets failed then each_task2 will not get executed on continue false. If all the tasks are failed in loop then the control goes to on_error defined at loop level. note on_error at loop level only gets executed when all the tasks are failed. If even one task gets successful then it won't get executed. summary : For each sample description : Here we transform the response of for loop tasks : - id : each_parallel_step1 description : for each fn : com.gs.each_parallel value : [ 1 , 2 , 3 , 4 ] tasks : - id : each_task1 fn : com.gs.transform args : <% 'each_task1 ' + task_value % > on_error : # on_error at task level continue : false response : <%Coffee/JS expression% > | String - id : each_task2 fn : com.gs.transform args : <% 'each_task2 ' + task_value % > on_error : # on_error at loop level continue : true response : <%Coffee/JS expression% > | String - id : each_parallel_step2 description : return the response fn : com.gs.transform args : <% outputs.each_parallel_step1 % > 7.6.11 com.gs.return ​ return statement The classic return statement It returns from the current function to the function caller. The function stops executing when the return statement is called. summary : Multiplexing create loan for hdfc api calls id : helloworld tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : create account in the bank fn : com.gs.return args : | <coffee% 'Hello ' + inputs.query.name %> 7.6.12 com.gs.log ​ It logs the intermediate inputs/outputs during the workflow execution in pino logging format. The args are level and data . level takes any value from the Pino log levels and data takes a coffee/js expression to be evaluated during runtime or anything (like string, number, etc.) which you want to get logged during the workflow execution. summary : Summing x + y description : Here we sum two hardcoded x and y values. Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.jfs.sum args : x : 1 y : 2 - id : sum_step2 description : log the output in logs fn : com.gs.log args : level : info # log levels: info, debug, error, warn, fatal, silent, trace data : <% outputs.sum_step1 % > - id : sum_step3 description : return the response fn : com.gs.transform args : <% outputs.sum_step1 % > 7.6.13 com.gs.dynamic_fn ​ It executes the workflow whose name is dynamically returned as the output of its task list. The tasks of this function should return a string output which will be the name of the workflow to be executed. Event DSL '/sum.http.get' : fn : com.jfs.sum_dynamic summary : A workflow to sum x and y description : This workflow sums two integers params : - name : x in : query required : true allow_empty_value : false schema : type : string - name : y in : query required : true allow_empty_value : false schema : type : string com.jfs.sum_dynamic.yaml summary : Dynamic function to call com.jfs.sum_workflow.yaml description : This function dynamically is taking workflow name and executing it at the runtime. tasks : - id : sum_dynamic_step1 description : add two numbers fn : com.gs.dynamic_fn tasks : # the tasks should return a string value which will the name of the workflow to be executed. # For example, in below task list, final workflow name will be `com.jfs.sum_workflow` - id : get_wf_name_step1 fn : com.gs.transform args : com.jfs.sum_workflow - id : get_wf_name_step2 # this task is returning a workflow name dynamically fn : com.gs.transform args : <% outputs.get_wf_name_step1.data % > com.jfs.sum_workflow.yaml summary : Summing x + y description : Here we sum two hardcoded x and y values. Feel free to try using API inputs from body or params ! tasks : - id : sum_step1 description : add two numbers fn : com.gs.return args : | <% +inputs.query.x + +inputs.query.y %> 7.6.14 com.gs.aws ​ Interacts with AWS to use its various services and methods. params is the list of params to the AWS service methods. We are using AWS v3 style services. Please refer AWS S3 for AWS S3 methods. summary : upload s3 tasks : - id : step1 description : upload s3 fn : com.gs.aws args : datasource : aws_s3 params : - Bucket : 'godspeedbucket' Key : 'file4.yml' Body : <% fs.createReadStream(inputs.files [ 0 ] .tempFilePath) % > config : service : S3 method : putObject 7.6.15 com.gs.redis ​ Developer can read / write to redis datasource using standard redis client functions. summary : demonstration of redis functions id : accessing_redis tasks : - id : store_value_to_key description : Writing user info in redis with key user fn : com.gs.redis args : config : method : set data : key : user value : Adam - id : retrieve_user_set_in_previous_task description : Retriving user from redis fn : com.gs.redis args : config : method : get data : key : user 7.6.16 com.gs.if, com.gs.elif, com.gs.else ​ control flow function The classic if-else flow execution The args are condition and tasks . condition takes a coffee/js expression to be evaluated during runtime. The tasks can invoke another function or a workflow. summary : Returning hello world tasks : - id : if fn : com.gs.if condition : <% inputs.query.status == 'Hello' % > tasks : - id : step1 description : Return hello world fn : com.gs.return args : 'Hello!' - id : elif1 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hell' % > tasks : - id : step2 description : Return hello world fn : com.gs.return args : 'Hell!' - id : elif2 description : Return hello world fn : com.gs.elif condition : <% inputs.query.status == 'Hel' % > tasks : - id : step3 description : Return hello world fn : com.gs.return args : 'Hel!' - id : else description : Return hello world fn : com.gs.else tasks : - id : step4 description : Return hello world fn : com.gs.return args : 'Hi!'",
    "title": "Workflows. . 7.6 Inbuilt functions ​",
    "tokens": 11647,
    "length": 22399
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#77-developer-written-functions",
    "content": "Workflows. . 7.7 Developer written functions ​ Developer can write functions in JS/TS and kept in src/functions folder at a path, which becomes its fully qualified name. Other languages support is planned. Once it is written, the function can be invoked from within any workflow or sub-workflow, with its fully qualified name and argument structure. summary : Custom workflow invocation id : custom_function tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : custom_fn fn : com.biz.custom_function # Can be JS/TS workflow in src/com/xyz directory with filename being custom.{js|ts} args : arg1 : 'hello world' arg2 : 'hello again'",
    "title": "Workflows. . 7.7 Developer written functions ​",
    "tokens": 309,
    "length": 816
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#78-headers-defined-at-workflow-level",
    "content": "Workflows. . 7.8 Headers defined at workflow level ​ Headers defined at workflow level are applicable for a single workflow only. You can find the example usage here",
    "title": "Workflows. . 7.8 Headers defined at workflow level ​",
    "tokens": 22,
    "length": 113
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/workflows#79-file-upload-feature",
    "content": "Workflows. . 7.9 File Upload feature ​ The framework provides file upload feature to upload files. Here is the sample event and workflow spec to upload any file. Event Spec /document.http.post : fn : com.biz.documents.upload_file id : '/sendDocuments' summary : upload document description : upload document on httpbin data : schema : body : required : false content : multipart/form-data : schema : type : object properties : fileName : type : string format : binary 7.9.1 Workflow spec to upload files with same file key ​ summary : upload file id : upload_file tasks : - id : step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : upload docfileuments fn : com.gs.http args : datasource : httpbin params : file_key : files files : <% inputs.files % > config : url : /v1/documents method : post retry : max_attempts : 5 type : constant interval : PT15M Note If file_key is same for all the files then you can use above workflow DSL. In case you have different file_keys for multiple files then you can directly use <% inputs.file_obj %> as given in the below section 6.9.2 7.9.2 Workflow spec to upload multiple files with different file keys ​ summary : upload multiple documents tasks : - id : upload_multiple_files_step1 description : upload multiple documents fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : <% inputs.file_obj % > config : url : /anything method : post 7.9.3 Workflow spec to upload file directly from URL ​ summary : upload document from url tasks : - id : upload_url_step1 description : upload document from url fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > files : sample : url : https : //s3.ap - south - 1.amazonaws.com/sample.pdf method : get config : url : /anything method : post headers : Content-Type : 'multipart/form-data'",
    "title": "Workflows. . 7.9 File Upload feature ​",
    "tokens": 1666,
    "length": 3033
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/intro",
    "content": "Datasources. . Datasources Any kind of entity which provides read and write mechanism for data is considered a datasource. For example, an API, a SQL or NoSQL datastore which includes RDBMS, key value stores, document stores etc. The settings for each datasource lies in src/datasources directory.",
    "title": "Datasources. . Datasources",
    "tokens": 63,
    "length": 272
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/intro#811-datasource-types",
    "content": "Datasources. . 8.1.1 Datasource types ​ Currently supported types API Datastores (SQL/NoSQL) Postgres Mysql Mongodb Kafka Elasticsearch Upcoming S3 File system",
    "title": "Datasources. . 8.1.1 Datasource types ​",
    "tokens": 50,
    "length": 139
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/intro",
    "content": "Datasources. . Datasources Any kind of entity which provides read and write mechanism for data is considered a datasource. For example, an API, a SQL or NoSQL datastore which includes RDBMS, key value stores, document stores etc. The settings for each datasource lies in src/datasources directory.",
    "title": "Datasources. . Datasources",
    "tokens": 63,
    "length": 272
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/intro#811-datasource-types",
    "content": "Datasources. . 8.1.1 Datasource types ​ Currently supported types API Datastores (SQL/NoSQL) Postgres Mysql Mongodb Kafka Elasticsearch Upcoming S3 File system",
    "title": "Datasources. . 8.1.1 Datasource types ​",
    "tokens": 50,
    "length": 139
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api",
    "content": "API datasource. . API datasource The API datasource acts as a wrapper around third party APIs. It helps interact with third party APIs or own microservices. It takes OpenAPI schema as its setting, and the datasource can be used in com.gs.http calls out of the box. Following functionality is provided by the framework based on the schema of the datasource Authentication and authorization as per the spec Validation of the input to the http method (must be compliant to the API spec) Validation of the response from the API (must be compliant to the API spec)",
    "title": "API datasource. . API datasource",
    "tokens": 114,
    "length": 530
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api#821-api-datasource-schema-defined-externally",
    "content": "API datasource. . 8.2.1 API datasource schema defined externally ​ If the OpenAPI spec of the API to consume/connect with is available at a URL, then one can simply refer the url here itself. idfc : schema : https : //raw.githubusercontent.com/Kong/swagger - ui - kong - theme/main/demo/public/specs/httpbin.yaml",
    "title": "API datasource. . 8.2.1 API datasource schema defined externally ​",
    "tokens": 100,
    "length": 272
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api#822-api-datasource-schema-defined-within-the-yaml-file",
    "content": "API datasource. . 8.2.2 API datasource schema defined within the yaml file ​ If there is no OpenAPI spec available for an API, then developer needs to provide details of the API schema in the .yaml file for that datasource. type : api schema : base_url : <% config.httpbin.base_url % > security : - ApiKey : sample - app - ApiToken : <% config.httpbin.api_token % > securitySchemes : ApiKey : type : apiKey in : header name : x - api - key ApiToken : type : apiKey in : header name : Authorization",
    "title": "API datasource. . 8.2.2 API datasource schema defined within the yaml file ​",
    "tokens": 328,
    "length": 631
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api#823-headers-defined-at-datasource-level",
    "content": "API datasource. . 8.2.3 Headers defined at datasource level ​ Headers defined at datasource level are applicable for all the workflows, which are using this datasource. For example, in below datasource, headers 'name' and 'title' are sent in each workflow which is using this datasource. type: api base_url: <% config.httpbin.base_url %> headers: name: godspeed title: <% inputs.headers['title'] %>",
    "title": "API datasource. . 8.2.3 Headers defined at datasource level ​",
    "tokens": 121,
    "length": 370
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api#824-headers-defined-at-task-level",
    "content": "API datasource. . 8.2.4 Headers defined at task level ​ Headers defined at task level are applicable for a single task only. You can find the example usage here",
    "title": "API datasource. . 8.2.4 Headers defined at task level ​",
    "tokens": 22,
    "length": 105
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/api#825-example-usage",
    "content": "API datasource. . 8.2.5 Example usage ​ You can find the example usage here",
    "title": "API datasource. . 8.2.5 Example usage ​",
    "tokens": 8,
    "length": 36
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore",
    "content": "Introduction. . Introduction The framework takes the approach of schema driven development. It supports multiple kinds of SQL and NoSQL datastores. The developer only needs to specify or generate the schema for a datastore, with authorization policies. The CRUD events and workflows are automatically generated from the schema itself. Shall the developer need to use these within other workflows, they can do that as well. Currently supported datastores Postgres (via Prisma) Mysql (via Prisma) Mongodb (via Prisma) Elasticsearch (via Elasticgraph, our inhouse implementation providing bunch of exciting features over Elasticsearch, including relationship management and joins.) The integration supports Model declaration (For both relational and non-relational stores) Schema generation from existing database Universal, autogenerated CRUD API. Validation of the CRUD requests Authorization mechanism at the entity, column, row and ownership levels Automatic caching based on configuration.",
    "title": "Introduction. . Introduction",
    "tokens": 201,
    "length": 974
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#831-schema-specification",
    "content": "Introduction. . 8.3.1 Schema specification ​ The framework extends Prisma specification for specifying the schema of any datastore. This can be generated from an existing database or manually created by the developer. The schema is present as {datastore_name}.prisma file in the src/datasources folder. Sample Schema generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique name String? }",
    "title": "Introduction. . 8.3.1 Schema specification ​",
    "tokens": 303,
    "length": 744
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#832-cli-commands",
    "content": "Introduction. . 8.3.2 CLI Commands ​ Any Prisma CLI command can be executed from godspeed CLI using godspeed prisma <command> . For example, $ godspeed prisma db pull --schema=./src/datasources/mongo_pull.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Prisma schema loaded from src/datasources/mongo_pull.prisma Environment variables loaded from .env Datasource \"db\" ✔ Introspected 6 models and wrote them into src/datasources/mongo_pull.prisma in 81ms *** WARNING *** Could not determine the types for the following fields. - Model \"Post\", field: \"slug\" - Model \"Profile\", field: \"userId\" - Model \"User\", field: \"email\" Run prisma generate to generate Prisma Client. note Please make sure that godspeed prisma <command> is executed inside from devcontainer/project root directory.",
    "title": "Introduction. . 8.3.2 CLI Commands ​",
    "tokens": 594,
    "length": 1219
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#833-prisma-datastore-setup",
    "content": "Introduction. . 8.3.3 Prisma Datastore Setup ​ The framework has inbuilt feature of setting up datastore automatically whenever a new {datastore_name}.prisma file is created in the src/datasources folder. In case, you are getting any error in the datastore setup, then you can refer to below section for manual setup: During the project setup, if you have not specified the type of datastore you just added, then you will have to execute godspeed update in project root directory, outside the dev container. This will deploy the container for this datastore in the dev container environment.",
    "title": "Introduction. . 8.3.3 Prisma Datastore Setup ​",
    "tokens": 127,
    "length": 553
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#model-setup",
    "content": "Introduction. 8.3.3 Prisma Datastore Setup ​. Model setup ​ Prisma model setup is done using prisma generate and db push commands.Step 1: godspeed prisma generate ​ $ godspeed prisma generate --schema=./src/datasources/mongo2.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo2.prisma ✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo2 in 111ms You can now start using Prisma Client in your code. Reference: https://pris.ly/d/client import { PrismaClient } from './src/datasources/generated-clients/mongo2' const prisma = new PrismaClient() Step 2: godspeed prisma db push ​ $ godspeed prisma db push --schema=./src/datasources/mongo.prisma _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| Environment variables loaded from .env Prisma schema loaded from src/datasources/mongo.prisma Datasource \"db\" The database is already in sync with the Prisma schema. ✔ Generated Prisma Client (3.15.2 | library) to ./src/datasources/generated-clients/mongo in 149ms",
    "title": "Introduction. 8.3.3 Prisma Datastore Setup ​. Model setup ​",
    "tokens": 962,
    "length": 1831
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#834-auto-generating-crud-apis-from-data-store-models",
    "content": "Introduction. . 8.3.4 Auto generating CRUD APIs from data store models ​ Developer can generate CRUD APIs for all the models in a datastore. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each model in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{modelName} and /functions/com/gs/{datasourceName}/{modelName} folders respectively. godspeed gen-crud-api",
    "title": "Introduction. . 8.3.4 Auto generating CRUD APIs from data store models ​",
    "tokens": 116,
    "length": 404
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#835-sample-datastore-crud-task",
    "content": "Introduction. . 8.3.5 Sample datastore CRUD task ​ Please find an example here",
    "title": "Introduction. . 8.3.5 Sample datastore CRUD task ​",
    "tokens": 6,
    "length": 28
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#836-prisma-encryption-of-fields",
    "content": "Introduction. . 8.3.6 Prisma encryption of fields ​ You can apply encryption on String type fields in Prisma. Be default, the encryption algorithm used is AES-GCM with 256 bit keys.",
    "title": "Introduction. . 8.3.6 Prisma encryption of fields ​",
    "tokens": 31,
    "length": 131
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#8361-specification",
    "content": "Introduction. 8.3.6 Prisma encryption of fields ​. 8.3.6.1 Specification ​ In your prisma schema, add /// @encrypted to the fields you want to encrypts. For example, email field in below schema: generator client { provider = \"prisma-client-js\" output = \"./generated-clients/mongo\" previewFeatures = [\"metrics\"] } datasource db { provider = \"mongodb\" url = env(\"MONGO_TEST_URL\") } model User1 { id String @id @default(auto()) @map(\"_id\") @db.ObjectId createdAt DateTime @default(now()) email String @unique /// @encrypted name String? }",
    "title": "Introduction. 8.3.6 Prisma encryption of fields ​. 8.3.6.1 Specification ​",
    "tokens": 270,
    "length": 600
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/datastore#8362-configuration",
    "content": "Introduction. 8.3.6 Prisma encryption of fields ​. 8.3.6.2 Configuration ​ You can specify prisma_secret in environment configuration For example, this is the sample configuration, set PRISMA_SECRET as env variable: prisma_secret : PRISMA_SECRET # secret used to generate hash of prisma fields",
    "title": "Introduction. 8.3.6 Prisma encryption of fields ​. 8.3.6.2 Configuration ​",
    "tokens": 69,
    "length": 235
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/kafka",
    "content": "Introduction. . Introduction The framework supports kafka as a datasource. It helps in interacting with kafka, to send/receive events on a kafka message bus.",
    "title": "Introduction. . Introduction",
    "tokens": 34,
    "length": 128
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/kafka#841-example-spec",
    "content": "Introduction. . 8.4.1 Example spec ​ The datasources for kafka are defined in src/datasources . Here, two kafka clients kafka1.yaml and kafka2.yaml are defined in datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── kafka2.yaml ├── events ├── functions └── mappings Sample configuration in kafka1.yaml type: kafka client_id: my_service brokers: [ \"kafka:9092\" ]",
    "title": "Introduction. . 8.4.1 Example spec ​",
    "tokens": 247,
    "length": 488
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph",
    "content": "Introduction. . Introduction The framework supports elasticgraph as a datasource. It supports elasticsearch as datastore. In addition, you can use various features of elasticgraph like deep graph search algorithms, joins, aggregations, multi-lingual support.",
    "title": "Introduction. . Introduction",
    "tokens": 47,
    "length": 229
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph#851-folder-structure",
    "content": "Introduction. . 8.5.1 Folder Structure ​ The datasources for elasticgraph are defined in src/datasources . Here, elasticgraph1.yaml and elasticgraph2.yaml are defined in datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── elasticgraph1.yaml │ ├── elasticgraph2.yaml ├── events ├── functions └── mappings",
    "title": "Introduction. . 8.5.1 Folder Structure ​",
    "tokens": 165,
    "length": 373
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph#852-datasource-dsl",
    "content": "Introduction. . 8.5.2 Datasource DSL ​ elasticgraph1.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg1/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch elasticgraph2.yaml type : elasticgraph schema_backend : /workspace/development/app/src/eg_config/eg2/ # schema path to config files deep : false # deep feature of elasticgraph to use graph algorithms collect : true # collect feature of elasticsearch",
    "title": "Introduction. . 8.5.2 Datasource DSL ​",
    "tokens": 254,
    "length": 632
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph#853-configuration-files-of-elasticgraph",
    "content": "Introduction. . 8.5.3 Configuration files of elasticgraph ​ All the configuration files of elasticgraph datasources should be defined in src/datasources/eg_config/ directory.Sample strucutre of config files under schema_backend path. . ├── eg1 │ ├── collect.toml │ ├── common.toml │ ├── config.toml │ ├── custom.toml │ ├── elasticsearch.toml │ ├── joins │ │ └── search.txt │ └── schema │ ├── aggregation.toml │ ├── dependencies.toml │ ├── entities │ │ ├── credit_card.toml │ │ └── user.toml │ ├── entitiesInfo.toml │ ├── relationships.txt │ ├── suggestions.toml │ └── union.toml └── eg2 ├── collect.toml ├── common.toml ├── config.toml ├── custom.toml ├── elasticsearch.toml ├── joins │ └── search.txt └── schema ├── aggregation.toml ├── dependencies.toml ├── entities │ ├── credit_card.toml │ └── user.toml ├── entitiesInfo.toml ├── relationships.txt ├── suggestions.toml └── union.toml",
    "title": "Introduction. . 8.5.3 Configuration files of elasticgraph ​",
    "tokens": 617,
    "length": 1201
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph#854-elasticgraph-setup",
    "content": "Introduction. . 8.5.4 Elasticgraph Setup ​ The framework has inbuilt feature of setting up elasticgraph model automatically whenever a new configuration is added in src/datasources/eg_config/ directory. In case, you are getting any error in the setup, then you can refer execute below step for manual setup: During the project setup, if you have not selected elasticsearch, then you will have to execute godspeed update in project root directory, outside the dev container. This will add elasticsearch in the dev container environment. Step 1: godspeed eg-push ​ $ godspeed eg-push _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 eg-push > for f in src/datasources/eg_config/*; do echo ${f}; node ./gs_service/elasticgraph/lib/mappingGenerator/reIndexer.js ${f} all; done src/datasources/eg_config/eg1",
    "title": "Introduction. . 8.5.4 Elasticgraph Setup ​",
    "tokens": 518,
    "length": 1162
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/elasticgraph#855-auto-generating-crud-apis-for-elasticgraph",
    "content": "Introduction. . 8.5.5 Auto generating CRUD APIs for elasticgraph ​ Developer can generate CRUD APIs for all the entities in src/datasources/eg_config/ directory. Events and Workflows will be auto generated for Create , Read , Update and Delete operations for each entity in respective datastore. Auto-generated events and workflows will be stored in /events/{datasourceName}/{entityName} and /functions/com/gs/eg/{datasourceName}/{entityName} folders respectively. $ godspeed gen-crud-api _ _ __ _ ___ __| | ___ _ __ ___ ___ __| | / _` | / _ \\ / _` | / __| | '_ \\ / _ \\ / _ \\ / _` | | (_| | | (_) | | (_| | \\__ \\ | |_) | | __/ | __/ | (_| | \\__, | \\___/ \\__,_| |___/ | .__/ \\___| \\___| \\__,_| |___/ |_| > eg_test@1.0.0 gen-crud-api > npx godspeed-crud-api-generator Select datasource / schema to generate CRUD APIs Events and Workflows are generated for elasticgraph.yaml",
    "title": "Introduction. . 8.5.5 Auto generating CRUD APIs for elasticgraph ​",
    "tokens": 503,
    "length": 1038
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources",
    "content": "Introduction. . Introduction The framework provides feature to extend datasources where you can add new datasources with any customized type as per your business logic.",
    "title": "Introduction. . Introduction",
    "tokens": 25,
    "length": 139
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#861-datasource-definition",
    "content": "Introduction. . 8.6.1 Datasource definition ​ You can define your datasource in yaml file inside src/datasources directory. For example, newDatasource.yaml is defined in the datasources. . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── kafka1.yaml │ └── newDatasource.yaml ├── events ├── functions └── mappings The three keys in yaml type , loadFn and executeFn are mandatory to define any new datasource which is not provided by the framework as core datasources. You can define other key/vaue pairs as per your need. Below is a sample of newDatasource.yaml type: sample loadFn: com.sample.loader executeFn: com.sample.execute client_url: https://sample.com client_id: sample123",
    "title": "Introduction. . 8.6.1 Datasource definition ​",
    "tokens": 312,
    "length": 781
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#type",
    "content": "Introduction. 8.6.1 Datasource definition ​. type ​ It defines the type of the datasource like api, soap, datastore, etc.",
    "title": "Introduction. 8.6.1 Datasource definition ​. type ​",
    "tokens": 19,
    "length": 69
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#loadfn",
    "content": "Introduction. 8.6.1 Datasource definition ​. loadFn ​ It defines the load function which loads the client for the datasource. The developer must define the load function in the workflows as mentioned in the below project structure. The loadFn can be a js/ts function which takes the datasource yaml as an input and return an object that contains client. . ├── config └── src ├── datasources ├── events ├── functions │ └── com │ └── sample │ ├── loader.ts │ └── execute.ts └── mappings A sample of loader.ts export default async function(args:{[key:string]:any;}) { const ds = { .args, client: new SampleClient(args) }; return ds; }",
    "title": "Introduction. 8.6.1 Datasource definition ​. loadFn ​",
    "tokens": 347,
    "length": 783
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#executefn",
    "content": "Introduction. 8.6.1 Datasource definition ​. executeFn ​ It defines the execute function which gets executed in the workflow. The developer must define the execute function in the workflows as mentioned in the above project structure. The executeFn can be a js/ts function which takes the workflow args as input and return status/output. export default async function(args:{[key:string]:any;}) { if(args.datasource) { const client = args.datasource.client; const data = args.data; if (!Array.isArray(args.data)) { data = [args.data]; } . . . . . . . . . . } else { return { success: false, code: 500, data: 'datasource not found in the workflow' }; } }",
    "title": "Introduction. 8.6.1 Datasource definition ​. executeFn ​",
    "tokens": 291,
    "length": 730
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#862-example-spec-for-the-event",
    "content": "Introduction. . 8.6.2 Example spec for the event ​ /sample_helloworld.http.post : id : sample_event fn : com.jfs.sample_helloworld body : description : The body of the query required : true content : application/json : # For ex. application/json application/xml schema : type : object properties : name : type : string required : [ name ]",
    "title": "Introduction. . 8.6.2 Example spec for the event ​",
    "tokens": 309,
    "length": 523
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/extensible-datasources#863-example-spec-for-the-workflow",
    "content": "Introduction. . 8.6.3 Example spec for the workflow ​ summary : hello world tasks : - id : helloworld_step1 fn : com.sample.execute args : datasource : newDatasource data : <% inputs % > config : method : sample",
    "title": "Introduction. . 8.6.3 Example spec for the workflow ​",
    "tokens": 169,
    "length": 283
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/aws",
    "content": "Introduction. . Introduction The framework supports AWS as a datasource. It helps in interacting with AWS, to use various AWS services and methods.",
    "title": "Introduction. . Introduction",
    "tokens": 24,
    "length": 118
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/aws#871-example-spec",
    "content": "Introduction. . 8.7.1 Example spec ​ The datasources for AWS are defined in src/datasources . Here, AWS datasource is defined in aws_s3.yaml . . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── aws_s3.yaml ├── events ├── functions └── mappings Sample configuration in aws_s3.yaml type: aws common: credentials: accessKeyId: 'AKIA4KQJJFGY2KPNNOEMmnbv' secretAccessKey: '+pf5xyyPSUfBNn0V9ZIH0oPVzARBvxoehR+mpzigcdfg' region: \"ap-south-1\" services: S3: config: {}",
    "title": "Introduction. . 8.7.1 Example spec ​",
    "tokens": 341,
    "length": 610
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/aws#872-comgsaws-workflow",
    "content": "Introduction. . 8.7.2 com.gs.aws workflow ​ Refer here for com.gs.aws workflow.",
    "title": "Introduction. . 8.7.2 com.gs.aws workflow ​",
    "tokens": 11,
    "length": 36
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/redis",
    "content": "Introduction. . Introduction The framework supports Redis as a datasource. It helps to utilize redis in different ways.",
    "title": "Introduction. . Introduction",
    "tokens": 20,
    "length": 90
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/datasources/redis#881-example-spec",
    "content": "Introduction. . 8.8.1 Example spec ​ The datasources for Redis are defined in src/datasources . Here, Redis datasource is defined in redis.yaml . . ├── config └── src ├── datasources │ └── httpbin.yaml │ ├── redis.yaml ├── events ├── functions └── mappings Sample configuration in redis.yaml type: redis url: redis[s]://[[username][:password]@][host][:port][/db-number] For full redis configuration, Refer redis node client documentation .",
    "title": "Introduction. . 8.8.1 Example spec ​",
    "tokens": 239,
    "length": 528
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/caching",
    "content": "Caching. . Caching Godspeed provides caching of the tasks using redis as cache. You can cache the result of any task in the workflows.",
    "title": "Caching. . Caching",
    "tokens": 26,
    "length": 115
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/caching#91-specifications",
    "content": "Caching. . 9.1 Specifications ​",
    "title": "Caching. . 9.1 Specifications ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/caching#911-datasource-spec-for-redis",
    "content": "Caching. 9.1 Specifications ​. 9.1.1 Datasource spec for redis ​ Define a datasource with type 'redis' in src/datasources . Here, redis datasource is defined in src/datasources/redis.yaml type : redis url : redis [ s ] : // [ [ username ] [ : password ] @ ] [ host ] [ : port ] [ /db - number ]",
    "title": "Caching. 9.1 Specifications ​. 9.1.1 Datasource spec for redis ​",
    "tokens": 123,
    "length": 278
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/caching#912-configuration",
    "content": "Caching. 9.1 Specifications ​. 9.1.2 Configuration ​ Define default caching datasource in static configuration log_level : debug lang : coffee server_url : https : //api.example.com : 8443/v1/api caching : redis",
    "title": "Caching. 9.1 Specifications ​. 9.1.2 Configuration ​",
    "tokens": 84,
    "length": 201
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/caching#913-workflow-spec",
    "content": "Caching. 9.1 Specifications ​. 9.1.3 Workflow spec ​ Here is the caching spec to write in the workflow. caching : key : <key name which is used to cache result in redis > invalidate : <used to invalidate the cache of some other task. Key name which we want to delete/remove from cache e.g. this field can be used in CRUD types task. While delete operation , invalidate the cache of read or update task > cache_on_failure : <true | false , whether you want to cache the failure result or not. By default , it is false > expires : <timer in seconds , until the cached result is valid > force : <true | false , force flag to specify not to use cache , always trigger task's function. Set it to true if you don't want to use cache > Example Spec summary : workflow to cache task results id : cache_wf tasks : - id : cache_step1 caching : key : cache_step1 invalidate : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'hello' config : url : /anything method : post - id : cache_step2 caching : key : cache_step2 cache_on_failure : false expires : 60 force : false fn : com.gs.http args : datasource : httpbin data : name : 'cache' config : url : /anything method : post When the workflow is triggered for the first time, then the result of the two tasks are cached in DB with keys cache_step1 and cache_step2 for 60 seconds. If the next call to this workflow occurs within 60 seconds then the cached results will be used, else API call will be triggered. In the cache_step1 , invaldiate spec is defined, which is invalidating/deleting the cached result of the cache_step2 . It means even if cache_step2 is cached, if any calls occurs within 60 seconds then the cache_step1 will delete the cached result of cache_step2 . So, no cache will be used for cache_step2 .",
    "title": "Caching. 9.1 Specifications ​. 9.1.3 Workflow spec ​",
    "tokens": 1174,
    "length": 2508
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/mappings",
    "content": "Mappings. . Mappings Mappings is a global object which will be available in your microservice. You can define anything in the mappings i.e. key/value pair map, array, etc. You can access these mappings inside your workflows at any time.",
    "title": "Mappings. . Mappings",
    "tokens": 51,
    "length": 215
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/mappings#101-project-structure",
    "content": "Mappings. . 10.1 Project structure ​ Mappings are present in src/mappings directory. The default format is yaml and you can store mappings in the nested directories also. The nested directories are also accessible in the same mappings object. . ├── config └── src └── mappings └── index.yaml └── generate.yaml",
    "title": "Mappings. . 10.1 Project structure ​",
    "tokens": 121,
    "length": 325
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/mappings#102-sample-mappings",
    "content": "Mappings. . 10.2 Sample mappings ​ This is a sample mapping which is accessible in the workflows inside mappings object using mappings.Gender and mappings.generate.genId index.yaml Gender : Male : M Female : F Others : O generate.yaml genId : 12345 Note If the file name is index.yaml then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the mappings object like mappings.Gender . However, for other file names you need to mention the file name while accessing the mappings object like mappings.generate.genId Smaple workflow accessing mappings object: - id: httpbinCof_step1 description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' gender: <% mappings.Gender[inputs.body.Gender] %> id: <% mappings.generate.genId %> config: url : /anything method: post",
    "title": "Mappings. . 10.2 Sample mappings ​",
    "tokens": 497,
    "length": 1168
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/mappings#103-use-mappings-constants-in-other-mapping-files",
    "content": "Mappings. . 10.3 Use mappings constants in other mapping files ​ You can use mapping constants in other mapping files using coffee/js scripting.For example, you have mapping files index.yaml , relations.json and reference.yaml . Use the mappings from first two files as reference in the third file as follows: index.yaml Gender : Male : M Female : F Others : O relations.json { \"id\" : 1 , \"title\" : \"Hello World\" , \"completed\" : false } reference.yaml NewGender : <% mappings.Gender.Others % > title : <% mappings.relations.title % >",
    "title": "Mappings. . 10.3 Use mappings constants in other mapping files ​",
    "tokens": 309,
    "length": 654
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/plugins",
    "content": "Plugins. . Plugins Plugins are small js/ts functions to enhance the workflows capabilities. You can write any piece of code in the plugin and can access it inside your workflows at any time.",
    "title": "Plugins. . Plugins",
    "tokens": 37,
    "length": 171
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/plugins#111-project-structure",
    "content": "Plugins. . 11.1 Project structure ​ Plugins are present in src/plugins directory. The default format is js/ts and you can store plugins in the nested directories also. . ├── config └── src └── plugins └── index.ts └── time └── epoch.ts └── epoch └── convertEpoch.ts",
    "title": "Plugins. . 11.1 Project structure ​",
    "tokens": 165,
    "length": 327
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/plugins#112-sample-plugins",
    "content": "Plugins. . 11.2 Sample plugins ​ These are the sample plugins file which export plugin functions named randomInt and convertEpochToDate . plugins/index.ts export function randomInt ( min : number , max : number ) { return Math . floor ( Math . random ( ) * ( max - min + 1 ) ) + min ; } plugins/time/epoch.ts import format from 'date-fns/format' ; export function convertEpochToDate ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } plugins/epoch/convertEpoch.ts import format from 'date-fns/format' ; export default function convertEpoch ( inputTimestamp : string ) { const newDateTime = new Date ( inputTimestamp ) ; return format ( newDateTime , 'yyyy-MM-dd HH:mm:ss' ) ; } Note If the file name is index.ts then its content is available directly at global level i.e. you don't need to write index explicitly while accessing the plugin e.g. randomInt . For other file names you need to mention the file name using underscore notation while accessing the plugins function inside your workflow e.g. time_epoch_convertEpochToDate If it's a default import then you don't need to mention the plugin function name e.g. epoch_convertEpoch",
    "title": "Plugins. . 11.2 Sample plugins ​",
    "tokens": 713,
    "length": 1591
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/plugins#113-sample-workflow-using-plugins",
    "content": "Plugins. . 11.3 Sample workflow using plugins ​ You can use these plugins in your workflows as given below: - id: httpbinCof_step1 description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http args: datasource: httpbin params: data: personal_email_id: 'ala.eforwich@email.com' id: <% 'UID-' + randomInt(1,9) %> date: <% time_epoch_convertEpochToDate(inputs.body.datetimestamp) %> default_date: <% epoch_convertEpoch(inputs.body.datetimestamp) %> config: url : /anything method: post",
    "title": "Plugins. . 11.3 Sample workflow using plugins ​",
    "tokens": 309,
    "length": 626
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author",
    "content": "Authentication & Authorization. . Authentication & Authorization",
    "title": "Authentication & Authorization. . Authentication & Authorization",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#121-authentication",
    "content": "Authentication & Authorization. . 12.1 Authentication ​ The framework provides JWT authentication for securely transmitting information among microservices. The user agent should send the JWT in the Authorization header using the Bearer schema. The content of the header should look like the following: Authorization: Bearer <token>",
    "title": "Authentication & Authorization. . 12.1 Authentication ​",
    "tokens": 57,
    "length": 282
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1211-jwt-configuration",
    "content": "Authentication & Authorization. 12.1 Authentication ​. 12.1.1 JWT Configuration ​ You can do JWT configuration in Configuration/Environment variables . For example, this is the sample configuration: jwt: iss: JWT_ISS #issuer aud: JWT_AUD #audience secretOrKey: JWT_SECRET You need to export these environment variables in your environment.12.1.1.1 Access JWT payload in Workflow DSL ​ You can access the complete JWT payload in <% inputs.user %> in workflow DSL as given below: summary : Call an API and transform the tasks : - id : httpbin_step1 description : Hit http bin with some dummy data. It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > jwt_payload : <% inputs.user % > config : url : /anything method : post",
    "title": "Authentication & Authorization. 12.1 Authentication ​. 12.1.1 JWT Configuration ​",
    "tokens": 436,
    "length": 946
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1212-event-spec",
    "content": "Authentication & Authorization. 12.1 Authentication ​. 12.1.2 Event spec ​ Add authn: true in the event DSL to enable authentication for any event. /v1/loan-application/:lender_loan_application_id/kyc/ckyc/initiate.http.post: authn: true fn: com.biz.kyc.ckyc.ckyc_initiate on_validation_error: com.jfs.handle_validation_error data: schema: body: required: true content: application/json: schema: type: 'object' required: [] properties: dob: { type : 'string', format : 'date', pattern : \"[0-9]{4}-[0-9]{2}-[0-9]{2}\" } meta: type: 'object' params: - name: lender_loan_application_id in: params required: true allow_empty_value: false schema: type: string responses: #Output data defined as per the OpenAPI spec 200: schema: data: required: # default value is false content: application/json: schema: type: object properties: application_id: type: string additionalProperties: false required: [application_id]",
    "title": "Authentication & Authorization. 12.1 Authentication ​. 12.1.2 Event spec ​",
    "tokens": 824,
    "length": 1400
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1213-generate-jwt",
    "content": "Authentication & Authorization. 12.1 Authentication ​. 12.1.3 Generate JWT ​ Generally, you will get JWT from your authentication service. For testing purposes, you can generate JWT at https://jwt.io/ by providing the iss , aud and secretOrKey to verify signature. Use the encoded token as JWT authentication token. For example, In the above case, the Authorization header should look like: Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJtcy5zYW1wbGUuY29tIiwiYXVkIjoic2FtcGxlLmNvbSJ9._1fpM6VYq1rfKdTEqi8BcPTm8KIm4cNP8VhX0kQOEts",
    "title": "Authentication & Authorization. 12.1 Authentication ​. 12.1.3 Generate JWT ​",
    "tokens": 193,
    "length": 486
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1214-datasource-authentication",
    "content": "Authentication & Authorization. 12.1 Authentication ​. 12.1.4 Datasource authentication ​ You can add authentication at datasource level on API datasource . You can define an authn workflow at datasource level which requests to any authentication service for token/authentication then this workflow can return headers, params or statusCodes to the main workflow. Here is the sample spec: Datasource type : api base_url : <% config.httpbin.base_url % > authn : com.jfs.httpbin_auth Here, com.jfs.httpbin_auth is the authentication workflow which gets called for the authentication of any request to this datasource. Sample workflow using the above datasource summary : Call an API and transform the tasks : - id : httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key description : Hit http bin with some dummy data. It will send back same as response fn : com.gs.http args : datasource : httpbin data : <% inputs.body % > config : url : /anything method : post Sample authentication workflow com.jfs.httpbin_auth summary : Auth workflow tasks : - id : auth_step1 description : Hit the authn request fn : com.gs.http args : datasource : authapi data : <% inputs.query.username % > config : url : /authenticate method : post - id : auth_step2 description : Transform the response received from authn api fn : com.gs.transform args : headers : Authorization : <% 'Bearer ' + outputs.auth_step1.auth.token % > params : queryid : <% outputs.auth_step1.params.queryid % > statusCodes : <% outputs.auth_step1.status_code % > The authentication workflow should return response in this format: headers : header1 : val1 params : param1 : val1 statusCodes : [ 401 , 403 , . . ] note The authentication workflow gets called when any request returns the specified statusCodes .",
    "title": "Authentication & Authorization. 12.1 Authentication ​. 12.1.4 Datasource authentication ​",
    "tokens": 1174,
    "length": 2483
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#122-authorization",
    "content": "Authentication & Authorization. . 12.2 Authorization ​ The framework provides authorization, to verify if any event/model is authorized to access specific information or is allowed to execute certain actions.",
    "title": "Authentication & Authorization. . 12.2 Authorization ​",
    "tokens": 26,
    "length": 153
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1221-workflow-dsl",
    "content": "Authentication & Authorization. 12.2 Authorization ​. 12.2.1 Workflow DSL ​ You can add authorization workflow at the task level in any workflow. The authorization workflow should return allow/deny or json output to the main worklfow. Allow/Deny If authz workflow returns data as true/false, it means the task is allowed/denied to get executed. JSON output If authz workflow returns JSON output then it is merged with args.data of the task for which authz is being executed.Here is the sample spec: Sample workflow calling the authz workflow summary : Call an API tasks : - id : httpbin_step1 description : Hit http bin with some dummy data. It will send back same as response authz : fn : com.jfs.authz args : <% inputs % > fn : com.gs.http args : datasource : httpbin data : <% inputs % > config : url : /anything method : post Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: true } else if outputs.authz_step1.data.code == 201 then { success: true data: where: role: 'USER' } else { success: false data: false } %> The authorization workflow should return response in this format to allow/deny: success : true/false data : true/false/JSON output When data is returned as false i.e. deny then the framework will send 403 Unauthorized response.",
    "title": "Authentication & Authorization. 12.2 Authorization ​. 12.2.1 Workflow DSL ​",
    "tokens": 1154,
    "length": 2319
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/authen-author#1222-sample-db-query-call-authorization",
    "content": "Authentication & Authorization. 12.2 Authorization ​. 12.2.2 Sample DB query call authorization ​ In DB query call, authz workflow can return JSON output with where clause, include clause etc. which will be merged with the args of the main workflow which is doing DB query.Here is the sample spec: Sample workflow calling the authz workflow summary : datastore demo tasks : - id : find_user description : find users authz : fn : com.jfs.auth args : <% inputs % > fn : com.gs.datastore args : datasource : mongo data : include : <% inputs.body.include % > where : <% inputs.body.where % > config : method : user.findMany Sample authorization workflow com.jfs.authz summary : Authorization workflow tasks : - id : authz_step1 description : return allow/deny based upon user fn : com.gs.http args : datasource : authz data : <% inputs.body.user % > config : url : /authorize method : post - id : authz_step2 description : transform response from authz api fn : com.gs.transform args : | <coffee% if outputs.authz_step1.data.code == 200 then { success: true data: where: role: 'USER' } else { success: false data: false } %> When authorization workflow com.jfs.authz returns success: true then its data will be merged with the main workflow which is calling the authz workflow. For example, in the above authz workflow, data is returned as: data : where : role : 'USER' This data will be merged with the args.data of the main workflow i.e. args : data : include : <% inputs.body.include % > where : <% inputs.body.where % > # where clause from authz workflow will be merged with this",
    "title": "Authentication & Authorization. 12.2 Authorization ​. 12.2.2 Sample DB query call authorization ​",
    "tokens": 1161,
    "length": 2256
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro",
    "content": "Observability. . Observability",
    "title": "Observability. . Observability",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#131-introduction",
    "content": "Observability. . 13.1 Introduction ​ For observability, the framework supports Application Performance Monitoring(APM) abd Business Performance Monitoring(BPM) out of the box. This includes distributed trace context propagation across sync and async channels, logging and basic metrics.For the same, we are leveraging the OpenTelemetry standard and its supporting tech ecosystem. Not even a single request must go untracked!",
    "title": "Observability. . 13.1 Introduction ​",
    "tokens": 77,
    "length": 390
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1311-architecture",
    "content": "Observability. 13.1 Introduction ​. 13.1.1 Architecture ​ Both Traces and Metrics are sent to OTEL Collector directly. Tempo is used as tracing backend for traces and Prometheus is used for metrics with Mimir as its backend. For Logs , a fluent bit daemonset is running on node, which collects logs from various applications on the node. Loki is used as logs aggregation solution.",
    "title": "Observability. 13.1 Introduction ​. 13.1.1 Architecture ​",
    "tokens": 84,
    "length": 336
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#132-goals",
    "content": "Observability. . 13.2 Goals ​",
    "title": "Observability. . 13.2 Goals ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#auto-application-performance-monitoring",
    "content": "Observability. 13.2 Goals ​. Auto application performance monitoring ​ No code APM across microservices, integrable with standard APM tools and logging backends, without any dev effort.",
    "title": "Observability. 13.2 Goals ​. Auto application performance monitoring ​",
    "tokens": 26,
    "length": 114
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#backend-agnostic",
    "content": "Observability. 13.2 Goals ​. Backend agnostic ​ Numerous open source and commercial softwares for Observability support OpenTelemetry out of the box, allowing one to switch between them if needed.",
    "title": "Observability. 13.2 Goals ​. Backend agnostic ​",
    "tokens": 30,
    "length": 148
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#complete-debuggability",
    "content": "Observability. 13.2 Goals ​. Complete debuggability ​ Collect, correlate and debug signals across logs (events), traces and metrics, based on the request id and the attributes defined for the organization. For example, app version, function, DB query, K8s pod, domain, microservice etc.",
    "title": "Observability. 13.2 Goals ​. Complete debuggability ​",
    "tokens": 50,
    "length": 232
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#133-configuration",
    "content": "Observability. . 13.3 Configuration ​",
    "title": "Observability. . 13.3 Configuration ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1331-otel-exporter-endpoint",
    "content": "Observability. 13.3 Configuration ​. 13.3.1 OTEL exporter endpoint ​ Specify the IP address of your OTEL collector as env variable. Refer OTEL Exporter for more information. $ export OTEL_EXPORTER_OTLP_ENDPOINT=<IP of OTEL collector>:4317 For example, export OTEL_EXPORTER_OTLP_ENDPOINT=http://172.17.0.1:4317",
    "title": "Observability. 13.3 Configuration ​. 13.3.1 OTEL exporter endpoint ​",
    "tokens": 110,
    "length": 270
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1332-otel-service-name",
    "content": "Observability. 13.3 Configuration ​. 13.3.2 OTEL service name ​ Specify the service name by which you want to setup observability and set it as env variable. $ export OTEL_SERVICE_NAME=sample_proj1 Let's assume you have setup SigNoz as the exporter then you will see something like this: In case you have any questions, please reach out to us on our Discord channel .",
    "title": "Observability. 13.3 Configuration ​. 13.3.2 OTEL service name ​",
    "tokens": 108,
    "length": 338
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1333-logging",
    "content": "Observability. 13.3 Configuration ​. 13.3.3 Logging ​ 13.3.3.1 Log level ​ The minimum level set to log above this level. Please refer Pino log levels for more information. Set log_level in Static variables 13.3.3.2 Log fields masking ​ If you want to hide sensitive information in logs then define the fields which need to be hidden in redact feature in Static variables . Redaction path syntax is standard JSON object lookup. For example, config/default.yaml redact : [ 'a.b.c' , 'a.b.*' , 'req.headers' ] By specifying the above redaction paths, the objects which have these properties will be masked in the logs. note Please refer Pino redaction paths for more information. Generic convention If you want to mask any field in the objects in all deep nesting levels then you can use **.<field_name> convention instead of specifying each path explicitly. For example, config/default.yaml redact : [ '**.mobileNumber' ] By specifying the above redaction path, mobileNumber field will be redacted in logs in all nesting levels. Sample masked logs: {\"Body\":\"args after evaluation: step1 {\\\"name\\\":\\\"ABC\\\",\\\"gender\\\":\\\"M\\\",\\\"age\\\":25,\\\"mobileNumber\\\":\\\"*****\\\"}\",\"Timestamp\":\"1684221387896000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"4030f41a75cb\",\"process.pid\":3593},\"Attributes\":{\"event\":\"/helloworld.http.get\",\"workflow_name\":\"helloworld\",\"task_id\":\"step1\"}} {\"Body\":\"Executing handler step1 {\\\"name\\\":\\\"ABC\\\",\\\"gender\\\":\\\"M\\\",\\\"age\\\":25,\\\"mobileNumber\\\":\\\"*****\\\"}\",\"Timestamp\":\"1684221387896000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"4030f41a75cb\",\"process.pid\":3593},\"Attributes\":{\"event\":\"/helloworld.http.get\",\"workflow_name\":\"helloworld\",\"task_id\":\"step1\"}} {\"Body\":\"Result of _executeFn step1 {\\\"name\\\":\\\"ABC\\\",\\\"gender\\\":\\\"M\\\",\\\"age\\\":25,\\\"mobileNumber\\\":\\\"*****\\\"}\",\"Timestamp\":\"1684221387897000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"4030f41a75cb\",\"process.pid\":3593},\"Attributes\":{\"event\":\"/helloworld.http.get\",\"workflow_name\":\"helloworld\",\"task_id\":\"step1\"}} {\"Body\":\"Result of _executeFn add_mobileNumber_transformation_step2 {\\\"request_data\\\":{\\\"payload\\\":{\\\"data\\\":{\\\"body\\\":{\\\"mobileNumber\\\":\\\"*****\\\"}}}}}\",\"Timestamp\":\"1684221387897000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"4030f41a75cb\",\"process.pid\":3593},\"Attributes\":{\"event\":\"/helloworld.http.get\",\"workflow_name\":\"helloworld\",\"task_id\":\"add_mobileNumber_transformation_step2\"}} {\"Body\":\"this.id: hello_world, output: {\\\"request_data\\\":{\\\"payload\\\":{\\\"data\\\":{\\\"body\\\":{\\\"mobileNumber\\\":\\\"*****\\\"}}}}}\",\"Timestamp\":\"1684221387898000000\",\"SeverityNumber\":5,\"SeverityText\":\"DEBUG\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"4030f41a75cb\",\"process.pid\":3593},\"Attributes\":{\"event\":\"/helloworld.http.get\",\"workflow_name\":\"helloworld\",\"task_id\":\"hello_world\"}} 13.3.3.3 Log format ​ By default, the logs are dumped in OTEL Logging format when you deploy your service {\"Body\":\"adding body schema for /upload_doc.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_multiple_docs.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"adding body schema for /upload_s3.http.post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /another_workflow post\",\"Timestamp\":\"1676531763727000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} {\"Body\":\"registering http handler /create/:entity_type post\",\"Timestamp\":\"1676531763728000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{}} . . . . . . . . . . . {\"Body\":\"args.retry {\\\"max_attempts\\\":3,\\\"type\\\":\\\"constant\\\",\\\"interval\\\":5000}\",\"Timestamp\":\"1676531764656000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Result of _executeFn test_step1 {\\\"success\\\":true,\\\"code\\\":200,\\\"data\\\":{\\\"args\\\":{},\\\"data\\\":\\\"{\\\\\\\"data\\\\\\\":{\\\\\\\"lan\\\\\\\":\\\\\\\"12345\\\\\\\"}}\\\",\\\"files\\\":{},\\\"form\\\":{},\\\"headers\\\":{\\\"Accept\\\":\\\"application/json, text/plain, */*\\\",\\\"Content-Length\\\":\\\"24\\\",\\\"Content-Type\\\":\\\"application/json\\\",\\\"Host\\\":\\\"httpbin.org\\\",\\\"Traceparent\\\":\\\"00-a58ef2d7ff7725c39f1e058bf22fe724-2f13e28430d61bdb-01\\\",\\\"User-Agent\\\":\\\"axios/0.25.0\\\",\\\"X-Amzn-Trace-Id\\\":\\\"Root=1-63edd835-22cff8e60555fa522c8544cf\\\"},\\\"json\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"method\\\":\\\"POST\\\",\\\"origin\\\":\\\"180.188.224.177\\\",\\\"url\\\":\\\"https://httpbin.org/anything\\\"},\\\"message\\\":\\\"OK\\\",\\\"headers\\\":{\\\"date\\\":\\\"Thu, 16 Feb 2023 07:16:05 GMT\\\",\\\"content-type\\\":\\\"application/json\\\",\\\"content-length\\\":\\\"598\\\",\\\"connection\\\":\\\"close\\\",\\\"server\\\":\\\"gunicorn/19.9.0\\\",\\\"access-control-allow-origin\\\":\\\"*\\\",\\\"access-control-allow-credentials\\\":\\\"true\\\"}}\",\"Timestamp\":\"1676531765810000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"}} {\"Body\":\"Validate Response JSON Schema Success\",\"Timestamp\":\"1676531765811000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"a58ef2d7ff7725c39f1e058bf22fe724\",\"SpanId\":\"751bc314bb6286b4\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9537a882ae58\",\"process.pid\":61741},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"}} Dev Format The dev format is basically a transformation of OTEL log format to increase readability for developers. To set the dev format logs, you need to set NODE_ENV value as dev inside your environment. export NODE_ENV=dev The format is as below: datetime [SeverityText] TraceId SpanId {Attributes} Body Sample Logs: 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_doc.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_multiple_docs.http.post 16/02/23, 12:44:42 pm [INFO] {} adding body schema for /upload_s3.http.post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /another_workflow post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /create/:entity_type post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /document post 16/02/23, 12:44:42 pm [INFO] {} registering http handler /fn_script post . . . . . . . . . . 16/02/23, 12:44:43 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} args.retry {\"max_attempts\":3,\"type\":\"constant\",\"interval\":5000} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"test_step1\"} Result of _executeFn test_step1 {\"success\":true,\"code\":200,\"data\":{\"args\":{},\"data\":\"{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}}\",\"files\":{},\"form\":{},\"headers\":{\"Accept\":\"application/json, text/plain, */*\",\"Content-Length\":\"24\",\"Content-Type\":\"application/json\",\"Host\":\"httpbin.org\",\"Traceparent\":\"00-f9f61d4940e3a8e5be8bc80faf6e36a2-f6c0a5ce67f5b07c-01\",\"User-Agent\":\"axios/0.25.0\",\"X-Amzn-Trace-Id\":\"Root=1-63edd7e4-0b8b6ba319833492520e6b0c\"},\"json\":{\"data\":{\"lan\":\"12345\"}},\"method\":\"POST\",\"origin\":\"180.188.224.177\",\"url\":\"https://httpbin.org/anything\"},\"message\":\"OK\",\"headers\":{\"date\":\"Thu, 16 Feb 2023 07:14:44 GMT\",\"content-type\":\"application/json\",\"content-length\":\"598\",\"connection\":\"close\",\"server\":\"gunicorn/19.9.0\",\"access-control-allow-origin\":\"*\",\"access-control-allow-credentials\":\"true\"}} 16/02/23, 12:44:44 pm [INFO] f9f61d4940e3a8e5be8bc80faf6e36a2 96e746f5cbbee1ac {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"task_id\":\"\"} Validate Response JSON Schema Success note If you set any other value in NODE_ENV then the logs are dumped in OTEL format by default. 13.3.3.4 Add custom identifiers in logs ​ You can add any custom identifier in the logging whenever any event is triggered on your service. The value for the custom identifier will be picked up from event body, params, query, or headers. To enable this feature ,you need to specify two things: log_attributes variable as environment variable / static variable which contains custom identifiers. For example, this is the sample static configuration: log_attributes: mobileNumber: \"query?.mobileNumber\" id: \"params?.id\" lan: \"body?.data?.lan\" location of the identifier in the request payload. As specified in the above example, if mobileNumber is present in query params then specify query?.mobileNumber . if id is present in path params then specify params?.id . if lan is present in data field inside body then specify body?.data?.lan . note Please make sure to add ? in case any field is optional like body?.data?.lan so that it works well with undefined values. This will add lan in the logs if it is present else it will not get added. Sample Logs Dev format 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} Processing event /test/:id.http.post 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event inputs {\"baseUrl\":\"\",\"body\":{\"data\":{\"lan\":\"12345\"}},\"fresh\":false,\"hostname\":\"localhost\",\"ip\":\"::ffff:172.22.0.1\",\"ips\":[],\"method\":\"POST\",\"originalUrl\":\"/test/12?mobileNumber=9878987898\",\"params\":{\"id\":\"12\"},\"path\":\"/test/12\",\"protocol\":\"http\",\"query\":{\"mobileNumber\":\"9878987898\"},\"route\":{\"path\":\"/test/:id\",\"stack\":[{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"},{\"name\":\"<anonymous>\",\"keys\":[],\"regexp\":{\"fast_star\":false,\"fast_slash\":false},\"method\":\"post\"}],\"methods\":{\"post\":true}},\"secure\":false,\"stale\":true,\"subdomains\":[],\"xhr\":false,\"headers\":{\"content-type\":\"application/json\",\"user-agent\":\"PostmanRuntime/7.29.2\",\"accept\":\"*/*\",\"postman-token\":\"835edd29-7c36-4e11-9b79-c661bbd911b0\",\"host\":\"localhost:4000\",\"accept-encoding\":\"gzip, deflate, br\",\"connection\":\"keep-alive\",\"content-length\":\"46\"},\"files\":[]} 21/02/23, 11:54:06 am [INFO] 48c894ed7d65caa236e8cc0664ee4e5e 5af2d3d564e86fb6 {\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"} event body and eventSpec exist OTEL format {\"Body\":\"Processing event /test/:id.http.post\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event inputs {\\\"baseUrl\\\":\\\"\\\",\\\"body\\\":{\\\"data\\\":{\\\"lan\\\":\\\"12345\\\"}},\\\"fresh\\\":false,\\\"hostname\\\":\\\"localhost\\\",\\\"ip\\\":\\\"::ffff:172.22.0.1\\\",\\\"ips\\\":[],\\\"method\\\":\\\"POST\\\",\\\"originalUrl\\\":\\\"/test/12?mobileNumber=9878987898\\\",\\\"params\\\":{\\\"id\\\":\\\"12\\\"},\\\"path\\\":\\\"/test/12\\\",\\\"protocol\\\":\\\"http\\\",\\\"query\\\":{\\\"mobileNumber\\\":\\\"9878987898\\\"},\\\"route\\\":{\\\"path\\\":\\\"/test/:id\\\",\\\"stack\\\":[{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"},{\\\"name\\\":\\\"<anonymous>\\\",\\\"keys\\\":[],\\\"regexp\\\":{\\\"fast_star\\\":false,\\\"fast_slash\\\":false},\\\"method\\\":\\\"post\\\"}],\\\"methods\\\":{\\\"post\\\":true}},\\\"secure\\\":false,\\\"stale\\\":true,\\\"subdomains\\\":[],\\\"xhr\\\":false,\\\"headers\\\":{\\\"content-type\\\":\\\"application/json\\\",\\\"user-agent\\\":\\\"PostmanRuntime/7.29.2\\\",\\\"accept\\\":\\\"*/*\\\",\\\"postman-token\\\":\\\"9e57df7d-0a75-48b6-bc52-921bd5c045b7\\\",\\\"host\\\":\\\"localhost:4000\\\",\\\"accept-encoding\\\":\\\"gzip, deflate, br\\\",\\\"connection\\\":\\\"keep-alive\\\",\\\"content-length\\\":\\\"46\\\"},\\\"files\\\":[]}\",\"Timestamp\":\"1676960742403000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}} {\"Body\":\"event body and eventSpec exist\",\"Timestamp\":\"1676960742404000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"3b66e6f8ec6624f6467af1226503a39e\",\"SpanId\":\"eb6e7d89ac381e9f\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"5252603e08be\",\"process.pid\":828},\"Attributes\":{\"event\":\"/test/:id.http.post\",\"workflow_name\":\"com.jfs.test\",\"mobileNumber\":\"9878987898\",\"id\":\"12\",\"lan\":\"12345\"}}",
    "title": "Observability. 13.3 Configuration ​. 13.3.3 Logging ​",
    "tokens": 5130,
    "length": 14304
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#134-custom-metrics-traces-and-logs-bpm",
    "content": "Observability. . 13.4 Custom metrics, traces and logs (BPM) ​ Custom metrics, traces and logs can be added in the workflow DSL at each task level then these will be available out of the box along with APM.",
    "title": "Observability. . 13.4 Custom metrics, traces and logs (BPM) ​",
    "tokens": 31,
    "length": 143
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1341-dsl-spec-for-custom-metrics",
    "content": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.1 DSL spec for custom metrics ​ # refer https://github.com/siimon/prom-client metrics: - name: metric_name type: counter|gauge|histogram|summary labels: label1: val1 label2: val2 # followng functions depending on the metric type and all of them could be scripts, can use inputs/outputs inc: 10 dec: 10 set: 100 observe: 2000 timer: true|false(boolean) starts at the beginning of workflow/task and ends at the end of workflow/task Example spec ​ In the following example, we are using two custom metrics: httpbin_calls_total: counter type metric, counter is incremented by 1. httpbin_calls_duration: histogram type metric, timer is set to true to record duration. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http metrics: - name: httpbin_calls_total help: 'httpbin_calls_total counter of httpbin requests labeled with: method, status_code' type: counter labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> inc: 1 - name: httpbin_calls_duration help: 'httpbin_calls_duration duration histogram of httpbin responses labeled with: method, status_code' type: histogram labels: method: httpbin status_code: <% outputs.httpbin_step1.code %> timer: true args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post",
    "title": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.1 DSL spec for custom metrics ​",
    "tokens": 963,
    "length": 2042
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1342-dsl-spec-for-custom-trace",
    "content": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.2 DSL spec for custom trace ​ trace: name: span_name attributes: attribute1: value1 attribute2: value2 Example spec ​ In the following example, we are creating a new span named httpbin_trace with span attributes request and param . This span gets created when the task starts and ended when the task completes its execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http trace: name: httpbin_trace attributes: request: <%inputs.body%> param: <%inputs.query%> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post",
    "title": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.2 DSL spec for custom trace ​",
    "tokens": 478,
    "length": 1060
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#1343-dsl-spec-for-custom-logs",
    "content": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.3 DSL spec for custom logs ​ logs: before: level: fatal|error|warn|info|debug|trace # refer pino for levels message: 'Sample log before' params: param1: val1 param2: val2 attributes: request: query: <%inputs.query%> after: level: info message: 'Sample log after' params: attributes: The logs are dumped in OTEL format. Please refer to OTEL Logging Data model for understanding of fields dumped in the logs. message and params are part of Body field and attributes are part of Attributes field in the log. Example spec ​ In the following example, we are two additional logs before and after the task execution. summary: Call an API and transform the tasks: - id: httpbin_step1 # the response of this will be accessible within the parent step key, under the step1 sub key name: http bin step description: Hit http bin with some dummy data. It will send back same as response fn: com.gs.http logs: before: level: error message: 'Hello' params: - key1: v1 key2: v2 - v1 attributes: request: <%inputs.query%> after: level: error message: 'World' params: key1: v1 key2: v2 attributes: customer_name: <% outputs.httpbin_step1.data.json.customer_name %> args: datasource: httpbin params: <% inputs.query %> data: <% inputs.body %> config: url : /anything method: post Sample Logs {\"Body\":\"Hello [{\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"},\\\"v1\\\"]\",\"Timestamp\":\"1676011973016000000\",\"SeverityNumber\":9,\"SeverityText\":\"INFO\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"request\":{\"status\":\"Hello\"},\"task_id\":\"if\",\"workflow_name\":\"if_else\"}} . . . . . . . . . . . {\"Body\":\"World {\\\"key1\\\":\\\"v1\\\",\\\"key2\\\":\\\"v2\\\"}\",\"Timestamp\":\"1676011973019000000\",\"SeverityNumber\":17,\"SeverityText\":\"ERROR\",\"TraceId\":\"afde0bf5bb3533d932c1c04c30d91172\",\"SpanId\":\"ad477b2cf81ca711\",\"TraceFlags\":\"01\",\"Resource\":{\"service.name\":\"unknown_service:node\",\"host.hostname\":\"9ce06d358ba7\",\"process.pid\":67228},\"Attributes\":{\"customer_name\":\"Hell!\",\"task_id\":\"if\",\"workflow_name\":\"if_else\"}}",
    "title": "Observability. 13.4 Custom metrics, traces and logs (BPM) ​. 13.4.3 DSL spec for custom logs ​",
    "tokens": 1331,
    "length": 2765
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#135-observability-stack",
    "content": "Observability. . 13.5 Observability Stack ​ The complete observability stack with K8s helm-charts will be made available soon.",
    "title": "Observability. . 13.5 Observability Stack ​",
    "tokens": 19,
    "length": 82
  },
  {
    "url": "https://docs.godspeed.systems/docs/telemetry/intro#136-recommended-model-for-telemetry-signals",
    "content": "Observability. . 13.6 Recommended model for telemetry signals ​ Please find the draft documentation here . This is compiled in one place from various references across the OpenTelemetry documentation. This may require works by the DevOps team as well e.g. K8s related attributes.",
    "title": "Observability. . 13.6 Recommended model for telemetry signals ​",
    "tokens": 45,
    "length": 216
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/custom-middleware",
    "content": "Custom Middleware. . Custom Middleware",
    "title": "Custom Middleware. . Custom Middleware",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/microservices/custom-middleware#141-how-to-add-custom-middleware-in-godspeed",
    "content": "Custom Middleware. . 14.1 How to add custom middleware in Godspeed ​ Step 1: Create an index.js/index.ts file in src/middlewares dierctory in your project. Project structure . ├── config └── src └── middlewares └── index.ts Step 2: index.ts/index.js should be exporting array of middleware functions with signature (req, res, next) index.ts import { uuid } from 'uuidv4' ; function addUuid ( req : any , res : any , next : any ) { // Set data req . body . uuid = uuid ( ) ; // Go to next middleware next ( ) ; } function addTitle ( req : any , res : any , next : any ) { // Set data req . body . title = \"Title from middleware/ts\" ; // Go to next middleware next ( ) ; } export default [ addUuid , addTitle ] ; caution If the current middleware function does not end the request-response cycle, it must call next() to pass control to the next middleware function. Otherwise, the request will be left hanging. Sample req object Here, two properties uuid and title are added in the body of req object. { \"_events\" : { } , \"_eventsCount\" : 1 , \"httpVersionMajor\" : 1 , \"httpVersionMinor\" : 1 , \"httpVersion\" : \"1.1\" , \"complete\" : true , \"rawHeaders\" : [ \"Content-Type\" , \"application/json\" , \"User-Agent\" , \"PostmanRuntime/7.29.2\" , \"Accept\" , \"*/*\" , \"Cache-Control\" , \"no-cache\" , \"Postman-Token\" , \"7ce46b80-61e1-44c4-b91a-8a3c914797e8\" , \"Host\" , \"localhost:4901\" , \"Accept-Encoding\" , \"gzip, deflate, br\" , \"Connection\" , \"keep-alive\" , \"Content-Length\" , \"2\" ] , \"rawTrailers\" : [ ] , \"aborted\" : false , \"upgrade\" : false , \"url\" : \"/test3\" , \"method\" : \"POST\" , \"statusCode\" : null , \"statusMessage\" : null , \"_consuming\" : true , \"_dumped\" : false , \"baseUrl\" : \"\" , \"originalUrl\" : \"/test3\" , \"params\" : { } , \"query\" : { } , \"body\" : { \"uuid\" : \"cfc5fc7f-cfdf-4fe7-99ad-08993f90f570\" , \"title\" : \"Title from middleware/ts\" } , \"_body\" : true , \"id\" : 2 , \"log\" : { } , \"route\" : { \"path\" : \"/test3\" , \"stack\" : [ { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } , { \"name\" : \"<anonymous>\" , \"keys\" : [ ] , \"regexp\" : { \"fast_star\" : false , \"fast_slash\" : false } , \"method\" : \"post\" } ] , \"methods\" : { \"post\" : true } } , \"protocol\" : \"http\" , \"secure\" : false , \"ip\" : \"::ffff:192.168.224.1\" , \"ips\" : [ ] , \"subdomains\" : [ ] , \"path\" : \"/test3\" , \"hostname\" : \"localhost\" , \"host\" : \"localhost\" , \"fresh\" : false , \"stale\" : true , \"xhr\" : false , \"files\" : [ ] }",
    "title": "Custom Middleware. . 14.1 How to add custom middleware in Godspeed ​",
    "tokens": 2719,
    "length": 4251
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap",
    "content": "Roadmap. . Roadmap",
    "title": "Roadmap. . Roadmap",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#godspeed-framework-roadmap-q1-and-q2---2023",
    "content": "Roadmap. . Godspeed Framework Roadmap Q1 and Q2 - 2023 ​",
    "title": "Roadmap. . Godspeed Framework Roadmap Q1 and Q2 - 2023 ​",
    "tokens": 0,
    "length": 0
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#features-core-framework",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Features [Core Framework] ​ Generative AI based microservice code generation [In progress - Q1] Generative AI based app generation [Q-2] Support to define and handle custom event sources [Done - Q1] Adding capability for defining reusable modules and use them in projects [Q-2] Support for Java, Golang, Python{on prioritisation by partners} [Q-2] Support for GraphQL like subscriptions - For event driven architecture and its varied use cases viz, Dual writes with eventual consistency, fraud detection, anomalies, notifications,any custom actions. [Q-2]",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Features [Core Framework] ​",
    "tokens": 123,
    "length": 534
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#language-features--debugging-vscode-extension",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Language Features & Debugging VSCode Extension ​ AI based FAQ & troubleshooting [Q-1] VS code Debugger for step through debuggability[Q-2] Enhanced language & DSL feature support for coding [Constant & ongoing][Q1-2]",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Language Features & Debugging VSCode Extension ​",
    "tokens": 48,
    "length": 170
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#platform",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Platform ​ Kubernetes based CI-CD governance setup using Argo stack [Done- Q1] K8s based Grafana observability stack - [Done- Q1] Shift left approach - Technology and infra agnostic, control plane for CI,CD, governance and observability management control plane. [In progress [Q1 - Q2] K8s based stack for common services (FOSS) for authn, authz, notifications etc [on Demand][Q1-2]",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Platform ​",
    "tokens": 106,
    "length": 374
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#other-minor-stories--core-framework-platform--language-features",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Other Minor Stories- Core Framework, Platform & Language features ​ Support for Authentication for dynamic JWT tokens [Q1] Support to call YAML workflows from JS workflows [Q1] Support for using mapping file constants in other mapping file [Q1]",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Other Minor Stories- Core Framework, Platform & Language features ​",
    "tokens": 44,
    "length": 178
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#enhancements-in-language",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Enhancements in Language ​ Support for validating / formatting inline js/coffee in yaml files [Q-1] Support to show proper error hints in events, workflows and datasources yaml files [Q-1] Help / Tooltip for different kind godspeed functions [Q-1] Support for JS/TS syntax check in workflows/datasources [Q-1] Better navigate between events and workflows and definitions through [Q-1]",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. Enhancements in Language ​",
    "tokens": 95,
    "length": 361
  },
  {
    "url": "https://docs.godspeed.systems/docs/roadmap#may-pick-up",
    "content": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. May pick up ​ Unified dashboard with SSO for CD and observability. Java flavour of microservice framework (on customer demand)",
    "title": "Roadmap. Godspeed Framework Roadmap Q1 and Q2 - 2023 ​. May pick up ​",
    "tokens": 24,
    "length": 113
  }
]
